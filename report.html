<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8"/>
    <title id="head-title">report.html</title>
      <link href="assets/style.css" rel="stylesheet" type="text/css"/>
  </head>
  <body>
    <h1 id="title">report.html</h1>
    <p>Report generated on 22-Jan-2026 at 12:17:34 by <a href="https://pypi.python.org/pypi/pytest-html">pytest-html</a>
        v4.2.0</p>
    <div id="environment-header">
      <h2>Environment</h2>
    </div>
    <table id="environment"></table>
    <!-- TEMPLATES -->
      <template id="template_environment_row">
      <tr>
        <td></td>
        <td></td>
      </tr>
    </template>
    <template id="template_results-table__body--empty">
      <tbody class="results-table-row">
        <tr id="not-found-message">
          <td colspan="4">No results found. Check the filters.</td>
        </tr>
      </tbody>
    </template>
    <template id="template_results-table__tbody">
      <tbody class="results-table-row">
        <tr class="collapsible">
        </tr>
        <tr class="extras-row">
          <td class="extra" colspan="4">
            <div class="extraHTML"></div>
            <div class="media">
              <div class="media-container">
                  <div class="media-container__nav--left">&lt;</div>
                  <div class="media-container__viewport">
                    <img src="" />
                    <video controls>
                      <source src="" type="video/mp4">
                    </video>
                  </div>
                  <div class="media-container__nav--right">&gt;</div>
                </div>
                <div class="media__name"></div>
                <div class="media__counter"></div>
            </div>
            <div class="logwrapper">
              <div class="logexpander"></div>
              <div class="log"></div>
            </div>
          </td>
        </tr>
      </tbody>
    </template>
    <!-- END TEMPLATES -->
    <div class="summary">
      <div class="summary__data">
        <h2>Summary</h2>
        <div class="additional-summary prefix">
        </div>
        <p class="run-count">62 tests took 00:00:27.</p>
        <p class="filter">(Un)check the boxes to filter the results.</p>
        <div class="summary__reload">
          <div class="summary__reload__button hidden" onclick="location.reload()">
            <div>There are still tests running. <br />Reload this page to get the latest results!</div>
          </div>
        </div>
        <div class="summary__spacer"></div>
        <div class="controls">
          <div class="filters">
            <input checked="true" class="filter" name="filter_checkbox" type="checkbox" data-test-result="failed" >
            <span class="failed">60 Failed,</span>
            <input checked="true" class="filter" name="filter_checkbox" type="checkbox" data-test-result="passed" >
            <span class="passed">2 Passed,</span>
            <input checked="true" class="filter" name="filter_checkbox" type="checkbox" data-test-result="skipped" disabled>
            <span class="skipped">0 Skipped,</span>
            <input checked="true" class="filter" name="filter_checkbox" type="checkbox" data-test-result="xfailed" disabled>
            <span class="xfailed">0 Expected failures,</span>
            <input checked="true" class="filter" name="filter_checkbox" type="checkbox" data-test-result="xpassed" disabled>
            <span class="xpassed">0 Unexpected passes,</span>
            <input checked="true" class="filter" name="filter_checkbox" type="checkbox" data-test-result="error" disabled>
            <span class="error">0 Errors,</span>
            <input checked="true" class="filter" name="filter_checkbox" type="checkbox" data-test-result="rerun" disabled>
            <span class="rerun">0 Reruns</span>
            <input checked="true" class="filter" name="filter_checkbox" type="checkbox" data-test-result="retried" disabled>
            <span class="retried">0 Retried,</span>
          </div>
          <div class="collapse">
            <button id="show_all_details">Show all details</button>&nbsp;/&nbsp;<button id="hide_all_details">Hide all details</button>
          </div>
        </div>
      </div>
      <div class="additional-summary summary">
      </div>
      <div class="additional-summary postfix">
      </div>
    </div>
    <table id="results-table">
      <thead id="results-table-head">
        <tr>
          <th class="sortable" data-column-type="result">Result</th>
          <th class="sortable" data-column-type="testId">Test</th>
          <th class="sortable" data-column-type="duration">Duration</th>
          <th>Links</th>
        </tr>
      </thead>
    </table>
  <footer>
    <div id="data-container" data-jsonblob="{&#34;environment&#34;: {&#34;Python&#34;: &#34;3.14.2&#34;, &#34;Platform&#34;: &#34;macOS-26.2-arm64-arm-64bit-Mach-O&#34;, &#34;Packages&#34;: {&#34;pytest&#34;: &#34;9.0.2&#34;, &#34;pluggy&#34;: &#34;1.6.0&#34;}, &#34;Plugins&#34;: {&#34;anyio&#34;: &#34;4.12.1&#34;, &#34;metadata&#34;: &#34;3.1.1&#34;, &#34;html&#34;: &#34;4.2.0&#34;, &#34;cov&#34;: &#34;7.0.0&#34;}, &#34;JAVA_HOME&#34;: &#34;/opt/homebrew/Cellar/openjdk@17/17.0.17/libexec/openjdk.jdk/Contents/Home&#34;}, &#34;tests&#34;: {&#34;tests/test_agent_roles_transform.py::test_transform_agent_roles_support_agent&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/test_agent_roles_transform.py::test_transform_agent_roles_support_agent&#34;, &#34;duration&#34;: &#34;00:00:10&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/test_agent_roles_transform.py::test_transform_agent_roles_support_agent&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;00:00:10&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;spark = &amp;lt;pyspark.sql.session.SparkSession object at 0x10ca1de80&amp;gt;\nagents_schema = StructType([StructField(&amp;#x27;id&amp;#x27;, LongType(), True), StructField(&amp;#x27;type&amp;#x27;, StringType(), True)])\nroles_schema = StructType([StructField(&amp;#x27;id&amp;#x27;, LongType(), True), StructField(&amp;#x27;agent_type&amp;#x27;, IntegerType(), True)])\n\n    def test_transform_agent_roles_support_agent(spark, agents_schema, roles_schema):\n        agents_data = [\n            (1, &amp;quot;support_agent&amp;quot;)\n        ]\n    \n        roles_data = [\n            (10, 1)\n        ]\n    \n        agents_df = spark.createDataFrame(agents_data, agents_schema)\n        roles_df = spark.createDataFrame(roles_data, roles_schema)\n    \n&amp;gt;       result_df = transform_agent_roles(agents_df, roles_df)\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\ntests/test_agent_roles_transform.py:48: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \nsrc/pipelines/agent_roles/agent_roles_transform.py:13: in transform_agent_roles\n    exploded_agents = agent_details_df.select(\nvenv/lib/python3.14/site-packages/pyspark/sql/classic/dataframe.py:1015: in select\n    jdf = self._jdf.select(self._jcols(*cols))\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nvenv/lib/python3.14/site-packages/py4j/java_gateway.py:1362: in __call__\n    return_value = get_return_value(\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\na = (&amp;#x27;xro76&amp;#x27;, &amp;lt;py4j.clientserver.JavaClient object at 0x10ca1c6e0&amp;gt;, &amp;#x27;o42&amp;#x27;, &amp;#x27;select&amp;#x27;)\nkw = {}, Py4JJavaError = &amp;lt;class &amp;#x27;py4j.protocol.Py4JJavaError&amp;#x27;&amp;gt;\nconverted = AnalysisException()\n\n    def deco(*a: Any, **kw: Any) -&amp;gt; Any:\n        from py4j.protocol import Py4JJavaError\n    \n        try:\n            return f(*a, **kw)\n        except Py4JJavaError as e:\n            converted = convert_exception(e.java_exception)\n            if not isinstance(converted, UnknownException):\n                # Hide where the exception came from that shows a non-Pythonic\n                # JVM exception message.\n&amp;gt;               raise converted from None\nE               pyspark.errors.exceptions.captured.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `role_ids` cannot be resolved. Did you mean one of the following? [`id`, `type`]. SQLSTATE: 42703;\nE               &amp;#x27;Project [cast(id#0L as bigint) AS agent_id#4L, &amp;#x27;explode(&amp;#x27;role_ids) AS role_id#5, &amp;#x27;role_ids AS original_role_ids#6]\nE               +- LogicalRDD [id#0L, type#1], false\n\nvenv/lib/python3.14/site-packages/pyspark/errors/exceptions/captured.py:269: AnalysisException\n\n---------------------------- Captured stderr setup -----------------------------\nWARNING: Using incubator modules: jdk.incubator.vector\nUsing Spark&amp;#x27;s default log4j profile: org/apache/spark/log4j2-defaults.properties\nSetting default log level to &amp;quot;WARN&amp;quot;.\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n26/01/22 12:17:14 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n\n----------------------------- Captured stderr call -----------------------------\n{&amp;quot;ts&amp;quot;: &amp;quot;2026-01-22 12:17:17.662&amp;quot;, &amp;quot;level&amp;quot;: &amp;quot;ERROR&amp;quot;, &amp;quot;logger&amp;quot;: &amp;quot;DataFrameQueryContextLogger&amp;quot;, &amp;quot;msg&amp;quot;: &amp;quot;[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `role_ids` cannot be resolved. Did you mean one of the following? [`id`, `type`]. SQLSTATE: 42703&amp;quot;, &amp;quot;context&amp;quot;: {&amp;quot;file&amp;quot;: &amp;quot;/Users/rithvik/zluri-data-pipeline/src/pipelines/agent_roles/agent_roles_transform.py&amp;quot;, &amp;quot;line&amp;quot;: &amp;quot;15&amp;quot;, &amp;quot;fragment&amp;quot;: &amp;quot;col&amp;quot;, &amp;quot;errorClass&amp;quot;: &amp;quot;UNRESOLVED_COLUMN.WITH_SUGGESTION&amp;quot;}, &amp;quot;exception&amp;quot;: {&amp;quot;class&amp;quot;: &amp;quot;Py4JJavaError&amp;quot;, &amp;quot;msg&amp;quot;: &amp;quot;An error occurred while calling o42.select.\\n: org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `role_ids` cannot be resolved. Did you mean one of the following? [`id`, `type`]. SQLSTATE: 42703;\\n&amp;#x27;Project [cast(id#0L as bigint) AS agent_id#4L, &amp;#x27;explode(&amp;#x27;role_ids) AS role_id#5, &amp;#x27;role_ids AS original_role_ids#6]\\n+- LogicalRDD [id#0L, type#1], false\\n\\n\\tat org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:430)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:166)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7(CheckAnalysis.scala:406)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7$adapted(CheckAnalysis.scala:404)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:273)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:272)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:272)\\n\\tat scala.collection.IterableOnceOps.foreach(IterableOnce.scala:630)\\n\\tat scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:628)\\n\\tat scala.collection.AbstractIterable.foreach(Iterable.scala:936)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:272)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:272)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:272)\\n\\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:272)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6(CheckAnalysis.scala:404)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6$adapted(CheckAnalysis.scala:404)\\n\\tat scala.collection.immutable.List.foreach(List.scala:323)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:404)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:281)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:273)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:281)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:252)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:289)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:241)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:228)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:289)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$resolveInFixedPoint$1(HybridAnalyzer.scala:238)\\n\\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\\n\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:238)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:91)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:122)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:84)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:322)\\n\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:322)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:139)\\n\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:330)\\n\\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:717)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:330)\\n\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\\n\\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:329)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:139)\\n\\tat scala.util.Try$.apply(Try.scala:217)\\n\\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1392)\\n\\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1453)\\n\\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\\n\\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:150)\\n\\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:90)\\n\\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$1(Dataset.scala:114)\\n\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\\n\\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:112)\\n\\tat org.apache.spark.sql.classic.Dataset.withPlan(Dataset.scala:2306)\\n\\tat org.apache.spark.sql.classic.Dataset.select(Dataset.scala:914)\\n\\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\\n\\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\\n\\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n\\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\\n\\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\\n\\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\\n\\tat py4j.Gateway.invoke(Gateway.java:282)\\n\\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\\n\\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\\n\\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\\n\\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\\n\\tat java.base/java.lang.Thread.run(Thread.java:840)\\n\\tSuppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller\\n\\t\\tat org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:430)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:166)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7(CheckAnalysis.scala:406)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7$adapted(CheckAnalysis.scala:404)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:273)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:272)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:272)\\n\\t\\tat scala.collection.IterableOnceOps.foreach(IterableOnce.scala:630)\\n\\t\\tat scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:628)\\n\\t\\tat scala.collection.AbstractIterable.foreach(Iterable.scala:936)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:272)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:272)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:272)\\n\\t\\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:272)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6(CheckAnalysis.scala:404)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6$adapted(CheckAnalysis.scala:404)\\n\\t\\tat scala.collection.immutable.List.foreach(List.scala:323)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:404)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:281)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:273)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:281)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:252)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:289)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:241)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:228)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:289)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$resolveInFixedPoint$1(HybridAnalyzer.scala:238)\\n\\t\\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\\n\\t\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:238)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:91)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:122)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:84)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:322)\\n\\t\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:322)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:139)\\n\\t\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:330)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:717)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:330)\\n\\t\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:329)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:139)\\n\\t\\tat scala.util.Try$.apply(Try.scala:217)\\n\\t\\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1392)\\n\\t\\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\\n\\t\\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\\n\\t\\t... 20 more\\n&amp;quot;, &amp;quot;stacktrace&amp;quot;: [{&amp;quot;class&amp;quot;: null, &amp;quot;method&amp;quot;: &amp;quot;deco&amp;quot;, &amp;quot;file&amp;quot;: &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/errors/exceptions/captured.py&amp;quot;, &amp;quot;line&amp;quot;: &amp;quot;263&amp;quot;}, {&amp;quot;class&amp;quot;: null, &amp;quot;method&amp;quot;: &amp;quot;get_return_value&amp;quot;, &amp;quot;file&amp;quot;: &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/py4j/protocol.py&amp;quot;, &amp;quot;line&amp;quot;: &amp;quot;327&amp;quot;}]}}\n&#34;}], &#34;tests/test_agent_roles_transform.py::test_transform_agent_roles_non_support_agent&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/test_agent_roles_transform.py::test_transform_agent_roles_non_support_agent&#34;, &#34;duration&#34;: &#34;62 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/test_agent_roles_transform.py::test_transform_agent_roles_non_support_agent&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;62 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;spark = &amp;lt;pyspark.sql.session.SparkSession object at 0x10ca1de80&amp;gt;\nagents_schema = StructType([StructField(&amp;#x27;id&amp;#x27;, LongType(), True), StructField(&amp;#x27;type&amp;#x27;, StringType(), True)])\nroles_schema = StructType([StructField(&amp;#x27;id&amp;#x27;, LongType(), True), StructField(&amp;#x27;agent_type&amp;#x27;, IntegerType(), True)])\n\n    def test_transform_agent_roles_non_support_agent(spark, agents_schema, roles_schema):\n        agents_data = [\n            (2, &amp;quot;collaborator&amp;quot;)\n        ]\n    \n        roles_data = [\n            (20, 3)\n        ]\n    \n        agents_df = spark.createDataFrame(agents_data, agents_schema)\n        roles_df = spark.createDataFrame(roles_data, roles_schema)\n    \n&amp;gt;       result_df = transform_agent_roles(agents_df, roles_df)\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\ntests/test_agent_roles_transform.py:72: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \nsrc/pipelines/agent_roles/agent_roles_transform.py:13: in transform_agent_roles\n    exploded_agents = agent_details_df.select(\nvenv/lib/python3.14/site-packages/pyspark/sql/classic/dataframe.py:1015: in select\n    jdf = self._jdf.select(self._jcols(*cols))\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nvenv/lib/python3.14/site-packages/py4j/java_gateway.py:1362: in __call__\n    return_value = get_return_value(\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\na = (&amp;#x27;xro129&amp;#x27;, &amp;lt;py4j.clientserver.JavaClient object at 0x10ca1c6e0&amp;gt;, &amp;#x27;o97&amp;#x27;, &amp;#x27;select&amp;#x27;)\nkw = {}, Py4JJavaError = &amp;lt;class &amp;#x27;py4j.protocol.Py4JJavaError&amp;#x27;&amp;gt;\nconverted = AnalysisException()\n\n    def deco(*a: Any, **kw: Any) -&amp;gt; Any:\n        from py4j.protocol import Py4JJavaError\n    \n        try:\n            return f(*a, **kw)\n        except Py4JJavaError as e:\n            converted = convert_exception(e.java_exception)\n            if not isinstance(converted, UnknownException):\n                # Hide where the exception came from that shows a non-Pythonic\n                # JVM exception message.\n&amp;gt;               raise converted from None\nE               pyspark.errors.exceptions.captured.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `role_ids` cannot be resolved. Did you mean one of the following? [`id`, `type`]. SQLSTATE: 42703;\nE               &amp;#x27;Project [cast(id#7L as bigint) AS agent_id#11L, &amp;#x27;explode(&amp;#x27;role_ids) AS role_id#12, &amp;#x27;role_ids AS original_role_ids#13]\nE               +- LogicalRDD [id#7L, type#8], false\n\nvenv/lib/python3.14/site-packages/pyspark/errors/exceptions/captured.py:269: AnalysisException\n\n----------------------------- Captured stderr call -----------------------------\n{&amp;quot;ts&amp;quot;: &amp;quot;2026-01-22 12:17:17.784&amp;quot;, &amp;quot;level&amp;quot;: &amp;quot;ERROR&amp;quot;, &amp;quot;logger&amp;quot;: &amp;quot;DataFrameQueryContextLogger&amp;quot;, &amp;quot;msg&amp;quot;: &amp;quot;[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `role_ids` cannot be resolved. Did you mean one of the following? [`id`, `type`]. SQLSTATE: 42703&amp;quot;, &amp;quot;context&amp;quot;: {&amp;quot;file&amp;quot;: &amp;quot;/Users/rithvik/zluri-data-pipeline/src/pipelines/agent_roles/agent_roles_transform.py&amp;quot;, &amp;quot;line&amp;quot;: &amp;quot;15&amp;quot;, &amp;quot;fragment&amp;quot;: &amp;quot;col&amp;quot;, &amp;quot;errorClass&amp;quot;: &amp;quot;UNRESOLVED_COLUMN.WITH_SUGGESTION&amp;quot;}, &amp;quot;exception&amp;quot;: {&amp;quot;class&amp;quot;: &amp;quot;Py4JJavaError&amp;quot;, &amp;quot;msg&amp;quot;: &amp;quot;An error occurred while calling o97.select.\\n: org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `role_ids` cannot be resolved. Did you mean one of the following? [`id`, `type`]. SQLSTATE: 42703;\\n&amp;#x27;Project [cast(id#7L as bigint) AS agent_id#11L, &amp;#x27;explode(&amp;#x27;role_ids) AS role_id#12, &amp;#x27;role_ids AS original_role_ids#13]\\n+- LogicalRDD [id#7L, type#8], false\\n\\n\\tat org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:430)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:166)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7(CheckAnalysis.scala:406)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7$adapted(CheckAnalysis.scala:404)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:273)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:272)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:272)\\n\\tat scala.collection.IterableOnceOps.foreach(IterableOnce.scala:630)\\n\\tat scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:628)\\n\\tat scala.collection.AbstractIterable.foreach(Iterable.scala:936)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:272)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:272)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:272)\\n\\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:272)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6(CheckAnalysis.scala:404)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6$adapted(CheckAnalysis.scala:404)\\n\\tat scala.collection.immutable.List.foreach(List.scala:323)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:404)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:281)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:273)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:281)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:252)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:289)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:241)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:228)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:289)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$resolveInFixedPoint$1(HybridAnalyzer.scala:238)\\n\\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\\n\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:238)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:91)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:122)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:84)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:322)\\n\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:322)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:139)\\n\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:330)\\n\\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:717)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:330)\\n\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\\n\\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:329)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:139)\\n\\tat scala.util.Try$.apply(Try.scala:217)\\n\\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1392)\\n\\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1453)\\n\\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\\n\\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:150)\\n\\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:90)\\n\\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$1(Dataset.scala:114)\\n\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\\n\\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:112)\\n\\tat org.apache.spark.sql.classic.Dataset.withPlan(Dataset.scala:2306)\\n\\tat org.apache.spark.sql.classic.Dataset.select(Dataset.scala:914)\\n\\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\\n\\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\\n\\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n\\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\\n\\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\\n\\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\\n\\tat py4j.Gateway.invoke(Gateway.java:282)\\n\\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\\n\\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\\n\\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\\n\\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\\n\\tat java.base/java.lang.Thread.run(Thread.java:840)\\n\\tSuppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller\\n\\t\\tat org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:430)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:166)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7(CheckAnalysis.scala:406)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7$adapted(CheckAnalysis.scala:404)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:273)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:272)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:272)\\n\\t\\tat scala.collection.IterableOnceOps.foreach(IterableOnce.scala:630)\\n\\t\\tat scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:628)\\n\\t\\tat scala.collection.AbstractIterable.foreach(Iterable.scala:936)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:272)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:272)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:272)\\n\\t\\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:272)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6(CheckAnalysis.scala:404)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6$adapted(CheckAnalysis.scala:404)\\n\\t\\tat scala.collection.immutable.List.foreach(List.scala:323)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:404)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:281)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:273)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:281)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:252)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:289)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:241)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:228)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:289)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$resolveInFixedPoint$1(HybridAnalyzer.scala:238)\\n\\t\\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\\n\\t\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:238)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:91)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:122)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:84)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:322)\\n\\t\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:322)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:139)\\n\\t\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:330)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:717)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:330)\\n\\t\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:329)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:139)\\n\\t\\tat scala.util.Try$.apply(Try.scala:217)\\n\\t\\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1392)\\n\\t\\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\\n\\t\\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\\n\\t\\t... 20 more\\n&amp;quot;, &amp;quot;stacktrace&amp;quot;: [{&amp;quot;class&amp;quot;: null, &amp;quot;method&amp;quot;: &amp;quot;deco&amp;quot;, &amp;quot;file&amp;quot;: &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/errors/exceptions/captured.py&amp;quot;, &amp;quot;line&amp;quot;: &amp;quot;263&amp;quot;}, {&amp;quot;class&amp;quot;: null, &amp;quot;method&amp;quot;: &amp;quot;get_return_value&amp;quot;, &amp;quot;file&amp;quot;: &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/py4j/protocol.py&amp;quot;, &amp;quot;line&amp;quot;: &amp;quot;327&amp;quot;}]}}\n&#34;}], &#34;tests/test_agent_roles_transform.py::test_transform_agent_roles_no_match&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/test_agent_roles_transform.py::test_transform_agent_roles_no_match&#34;, &#34;duration&#34;: &#34;56 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/test_agent_roles_transform.py::test_transform_agent_roles_no_match&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;56 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;spark = &amp;lt;pyspark.sql.session.SparkSession object at 0x10ca1de80&amp;gt;\nagents_schema = StructType([StructField(&amp;#x27;id&amp;#x27;, LongType(), True), StructField(&amp;#x27;type&amp;#x27;, StringType(), True)])\nroles_schema = StructType([StructField(&amp;#x27;id&amp;#x27;, LongType(), True), StructField(&amp;#x27;agent_type&amp;#x27;, IntegerType(), True)])\n\n    def test_transform_agent_roles_no_match(spark, agents_schema, roles_schema):\n        agents_data = [\n            (1, &amp;quot;support_agent&amp;quot;)\n        ]\n    \n        roles_data = [\n            (10, 3)  # mismatch\n        ]\n    \n        agents_df = spark.createDataFrame(agents_data, agents_schema)\n        roles_df = spark.createDataFrame(roles_data, roles_schema)\n    \n&amp;gt;       result_df = transform_agent_roles(agents_df, roles_df)\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\ntests/test_agent_roles_transform.py:96: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \nsrc/pipelines/agent_roles/agent_roles_transform.py:13: in transform_agent_roles\n    exploded_agents = agent_details_df.select(\nvenv/lib/python3.14/site-packages/pyspark/sql/classic/dataframe.py:1015: in select\n    jdf = self._jdf.select(self._jcols(*cols))\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nvenv/lib/python3.14/site-packages/py4j/java_gateway.py:1362: in __call__\n    return_value = get_return_value(\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\na = (&amp;#x27;xro182&amp;#x27;, &amp;lt;py4j.clientserver.JavaClient object at 0x10ca1c6e0&amp;gt;, &amp;#x27;o150&amp;#x27;, &amp;#x27;select&amp;#x27;)\nkw = {}, Py4JJavaError = &amp;lt;class &amp;#x27;py4j.protocol.Py4JJavaError&amp;#x27;&amp;gt;\nconverted = AnalysisException()\n\n    def deco(*a: Any, **kw: Any) -&amp;gt; Any:\n        from py4j.protocol import Py4JJavaError\n    \n        try:\n            return f(*a, **kw)\n        except Py4JJavaError as e:\n            converted = convert_exception(e.java_exception)\n            if not isinstance(converted, UnknownException):\n                # Hide where the exception came from that shows a non-Pythonic\n                # JVM exception message.\n&amp;gt;               raise converted from None\nE               pyspark.errors.exceptions.captured.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `role_ids` cannot be resolved. Did you mean one of the following? [`id`, `type`]. SQLSTATE: 42703;\nE               &amp;#x27;Project [cast(id#14L as bigint) AS agent_id#18L, &amp;#x27;explode(&amp;#x27;role_ids) AS role_id#19, &amp;#x27;role_ids AS original_role_ids#20]\nE               +- LogicalRDD [id#14L, type#15], false\n\nvenv/lib/python3.14/site-packages/pyspark/errors/exceptions/captured.py:269: AnalysisException\n\n----------------------------- Captured stderr call -----------------------------\n{&amp;quot;ts&amp;quot;: &amp;quot;2026-01-22 12:17:17.887&amp;quot;, &amp;quot;level&amp;quot;: &amp;quot;ERROR&amp;quot;, &amp;quot;logger&amp;quot;: &amp;quot;DataFrameQueryContextLogger&amp;quot;, &amp;quot;msg&amp;quot;: &amp;quot;[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `role_ids` cannot be resolved. Did you mean one of the following? [`id`, `type`]. SQLSTATE: 42703&amp;quot;, &amp;quot;context&amp;quot;: {&amp;quot;file&amp;quot;: &amp;quot;/Users/rithvik/zluri-data-pipeline/src/pipelines/agent_roles/agent_roles_transform.py&amp;quot;, &amp;quot;line&amp;quot;: &amp;quot;15&amp;quot;, &amp;quot;fragment&amp;quot;: &amp;quot;col&amp;quot;, &amp;quot;errorClass&amp;quot;: &amp;quot;UNRESOLVED_COLUMN.WITH_SUGGESTION&amp;quot;}, &amp;quot;exception&amp;quot;: {&amp;quot;class&amp;quot;: &amp;quot;Py4JJavaError&amp;quot;, &amp;quot;msg&amp;quot;: &amp;quot;An error occurred while calling o150.select.\\n: org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `role_ids` cannot be resolved. Did you mean one of the following? [`id`, `type`]. SQLSTATE: 42703;\\n&amp;#x27;Project [cast(id#14L as bigint) AS agent_id#18L, &amp;#x27;explode(&amp;#x27;role_ids) AS role_id#19, &amp;#x27;role_ids AS original_role_ids#20]\\n+- LogicalRDD [id#14L, type#15], false\\n\\n\\tat org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:430)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:166)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7(CheckAnalysis.scala:406)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7$adapted(CheckAnalysis.scala:404)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:273)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:272)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:272)\\n\\tat scala.collection.IterableOnceOps.foreach(IterableOnce.scala:630)\\n\\tat scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:628)\\n\\tat scala.collection.AbstractIterable.foreach(Iterable.scala:936)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:272)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:272)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:272)\\n\\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:272)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6(CheckAnalysis.scala:404)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6$adapted(CheckAnalysis.scala:404)\\n\\tat scala.collection.immutable.List.foreach(List.scala:323)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:404)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:281)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:273)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:281)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:252)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:289)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:241)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:228)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:289)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$resolveInFixedPoint$1(HybridAnalyzer.scala:238)\\n\\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\\n\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:238)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:91)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:122)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:84)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:322)\\n\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:322)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:139)\\n\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:330)\\n\\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:717)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:330)\\n\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\\n\\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:329)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:139)\\n\\tat scala.util.Try$.apply(Try.scala:217)\\n\\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1392)\\n\\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1453)\\n\\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\\n\\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:150)\\n\\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:90)\\n\\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$1(Dataset.scala:114)\\n\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\\n\\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:112)\\n\\tat org.apache.spark.sql.classic.Dataset.withPlan(Dataset.scala:2306)\\n\\tat org.apache.spark.sql.classic.Dataset.select(Dataset.scala:914)\\n\\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\\n\\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\\n\\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n\\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\\n\\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\\n\\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\\n\\tat py4j.Gateway.invoke(Gateway.java:282)\\n\\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\\n\\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\\n\\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\\n\\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\\n\\tat java.base/java.lang.Thread.run(Thread.java:840)\\n\\tSuppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller\\n\\t\\tat org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:430)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:166)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7(CheckAnalysis.scala:406)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7$adapted(CheckAnalysis.scala:404)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:273)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:272)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:272)\\n\\t\\tat scala.collection.IterableOnceOps.foreach(IterableOnce.scala:630)\\n\\t\\tat scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:628)\\n\\t\\tat scala.collection.AbstractIterable.foreach(Iterable.scala:936)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:272)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:272)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:272)\\n\\t\\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:272)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6(CheckAnalysis.scala:404)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6$adapted(CheckAnalysis.scala:404)\\n\\t\\tat scala.collection.immutable.List.foreach(List.scala:323)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:404)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:281)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:273)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:281)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:252)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:289)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:241)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:228)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:289)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$resolveInFixedPoint$1(HybridAnalyzer.scala:238)\\n\\t\\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\\n\\t\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:238)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:91)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:122)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:84)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:322)\\n\\t\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:322)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:139)\\n\\t\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:330)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:717)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:330)\\n\\t\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:329)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:139)\\n\\t\\tat scala.util.Try$.apply(Try.scala:217)\\n\\t\\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1392)\\n\\t\\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\\n\\t\\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\\n\\t\\t... 20 more\\n&amp;quot;, &amp;quot;stacktrace&amp;quot;: [{&amp;quot;class&amp;quot;: null, &amp;quot;method&amp;quot;: &amp;quot;deco&amp;quot;, &amp;quot;file&amp;quot;: &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/errors/exceptions/captured.py&amp;quot;, &amp;quot;line&amp;quot;: &amp;quot;263&amp;quot;}, {&amp;quot;class&amp;quot;: null, &amp;quot;method&amp;quot;: &amp;quot;get_return_value&amp;quot;, &amp;quot;file&amp;quot;: &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/py4j/protocol.py&amp;quot;, &amp;quot;line&amp;quot;: &amp;quot;327&amp;quot;}]}}\n&#34;}], &#34;tests/test_agent_roles_transform.py::test_transform_agent_roles_mixed_agents&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/test_agent_roles_transform.py::test_transform_agent_roles_mixed_agents&#34;, &#34;duration&#34;: &#34;53 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/test_agent_roles_transform.py::test_transform_agent_roles_mixed_agents&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;53 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;spark = &amp;lt;pyspark.sql.session.SparkSession object at 0x10ca1de80&amp;gt;\nagents_schema = StructType([StructField(&amp;#x27;id&amp;#x27;, LongType(), True), StructField(&amp;#x27;type&amp;#x27;, StringType(), True)])\nroles_schema = StructType([StructField(&amp;#x27;id&amp;#x27;, LongType(), True), StructField(&amp;#x27;agent_type&amp;#x27;, IntegerType(), True)])\n\n    def test_transform_agent_roles_mixed_agents(spark, agents_schema, roles_schema):\n        agents_data = [\n            (1, &amp;quot;support_agent&amp;quot;),\n            (2, &amp;quot;collaborator&amp;quot;),\n            (3, &amp;quot;support_agent&amp;quot;),\n        ]\n    \n        roles_data = [\n            (10, 1),\n            (20, 3),\n        ]\n    \n        agents_df = spark.createDataFrame(agents_data, agents_schema)\n        roles_df = spark.createDataFrame(roles_data, roles_schema)\n    \n&amp;gt;       result_df = transform_agent_roles(agents_df, roles_df)\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\ntests/test_agent_roles_transform.py:119: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \nsrc/pipelines/agent_roles/agent_roles_transform.py:13: in transform_agent_roles\n    exploded_agents = agent_details_df.select(\nvenv/lib/python3.14/site-packages/pyspark/sql/classic/dataframe.py:1015: in select\n    jdf = self._jdf.select(self._jcols(*cols))\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nvenv/lib/python3.14/site-packages/py4j/java_gateway.py:1362: in __call__\n    return_value = get_return_value(\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\na = (&amp;#x27;xro235&amp;#x27;, &amp;lt;py4j.clientserver.JavaClient object at 0x10ca1c6e0&amp;gt;, &amp;#x27;o203&amp;#x27;, &amp;#x27;select&amp;#x27;)\nkw = {}, Py4JJavaError = &amp;lt;class &amp;#x27;py4j.protocol.Py4JJavaError&amp;#x27;&amp;gt;\nconverted = AnalysisException()\n\n    def deco(*a: Any, **kw: Any) -&amp;gt; Any:\n        from py4j.protocol import Py4JJavaError\n    \n        try:\n            return f(*a, **kw)\n        except Py4JJavaError as e:\n            converted = convert_exception(e.java_exception)\n            if not isinstance(converted, UnknownException):\n                # Hide where the exception came from that shows a non-Pythonic\n                # JVM exception message.\n&amp;gt;               raise converted from None\nE               pyspark.errors.exceptions.captured.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `role_ids` cannot be resolved. Did you mean one of the following? [`id`, `type`]. SQLSTATE: 42703;\nE               &amp;#x27;Project [cast(id#21L as bigint) AS agent_id#25L, &amp;#x27;explode(&amp;#x27;role_ids) AS role_id#26, &amp;#x27;role_ids AS original_role_ids#27]\nE               +- LogicalRDD [id#21L, type#22], false\n\nvenv/lib/python3.14/site-packages/pyspark/errors/exceptions/captured.py:269: AnalysisException\n\n----------------------------- Captured stderr call -----------------------------\n{&amp;quot;ts&amp;quot;: &amp;quot;2026-01-22 12:17:17.979&amp;quot;, &amp;quot;level&amp;quot;: &amp;quot;ERROR&amp;quot;, &amp;quot;logger&amp;quot;: &amp;quot;DataFrameQueryContextLogger&amp;quot;, &amp;quot;msg&amp;quot;: &amp;quot;[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `role_ids` cannot be resolved. Did you mean one of the following? [`id`, `type`]. SQLSTATE: 42703&amp;quot;, &amp;quot;context&amp;quot;: {&amp;quot;file&amp;quot;: &amp;quot;/Users/rithvik/zluri-data-pipeline/src/pipelines/agent_roles/agent_roles_transform.py&amp;quot;, &amp;quot;line&amp;quot;: &amp;quot;15&amp;quot;, &amp;quot;fragment&amp;quot;: &amp;quot;col&amp;quot;, &amp;quot;errorClass&amp;quot;: &amp;quot;UNRESOLVED_COLUMN.WITH_SUGGESTION&amp;quot;}, &amp;quot;exception&amp;quot;: {&amp;quot;class&amp;quot;: &amp;quot;Py4JJavaError&amp;quot;, &amp;quot;msg&amp;quot;: &amp;quot;An error occurred while calling o203.select.\\n: org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `role_ids` cannot be resolved. Did you mean one of the following? [`id`, `type`]. SQLSTATE: 42703;\\n&amp;#x27;Project [cast(id#21L as bigint) AS agent_id#25L, &amp;#x27;explode(&amp;#x27;role_ids) AS role_id#26, &amp;#x27;role_ids AS original_role_ids#27]\\n+- LogicalRDD [id#21L, type#22], false\\n\\n\\tat org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:430)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:166)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7(CheckAnalysis.scala:406)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7$adapted(CheckAnalysis.scala:404)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:273)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:272)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:272)\\n\\tat scala.collection.IterableOnceOps.foreach(IterableOnce.scala:630)\\n\\tat scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:628)\\n\\tat scala.collection.AbstractIterable.foreach(Iterable.scala:936)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:272)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:272)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:272)\\n\\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:272)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6(CheckAnalysis.scala:404)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6$adapted(CheckAnalysis.scala:404)\\n\\tat scala.collection.immutable.List.foreach(List.scala:323)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:404)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:281)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:273)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:281)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:252)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:289)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:241)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:228)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:289)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$resolveInFixedPoint$1(HybridAnalyzer.scala:238)\\n\\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\\n\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:238)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:91)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:122)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:84)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:322)\\n\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:322)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:139)\\n\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:330)\\n\\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:717)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:330)\\n\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\\n\\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:329)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:139)\\n\\tat scala.util.Try$.apply(Try.scala:217)\\n\\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1392)\\n\\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1453)\\n\\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\\n\\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:150)\\n\\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:90)\\n\\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$1(Dataset.scala:114)\\n\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\\n\\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:112)\\n\\tat org.apache.spark.sql.classic.Dataset.withPlan(Dataset.scala:2306)\\n\\tat org.apache.spark.sql.classic.Dataset.select(Dataset.scala:914)\\n\\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\\n\\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\\n\\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n\\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\\n\\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\\n\\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\\n\\tat py4j.Gateway.invoke(Gateway.java:282)\\n\\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\\n\\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\\n\\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\\n\\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\\n\\tat java.base/java.lang.Thread.run(Thread.java:840)\\n\\tSuppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller\\n\\t\\tat org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:430)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:166)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7(CheckAnalysis.scala:406)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7$adapted(CheckAnalysis.scala:404)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:273)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:272)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:272)\\n\\t\\tat scala.collection.IterableOnceOps.foreach(IterableOnce.scala:630)\\n\\t\\tat scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:628)\\n\\t\\tat scala.collection.AbstractIterable.foreach(Iterable.scala:936)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:272)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:272)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:272)\\n\\t\\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:272)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6(CheckAnalysis.scala:404)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6$adapted(CheckAnalysis.scala:404)\\n\\t\\tat scala.collection.immutable.List.foreach(List.scala:323)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:404)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:281)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:273)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:281)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:252)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:289)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:241)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:228)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:289)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$resolveInFixedPoint$1(HybridAnalyzer.scala:238)\\n\\t\\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\\n\\t\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:238)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:91)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:122)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:84)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:322)\\n\\t\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:322)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:139)\\n\\t\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:330)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:717)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:330)\\n\\t\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:329)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:139)\\n\\t\\tat scala.util.Try$.apply(Try.scala:217)\\n\\t\\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1392)\\n\\t\\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\\n\\t\\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\\n\\t\\t... 20 more\\n&amp;quot;, &amp;quot;stacktrace&amp;quot;: [{&amp;quot;class&amp;quot;: null, &amp;quot;method&amp;quot;: &amp;quot;deco&amp;quot;, &amp;quot;file&amp;quot;: &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/errors/exceptions/captured.py&amp;quot;, &amp;quot;line&amp;quot;: &amp;quot;263&amp;quot;}, {&amp;quot;class&amp;quot;: null, &amp;quot;method&amp;quot;: &amp;quot;get_return_value&amp;quot;, &amp;quot;file&amp;quot;: &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/py4j/protocol.py&amp;quot;, &amp;quot;line&amp;quot;: &amp;quot;327&amp;quot;}]}}\n&#34;}], &#34;tests/test_agent_roles_transform.py::test_transform_agent_roles_empty_agents&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/test_agent_roles_transform.py::test_transform_agent_roles_empty_agents&#34;, &#34;duration&#34;: &#34;45 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/test_agent_roles_transform.py::test_transform_agent_roles_empty_agents&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;45 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;spark = &amp;lt;pyspark.sql.session.SparkSession object at 0x10ca1de80&amp;gt;\nagents_schema = StructType([StructField(&amp;#x27;id&amp;#x27;, LongType(), True), StructField(&amp;#x27;type&amp;#x27;, StringType(), True)])\nroles_schema = StructType([StructField(&amp;#x27;id&amp;#x27;, LongType(), True), StructField(&amp;#x27;agent_type&amp;#x27;, IntegerType(), True)])\n\n    def test_transform_agent_roles_empty_agents(spark, agents_schema, roles_schema):\n        agents_df = spark.createDataFrame([], agents_schema)\n    \n        roles_data = [\n            (10, 1),\n            (20, 3),\n        ]\n        roles_df = spark.createDataFrame(roles_data, roles_schema)\n    \n&amp;gt;       result_df = transform_agent_roles(agents_df, roles_df)\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\ntests/test_agent_roles_transform.py:138: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \nsrc/pipelines/agent_roles/agent_roles_transform.py:13: in transform_agent_roles\n    exploded_agents = agent_details_df.select(\nvenv/lib/python3.14/site-packages/pyspark/sql/classic/dataframe.py:1015: in select\n    jdf = self._jdf.select(self._jcols(*cols))\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nvenv/lib/python3.14/site-packages/py4j/java_gateway.py:1362: in __call__\n    return_value = get_return_value(\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\na = (&amp;#x27;xro288&amp;#x27;, &amp;lt;py4j.clientserver.JavaClient object at 0x10ca1c6e0&amp;gt;, &amp;#x27;o256&amp;#x27;, &amp;#x27;select&amp;#x27;)\nkw = {}, Py4JJavaError = &amp;lt;class &amp;#x27;py4j.protocol.Py4JJavaError&amp;#x27;&amp;gt;\nconverted = AnalysisException()\n\n    def deco(*a: Any, **kw: Any) -&amp;gt; Any:\n        from py4j.protocol import Py4JJavaError\n    \n        try:\n            return f(*a, **kw)\n        except Py4JJavaError as e:\n            converted = convert_exception(e.java_exception)\n            if not isinstance(converted, UnknownException):\n                # Hide where the exception came from that shows a non-Pythonic\n                # JVM exception message.\n&amp;gt;               raise converted from None\nE               pyspark.errors.exceptions.captured.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `role_ids` cannot be resolved. Did you mean one of the following? [`id`, `type`]. SQLSTATE: 42703;\nE               &amp;#x27;Project [cast(id#28L as bigint) AS agent_id#32L, &amp;#x27;explode(&amp;#x27;role_ids) AS role_id#33, &amp;#x27;role_ids AS original_role_ids#34]\nE               +- LogicalRDD [id#28L, type#29], false\n\nvenv/lib/python3.14/site-packages/pyspark/errors/exceptions/captured.py:269: AnalysisException\n\n----------------------------- Captured stderr call -----------------------------\n{&amp;quot;ts&amp;quot;: &amp;quot;2026-01-22 12:17:18.064&amp;quot;, &amp;quot;level&amp;quot;: &amp;quot;ERROR&amp;quot;, &amp;quot;logger&amp;quot;: &amp;quot;DataFrameQueryContextLogger&amp;quot;, &amp;quot;msg&amp;quot;: &amp;quot;[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `role_ids` cannot be resolved. Did you mean one of the following? [`id`, `type`]. SQLSTATE: 42703&amp;quot;, &amp;quot;context&amp;quot;: {&amp;quot;file&amp;quot;: &amp;quot;/Users/rithvik/zluri-data-pipeline/src/pipelines/agent_roles/agent_roles_transform.py&amp;quot;, &amp;quot;line&amp;quot;: &amp;quot;15&amp;quot;, &amp;quot;fragment&amp;quot;: &amp;quot;col&amp;quot;, &amp;quot;errorClass&amp;quot;: &amp;quot;UNRESOLVED_COLUMN.WITH_SUGGESTION&amp;quot;}, &amp;quot;exception&amp;quot;: {&amp;quot;class&amp;quot;: &amp;quot;Py4JJavaError&amp;quot;, &amp;quot;msg&amp;quot;: &amp;quot;An error occurred while calling o256.select.\\n: org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `role_ids` cannot be resolved. Did you mean one of the following? [`id`, `type`]. SQLSTATE: 42703;\\n&amp;#x27;Project [cast(id#28L as bigint) AS agent_id#32L, &amp;#x27;explode(&amp;#x27;role_ids) AS role_id#33, &amp;#x27;role_ids AS original_role_ids#34]\\n+- LogicalRDD [id#28L, type#29], false\\n\\n\\tat org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:430)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:166)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7(CheckAnalysis.scala:406)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7$adapted(CheckAnalysis.scala:404)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:273)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:272)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:272)\\n\\tat scala.collection.IterableOnceOps.foreach(IterableOnce.scala:630)\\n\\tat scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:628)\\n\\tat scala.collection.AbstractIterable.foreach(Iterable.scala:936)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:272)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:272)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:272)\\n\\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:272)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6(CheckAnalysis.scala:404)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6$adapted(CheckAnalysis.scala:404)\\n\\tat scala.collection.immutable.List.foreach(List.scala:323)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:404)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:281)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:273)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:281)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:252)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:289)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:241)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:228)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:289)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$resolveInFixedPoint$1(HybridAnalyzer.scala:238)\\n\\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\\n\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:238)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:91)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:122)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:84)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:322)\\n\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:322)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:139)\\n\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:330)\\n\\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:717)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:330)\\n\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\\n\\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:329)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:139)\\n\\tat scala.util.Try$.apply(Try.scala:217)\\n\\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1392)\\n\\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1453)\\n\\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\\n\\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:150)\\n\\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:90)\\n\\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$1(Dataset.scala:114)\\n\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\\n\\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:112)\\n\\tat org.apache.spark.sql.classic.Dataset.withPlan(Dataset.scala:2306)\\n\\tat org.apache.spark.sql.classic.Dataset.select(Dataset.scala:914)\\n\\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\\n\\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\\n\\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n\\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\\n\\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\\n\\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\\n\\tat py4j.Gateway.invoke(Gateway.java:282)\\n\\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\\n\\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\\n\\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\\n\\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\\n\\tat java.base/java.lang.Thread.run(Thread.java:840)\\n\\tSuppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller\\n\\t\\tat org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:430)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:166)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7(CheckAnalysis.scala:406)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7$adapted(CheckAnalysis.scala:404)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:273)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:272)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:272)\\n\\t\\tat scala.collection.IterableOnceOps.foreach(IterableOnce.scala:630)\\n\\t\\tat scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:628)\\n\\t\\tat scala.collection.AbstractIterable.foreach(Iterable.scala:936)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:272)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:272)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:272)\\n\\t\\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:272)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6(CheckAnalysis.scala:404)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6$adapted(CheckAnalysis.scala:404)\\n\\t\\tat scala.collection.immutable.List.foreach(List.scala:323)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:404)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:281)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:273)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:281)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:252)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:289)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:241)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:228)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:289)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$resolveInFixedPoint$1(HybridAnalyzer.scala:238)\\n\\t\\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\\n\\t\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:238)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:91)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:122)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:84)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:322)\\n\\t\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:322)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:139)\\n\\t\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:330)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:717)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:330)\\n\\t\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:329)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:139)\\n\\t\\tat scala.util.Try$.apply(Try.scala:217)\\n\\t\\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1392)\\n\\t\\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\\n\\t\\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\\n\\t\\t... 20 more\\n&amp;quot;, &amp;quot;stacktrace&amp;quot;: [{&amp;quot;class&amp;quot;: null, &amp;quot;method&amp;quot;: &amp;quot;deco&amp;quot;, &amp;quot;file&amp;quot;: &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/errors/exceptions/captured.py&amp;quot;, &amp;quot;line&amp;quot;: &amp;quot;263&amp;quot;}, {&amp;quot;class&amp;quot;: null, &amp;quot;method&amp;quot;: &amp;quot;get_return_value&amp;quot;, &amp;quot;file&amp;quot;: &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/py4j/protocol.py&amp;quot;, &amp;quot;line&amp;quot;: &amp;quot;327&amp;quot;}]}}\n&#34;}], &#34;tests/test_agent_roles_transform.py::test_transform_agent_roles_empty_roles&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/test_agent_roles_transform.py::test_transform_agent_roles_empty_roles&#34;, &#34;duration&#34;: &#34;53 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/test_agent_roles_transform.py::test_transform_agent_roles_empty_roles&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;53 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;spark = &amp;lt;pyspark.sql.session.SparkSession object at 0x10ca1de80&amp;gt;\nagents_schema = StructType([StructField(&amp;#x27;id&amp;#x27;, LongType(), True), StructField(&amp;#x27;type&amp;#x27;, StringType(), True)])\nroles_schema = StructType([StructField(&amp;#x27;id&amp;#x27;, LongType(), True), StructField(&amp;#x27;agent_type&amp;#x27;, IntegerType(), True)])\n\n    def test_transform_agent_roles_empty_roles(spark, agents_schema, roles_schema):\n        agents_data = [\n            (1, &amp;quot;support_agent&amp;quot;),\n            (2, &amp;quot;collaborator&amp;quot;),\n        ]\n        agents_df = spark.createDataFrame(agents_data, agents_schema)\n    \n        roles_df = spark.createDataFrame([], roles_schema)\n    \n&amp;gt;       result_df = transform_agent_roles(agents_df, roles_df)\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\ntests/test_agent_roles_transform.py:155: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \nsrc/pipelines/agent_roles/agent_roles_transform.py:13: in transform_agent_roles\n    exploded_agents = agent_details_df.select(\nvenv/lib/python3.14/site-packages/pyspark/sql/classic/dataframe.py:1015: in select\n    jdf = self._jdf.select(self._jcols(*cols))\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nvenv/lib/python3.14/site-packages/py4j/java_gateway.py:1362: in __call__\n    return_value = get_return_value(\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\na = (&amp;#x27;xro341&amp;#x27;, &amp;lt;py4j.clientserver.JavaClient object at 0x10ca1c6e0&amp;gt;, &amp;#x27;o309&amp;#x27;, &amp;#x27;select&amp;#x27;)\nkw = {}, Py4JJavaError = &amp;lt;class &amp;#x27;py4j.protocol.Py4JJavaError&amp;#x27;&amp;gt;\nconverted = AnalysisException()\n\n    def deco(*a: Any, **kw: Any) -&amp;gt; Any:\n        from py4j.protocol import Py4JJavaError\n    \n        try:\n            return f(*a, **kw)\n        except Py4JJavaError as e:\n            converted = convert_exception(e.java_exception)\n            if not isinstance(converted, UnknownException):\n                # Hide where the exception came from that shows a non-Pythonic\n                # JVM exception message.\n&amp;gt;               raise converted from None\nE               pyspark.errors.exceptions.captured.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `role_ids` cannot be resolved. Did you mean one of the following? [`id`, `type`]. SQLSTATE: 42703;\nE               &amp;#x27;Project [cast(id#35L as bigint) AS agent_id#39L, &amp;#x27;explode(&amp;#x27;role_ids) AS role_id#40, &amp;#x27;role_ids AS original_role_ids#41]\nE               +- LogicalRDD [id#35L, type#36], false\n\nvenv/lib/python3.14/site-packages/pyspark/errors/exceptions/captured.py:269: AnalysisException\n\n----------------------------- Captured stderr call -----------------------------\n{&amp;quot;ts&amp;quot;: &amp;quot;2026-01-22 12:17:18.163&amp;quot;, &amp;quot;level&amp;quot;: &amp;quot;ERROR&amp;quot;, &amp;quot;logger&amp;quot;: &amp;quot;DataFrameQueryContextLogger&amp;quot;, &amp;quot;msg&amp;quot;: &amp;quot;[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `role_ids` cannot be resolved. Did you mean one of the following? [`id`, `type`]. SQLSTATE: 42703&amp;quot;, &amp;quot;context&amp;quot;: {&amp;quot;file&amp;quot;: &amp;quot;/Users/rithvik/zluri-data-pipeline/src/pipelines/agent_roles/agent_roles_transform.py&amp;quot;, &amp;quot;line&amp;quot;: &amp;quot;15&amp;quot;, &amp;quot;fragment&amp;quot;: &amp;quot;col&amp;quot;, &amp;quot;errorClass&amp;quot;: &amp;quot;UNRESOLVED_COLUMN.WITH_SUGGESTION&amp;quot;}, &amp;quot;exception&amp;quot;: {&amp;quot;class&amp;quot;: &amp;quot;Py4JJavaError&amp;quot;, &amp;quot;msg&amp;quot;: &amp;quot;An error occurred while calling o309.select.\\n: org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `role_ids` cannot be resolved. Did you mean one of the following? [`id`, `type`]. SQLSTATE: 42703;\\n&amp;#x27;Project [cast(id#35L as bigint) AS agent_id#39L, &amp;#x27;explode(&amp;#x27;role_ids) AS role_id#40, &amp;#x27;role_ids AS original_role_ids#41]\\n+- LogicalRDD [id#35L, type#36], false\\n\\n\\tat org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:430)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:166)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7(CheckAnalysis.scala:406)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7$adapted(CheckAnalysis.scala:404)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:273)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:272)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:272)\\n\\tat scala.collection.IterableOnceOps.foreach(IterableOnce.scala:630)\\n\\tat scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:628)\\n\\tat scala.collection.AbstractIterable.foreach(Iterable.scala:936)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:272)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:272)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:272)\\n\\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:272)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6(CheckAnalysis.scala:404)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6$adapted(CheckAnalysis.scala:404)\\n\\tat scala.collection.immutable.List.foreach(List.scala:323)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:404)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:281)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:273)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:281)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:252)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:289)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:241)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:228)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:289)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$resolveInFixedPoint$1(HybridAnalyzer.scala:238)\\n\\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\\n\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:238)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:91)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:122)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:84)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:322)\\n\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:322)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:139)\\n\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:330)\\n\\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:717)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:330)\\n\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\\n\\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:329)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:139)\\n\\tat scala.util.Try$.apply(Try.scala:217)\\n\\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1392)\\n\\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1453)\\n\\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\\n\\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:150)\\n\\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:90)\\n\\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$1(Dataset.scala:114)\\n\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\\n\\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:112)\\n\\tat org.apache.spark.sql.classic.Dataset.withPlan(Dataset.scala:2306)\\n\\tat org.apache.spark.sql.classic.Dataset.select(Dataset.scala:914)\\n\\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\\n\\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\\n\\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n\\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\\n\\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\\n\\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\\n\\tat py4j.Gateway.invoke(Gateway.java:282)\\n\\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\\n\\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\\n\\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\\n\\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\\n\\tat java.base/java.lang.Thread.run(Thread.java:840)\\n\\tSuppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller\\n\\t\\tat org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:430)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:166)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7(CheckAnalysis.scala:406)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7$adapted(CheckAnalysis.scala:404)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:273)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:272)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:272)\\n\\t\\tat scala.collection.IterableOnceOps.foreach(IterableOnce.scala:630)\\n\\t\\tat scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:628)\\n\\t\\tat scala.collection.AbstractIterable.foreach(Iterable.scala:936)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:272)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:272)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:272)\\n\\t\\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:272)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6(CheckAnalysis.scala:404)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6$adapted(CheckAnalysis.scala:404)\\n\\t\\tat scala.collection.immutable.List.foreach(List.scala:323)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:404)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:281)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:273)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:281)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:252)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:289)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:241)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:228)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:289)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$resolveInFixedPoint$1(HybridAnalyzer.scala:238)\\n\\t\\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\\n\\t\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:238)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:91)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:122)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:84)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:322)\\n\\t\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:322)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:139)\\n\\t\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:330)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:717)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:330)\\n\\t\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:329)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:139)\\n\\t\\tat scala.util.Try$.apply(Try.scala:217)\\n\\t\\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1392)\\n\\t\\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\\n\\t\\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\\n\\t\\t... 20 more\\n&amp;quot;, &amp;quot;stacktrace&amp;quot;: [{&amp;quot;class&amp;quot;: null, &amp;quot;method&amp;quot;: &amp;quot;deco&amp;quot;, &amp;quot;file&amp;quot;: &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/errors/exceptions/captured.py&amp;quot;, &amp;quot;line&amp;quot;: &amp;quot;263&amp;quot;}, {&amp;quot;class&amp;quot;: null, &amp;quot;method&amp;quot;: &amp;quot;get_return_value&amp;quot;, &amp;quot;file&amp;quot;: &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/py4j/protocol.py&amp;quot;, &amp;quot;line&amp;quot;: &amp;quot;327&amp;quot;}]}}\n&#34;}], &#34;tests/test_agent_roles_transform.py::test_transform_agent_roles_field_types&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/test_agent_roles_transform.py::test_transform_agent_roles_field_types&#34;, &#34;duration&#34;: &#34;80 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/test_agent_roles_transform.py::test_transform_agent_roles_field_types&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;80 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;spark = &amp;lt;pyspark.sql.session.SparkSession object at 0x10ca1de80&amp;gt;\nagents_schema = StructType([StructField(&amp;#x27;id&amp;#x27;, LongType(), True), StructField(&amp;#x27;type&amp;#x27;, StringType(), True)])\nroles_schema = StructType([StructField(&amp;#x27;id&amp;#x27;, LongType(), True), StructField(&amp;#x27;agent_type&amp;#x27;, IntegerType(), True)])\n\n    def test_transform_agent_roles_field_types(spark, agents_schema, roles_schema):\n        agents_data = [\n            (5, &amp;quot;support_agent&amp;quot;)\n        ]\n    \n        roles_data = [\n            (50, 1)\n        ]\n    \n        agents_df = spark.createDataFrame(agents_data, agents_schema)\n        roles_df = spark.createDataFrame(roles_data, roles_schema)\n    \n&amp;gt;       result_df = transform_agent_roles(agents_df, roles_df)\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\ntests/test_agent_roles_transform.py:175: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \nsrc/pipelines/agent_roles/agent_roles_transform.py:13: in transform_agent_roles\n    exploded_agents = agent_details_df.select(\nvenv/lib/python3.14/site-packages/pyspark/sql/classic/dataframe.py:1015: in select\n    jdf = self._jdf.select(self._jcols(*cols))\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nvenv/lib/python3.14/site-packages/py4j/java_gateway.py:1362: in __call__\n    return_value = get_return_value(\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\na = (&amp;#x27;xro394&amp;#x27;, &amp;lt;py4j.clientserver.JavaClient object at 0x10ca1c6e0&amp;gt;, &amp;#x27;o362&amp;#x27;, &amp;#x27;select&amp;#x27;)\nkw = {}, Py4JJavaError = &amp;lt;class &amp;#x27;py4j.protocol.Py4JJavaError&amp;#x27;&amp;gt;\nconverted = AnalysisException()\n\n    def deco(*a: Any, **kw: Any) -&amp;gt; Any:\n        from py4j.protocol import Py4JJavaError\n    \n        try:\n            return f(*a, **kw)\n        except Py4JJavaError as e:\n            converted = convert_exception(e.java_exception)\n            if not isinstance(converted, UnknownException):\n                # Hide where the exception came from that shows a non-Pythonic\n                # JVM exception message.\n&amp;gt;               raise converted from None\nE               pyspark.errors.exceptions.captured.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `role_ids` cannot be resolved. Did you mean one of the following? [`id`, `type`]. SQLSTATE: 42703;\nE               &amp;#x27;Project [cast(id#42L as bigint) AS agent_id#46L, &amp;#x27;explode(&amp;#x27;role_ids) AS role_id#47, &amp;#x27;role_ids AS original_role_ids#48]\nE               +- LogicalRDD [id#42L, type#43], false\n\nvenv/lib/python3.14/site-packages/pyspark/errors/exceptions/captured.py:269: AnalysisException\n\n----------------------------- Captured stderr call -----------------------------\n{&amp;quot;ts&amp;quot;: &amp;quot;2026-01-22 12:17:18.283&amp;quot;, &amp;quot;level&amp;quot;: &amp;quot;ERROR&amp;quot;, &amp;quot;logger&amp;quot;: &amp;quot;DataFrameQueryContextLogger&amp;quot;, &amp;quot;msg&amp;quot;: &amp;quot;[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `role_ids` cannot be resolved. Did you mean one of the following? [`id`, `type`]. SQLSTATE: 42703&amp;quot;, &amp;quot;context&amp;quot;: {&amp;quot;file&amp;quot;: &amp;quot;/Users/rithvik/zluri-data-pipeline/src/pipelines/agent_roles/agent_roles_transform.py&amp;quot;, &amp;quot;line&amp;quot;: &amp;quot;15&amp;quot;, &amp;quot;fragment&amp;quot;: &amp;quot;col&amp;quot;, &amp;quot;errorClass&amp;quot;: &amp;quot;UNRESOLVED_COLUMN.WITH_SUGGESTION&amp;quot;}, &amp;quot;exception&amp;quot;: {&amp;quot;class&amp;quot;: &amp;quot;Py4JJavaError&amp;quot;, &amp;quot;msg&amp;quot;: &amp;quot;An error occurred while calling o362.select.\\n: org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `role_ids` cannot be resolved. Did you mean one of the following? [`id`, `type`]. SQLSTATE: 42703;\\n&amp;#x27;Project [cast(id#42L as bigint) AS agent_id#46L, &amp;#x27;explode(&amp;#x27;role_ids) AS role_id#47, &amp;#x27;role_ids AS original_role_ids#48]\\n+- LogicalRDD [id#42L, type#43], false\\n\\n\\tat org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:430)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:166)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7(CheckAnalysis.scala:406)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7$adapted(CheckAnalysis.scala:404)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:273)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:272)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:272)\\n\\tat scala.collection.IterableOnceOps.foreach(IterableOnce.scala:630)\\n\\tat scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:628)\\n\\tat scala.collection.AbstractIterable.foreach(Iterable.scala:936)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:272)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:272)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:272)\\n\\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:272)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6(CheckAnalysis.scala:404)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6$adapted(CheckAnalysis.scala:404)\\n\\tat scala.collection.immutable.List.foreach(List.scala:323)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:404)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:281)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:273)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:281)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:252)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:289)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:241)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:228)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:289)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$resolveInFixedPoint$1(HybridAnalyzer.scala:238)\\n\\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\\n\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:238)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:91)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:122)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:84)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:322)\\n\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:322)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:139)\\n\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:330)\\n\\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:717)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:330)\\n\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\\n\\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:329)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:139)\\n\\tat scala.util.Try$.apply(Try.scala:217)\\n\\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1392)\\n\\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1453)\\n\\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\\n\\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:150)\\n\\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:90)\\n\\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$1(Dataset.scala:114)\\n\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\\n\\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:112)\\n\\tat org.apache.spark.sql.classic.Dataset.withPlan(Dataset.scala:2306)\\n\\tat org.apache.spark.sql.classic.Dataset.select(Dataset.scala:914)\\n\\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\\n\\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\\n\\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n\\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\\n\\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\\n\\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\\n\\tat py4j.Gateway.invoke(Gateway.java:282)\\n\\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\\n\\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\\n\\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\\n\\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\\n\\tat java.base/java.lang.Thread.run(Thread.java:840)\\n\\tSuppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller\\n\\t\\tat org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:430)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:166)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7(CheckAnalysis.scala:406)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7$adapted(CheckAnalysis.scala:404)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:273)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:272)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:272)\\n\\t\\tat scala.collection.IterableOnceOps.foreach(IterableOnce.scala:630)\\n\\t\\tat scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:628)\\n\\t\\tat scala.collection.AbstractIterable.foreach(Iterable.scala:936)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:272)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:272)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:272)\\n\\t\\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:272)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6(CheckAnalysis.scala:404)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6$adapted(CheckAnalysis.scala:404)\\n\\t\\tat scala.collection.immutable.List.foreach(List.scala:323)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:404)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:281)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:273)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:281)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:252)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:289)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:241)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:228)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:289)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$resolveInFixedPoint$1(HybridAnalyzer.scala:238)\\n\\t\\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\\n\\t\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:238)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:91)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:122)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:84)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:322)\\n\\t\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:322)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:139)\\n\\t\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:330)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:717)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:330)\\n\\t\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:329)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:139)\\n\\t\\tat scala.util.Try$.apply(Try.scala:217)\\n\\t\\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1392)\\n\\t\\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\\n\\t\\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\\n\\t\\t... 20 more\\n&amp;quot;, &amp;quot;stacktrace&amp;quot;: [{&amp;quot;class&amp;quot;: null, &amp;quot;method&amp;quot;: &amp;quot;deco&amp;quot;, &amp;quot;file&amp;quot;: &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/errors/exceptions/captured.py&amp;quot;, &amp;quot;line&amp;quot;: &amp;quot;263&amp;quot;}, {&amp;quot;class&amp;quot;: null, &amp;quot;method&amp;quot;: &amp;quot;get_return_value&amp;quot;, &amp;quot;file&amp;quot;: &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/py4j/protocol.py&amp;quot;, &amp;quot;line&amp;quot;: &amp;quot;327&amp;quot;}]}}\n&#34;}], &#34;tests/test_agents_transform.py::test_active_agent_status&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/test_agents_transform.py::test_active_agent_status&#34;, &#34;duration&#34;: &#34;00:00:02&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/test_agents_transform.py::test_active_agent_status&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;00:00:02&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;spark = &amp;lt;pyspark.sql.session.SparkSession object at 0x10ca1de80&amp;gt;\n\n    def test_active_agent_status(spark):\n        df = agent_details_df(spark, [make_agent(1)])\n        active_df = agents_df(spark, [1])\n    \n        final_df, error_df = transform_agents(df, active_df)\n    \n&amp;gt;       assert final_df.count() == 1\n               ^^^^^^^^^^^^^^^^\n\ntests/test_agents_transform.py:95: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \nvenv/lib/python3.14/site-packages/pyspark/sql/classic/dataframe.py:439: in count\n    return int(self._jdf.count())\n               ^^^^^^^^^^^^^^^^^\nvenv/lib/python3.14/site-packages/py4j/java_gateway.py:1362: in __call__\n    return_value = get_return_value(\nvenv/lib/python3.14/site-packages/pyspark/errors/exceptions/captured.py:263: in deco\n    return f(*a, **kw)\n           ^^^^^^^^^^^\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nanswer = &amp;#x27;xro646&amp;#x27;\ngateway_client = &amp;lt;py4j.clientserver.JavaClient object at 0x10ca1c6e0&amp;gt;\ntarget_id = &amp;#x27;o645&amp;#x27;, name = &amp;#x27;count&amp;#x27;\n\n    def get_return_value(answer, gateway_client, target_id=None, name=None):\n        &amp;quot;&amp;quot;&amp;quot;Converts an answer received from the Java gateway into a Python object.\n    \n        For example, string representation of integers are converted to Python\n        integer, string representation of objects are converted to JavaObject\n        instances, etc.\n    \n        :param answer: the string returned by the Java gateway\n        :param gateway_client: the gateway client used to communicate with the Java\n            Gateway. Only necessary if the answer is a reference (e.g., object,\n            list, map)\n        :param target_id: the name of the object from which the answer comes from\n            (e.g., *object1* in `object1.hello()`). Optional.\n        :param name: the name of the member from which the answer comes from\n            (e.g., *hello* in `object1.hello()`). Optional.\n        &amp;quot;&amp;quot;&amp;quot;\n        if is_error(answer)[0]:\n            if len(answer) &amp;gt; 1:\n                type = answer[1]\n                value = OUTPUT_CONVERTER[type](answer[2:], gateway_client)\n                if answer[1] == REFERENCE_TYPE:\n&amp;gt;                   raise Py4JJavaError(\n                        &amp;quot;An error occurred while calling {0}{1}{2}.\\n&amp;quot;.\n                        format(target_id, &amp;quot;.&amp;quot;, name), value)\nE                   py4j.protocol.Py4JJavaError: An error occurred while calling o645.count.\nE                   : org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0) (10.20.80.191 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\nE                       check_python_version(infile)\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\nE                       raise PySparkRuntimeError(\nE                   pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nE                   Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\nE                   \nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\nE                   \tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\nE                   \tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\nE                   \tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\nE                   \tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\nE                   \tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\nE                   \tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\nE                   \tat org.apache.spark.scheduler.Task.run(Task.scala:147)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\nE                   \tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\nE                   \tat java.base/java.lang.Thread.run(Thread.java:840)\nE                   \nE                   Driver stacktrace:\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$3(DAGScheduler.scala:3122)\nE                   \tat scala.Option.getOrElse(Option.scala:201)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:3122)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:3114)\nE                   \tat scala.collection.immutable.List.foreach(List.scala:323)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:3114)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1303)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1303)\nE                   \tat scala.Option.foreach(Option.scala:437)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1303)\nE                   \tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3397)\nE                   \tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3328)\nE                   \tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3317)\nE                   \tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:50)\nE                   Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\nE                       check_python_version(infile)\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\nE                       raise PySparkRuntimeError(\nE                   pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nE                   Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\nE                   \nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\nE                   \tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\nE                   \tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\nE                   \tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\nE                   \tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\nE                   \tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\nE                   \tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\nE                   \tat org.apache.spark.scheduler.Task.run(Task.scala:147)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\nE                   \tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\nE                   \tat java.base/java.lang.Thread.run(Thread.java:840)\n\nvenv/lib/python3.14/site-packages/py4j/protocol.py:327: Py4JJavaError\n\n---------------------------- Captured stderr setup -----------------------------\n26/01/22 12:17:18 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n\n----------------------------- Captured stderr call -----------------------------\n\r[Stage 0:&amp;gt;                                                          (0 + 1) / 1]\r26/01/22 12:17:20 ERROR Executor: Exception in task 0.0 in stage 0.0 (TID 0)\norg.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\n    check_python_version(infile)\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\n    raise PySparkRuntimeError(\npyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nPlease check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n26/01/22 12:17:20 WARN TaskSetManager: Lost task 0.0 in stage 0.0 (TID 0) (10.20.80.191 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\n    check_python_version(infile)\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\n    raise PySparkRuntimeError(\npyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nPlease check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n\n26/01/22 12:17:20 ERROR TaskSetManager: Task 0 in stage 0.0 failed 1 times; aborting job\n&#34;}], &#34;tests/test_agents_transform.py::test_inactive_agent_status&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/test_agents_transform.py::test_inactive_agent_status&#34;, &#34;duration&#34;: &#34;471 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/test_agents_transform.py::test_inactive_agent_status&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;471 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;spark = &amp;lt;pyspark.sql.session.SparkSession object at 0x10ca1de80&amp;gt;\n\n    def test_inactive_agent_status(spark):\n        df = agent_details_df(spark, [make_agent(2)])\n        active_df = agents_df(spark, [])\n    \n        final_df, _ = transform_agents(df, active_df)\n    \n&amp;gt;       assert final_df.collect()[0][&amp;quot;status&amp;quot;] == &amp;quot;inactive&amp;quot;\n               ^^^^^^^^^^^^^^^^^^\n\ntests/test_agents_transform.py:109: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \nvenv/lib/python3.14/site-packages/pyspark/sql/classic/dataframe.py:443: in collect\n    sock_info = self._jdf.collectToPython()\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^\nvenv/lib/python3.14/site-packages/py4j/java_gateway.py:1362: in __call__\n    return_value = get_return_value(\nvenv/lib/python3.14/site-packages/pyspark/errors/exceptions/captured.py:263: in deco\n    return f(*a, **kw)\n           ^^^^^^^^^^^\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nanswer = &amp;#x27;xro920&amp;#x27;\ngateway_client = &amp;lt;py4j.clientserver.JavaClient object at 0x10ca1c6e0&amp;gt;\ntarget_id = &amp;#x27;o919&amp;#x27;, name = &amp;#x27;collectToPython&amp;#x27;\n\n    def get_return_value(answer, gateway_client, target_id=None, name=None):\n        &amp;quot;&amp;quot;&amp;quot;Converts an answer received from the Java gateway into a Python object.\n    \n        For example, string representation of integers are converted to Python\n        integer, string representation of objects are converted to JavaObject\n        instances, etc.\n    \n        :param answer: the string returned by the Java gateway\n        :param gateway_client: the gateway client used to communicate with the Java\n            Gateway. Only necessary if the answer is a reference (e.g., object,\n            list, map)\n        :param target_id: the name of the object from which the answer comes from\n            (e.g., *object1* in `object1.hello()`). Optional.\n        :param name: the name of the member from which the answer comes from\n            (e.g., *hello* in `object1.hello()`). Optional.\n        &amp;quot;&amp;quot;&amp;quot;\n        if is_error(answer)[0]:\n            if len(answer) &amp;gt; 1:\n                type = answer[1]\n                value = OUTPUT_CONVERTER[type](answer[2:], gateway_client)\n                if answer[1] == REFERENCE_TYPE:\n&amp;gt;                   raise Py4JJavaError(\n                        &amp;quot;An error occurred while calling {0}{1}{2}.\\n&amp;quot;.\n                        format(target_id, &amp;quot;.&amp;quot;, name), value)\nE                   py4j.protocol.Py4JJavaError: An error occurred while calling o919.collectToPython.\nE                   : org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1) (10.20.80.191 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\nE                       check_python_version(infile)\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\nE                       raise PySparkRuntimeError(\nE                   pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nE                   Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\nE                   \nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\nE                   \tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\nE                   \tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\nE                   \tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\nE                   \tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\nE                   \tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\nE                   \tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\nE                   \tat org.apache.spark.scheduler.Task.run(Task.scala:147)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\nE                   \tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\nE                   \tat java.base/java.lang.Thread.run(Thread.java:840)\nE                   \nE                   Driver stacktrace:\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$3(DAGScheduler.scala:3122)\nE                   \tat scala.Option.getOrElse(Option.scala:201)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:3122)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:3114)\nE                   \tat scala.collection.immutable.List.foreach(List.scala:323)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:3114)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1303)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1303)\nE                   \tat scala.Option.foreach(Option.scala:437)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1303)\nE                   \tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3397)\nE                   \tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3328)\nE                   \tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3317)\nE                   \tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:50)\nE                   Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\nE                       check_python_version(infile)\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\nE                       raise PySparkRuntimeError(\nE                   pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nE                   Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\nE                   \nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\nE                   \tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\nE                   \tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\nE                   \tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\nE                   \tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\nE                   \tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\nE                   \tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\nE                   \tat org.apache.spark.scheduler.Task.run(Task.scala:147)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\nE                   \tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\nE                   \tat java.base/java.lang.Thread.run(Thread.java:840)\n\nvenv/lib/python3.14/site-packages/py4j/protocol.py:327: Py4JJavaError\n\n----------------------------- Captured stderr call -----------------------------\n26/01/22 12:17:20 ERROR Executor: Exception in task 0.0 in stage 1.0 (TID 1)\norg.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\n    check_python_version(infile)\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\n    raise PySparkRuntimeError(\npyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nPlease check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n26/01/22 12:17:20 WARN TaskSetManager: Lost task 0.0 in stage 1.0 (TID 1) (10.20.80.191 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\n    check_python_version(infile)\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\n    raise PySparkRuntimeError(\npyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nPlease check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n\n26/01/22 12:17:20 ERROR TaskSetManager: Task 0 in stage 1.0 failed 1 times; aborting job\n26/01/22 12:17:20 WARN TaskSetManager: Lost task 0.0 in stage 2.0 (TID 2) (10.20.80.191 executor driver): TaskKilled (Stage cancelled: [SPARK_JOB_CANCELLED] Job 2 cancelled The corresponding SQL query has failed. SQLSTATE: XXKDA)\n&#34;}], &#34;tests/test_agents_transform.py::test_missing_optional_columns_are_added&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/test_agents_transform.py::test_missing_optional_columns_are_added&#34;, &#34;duration&#34;: &#34;301 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/test_agents_transform.py::test_missing_optional_columns_are_added&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;301 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;spark = &amp;lt;pyspark.sql.session.SparkSession object at 0x10ca1de80&amp;gt;\n\n    def test_missing_optional_columns_are_added(spark):\n        df = spark.createDataFrame(\n            [\n                Row(\n                    id=3,\n                    contact=Row(\n                        email=&amp;quot;x@y.com&amp;quot;,\n                        name=&amp;quot;X&amp;quot;,\n                        job_title=None,\n                        language=None,\n                        mobile=None,\n                        phone=None,\n                        time_zone=None,\n                        last_login_at=None,\n                    )\n                )\n            ],\n            schema=StructType([\n                StructField(&amp;quot;id&amp;quot;, LongType(), True),\n                StructField(&amp;quot;contact&amp;quot;, contact_schema, True),\n            ])\n        )\n    \n        final_df, error_df = transform_agents(df, agents_df(spark, [3]))\n    \n        for col in [\n            &amp;quot;ticket_scope&amp;quot;,\n            &amp;quot;org_agent_id&amp;quot;,\n            &amp;quot;signature&amp;quot;,\n            &amp;quot;freshchat_agent&amp;quot;,\n        ]:\n            assert col in final_df.columns\n    \n&amp;gt;       assert error_df.count() == 0\n               ^^^^^^^^^^^^^^^^\n\ntests/test_agents_transform.py:148: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \nvenv/lib/python3.14/site-packages/pyspark/sql/classic/dataframe.py:439: in count\n    return int(self._jdf.count())\n               ^^^^^^^^^^^^^^^^^\nvenv/lib/python3.14/site-packages/py4j/java_gateway.py:1362: in __call__\n    return_value = get_return_value(\nvenv/lib/python3.14/site-packages/pyspark/errors/exceptions/captured.py:263: in deco\n    return f(*a, **kw)\n           ^^^^^^^^^^^\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nanswer = &amp;#x27;xro1197&amp;#x27;\ngateway_client = &amp;lt;py4j.clientserver.JavaClient object at 0x10ca1c6e0&amp;gt;\ntarget_id = &amp;#x27;o1087&amp;#x27;, name = &amp;#x27;count&amp;#x27;\n\n    def get_return_value(answer, gateway_client, target_id=None, name=None):\n        &amp;quot;&amp;quot;&amp;quot;Converts an answer received from the Java gateway into a Python object.\n    \n        For example, string representation of integers are converted to Python\n        integer, string representation of objects are converted to JavaObject\n        instances, etc.\n    \n        :param answer: the string returned by the Java gateway\n        :param gateway_client: the gateway client used to communicate with the Java\n            Gateway. Only necessary if the answer is a reference (e.g., object,\n            list, map)\n        :param target_id: the name of the object from which the answer comes from\n            (e.g., *object1* in `object1.hello()`). Optional.\n        :param name: the name of the member from which the answer comes from\n            (e.g., *hello* in `object1.hello()`). Optional.\n        &amp;quot;&amp;quot;&amp;quot;\n        if is_error(answer)[0]:\n            if len(answer) &amp;gt; 1:\n                type = answer[1]\n                value = OUTPUT_CONVERTER[type](answer[2:], gateway_client)\n                if answer[1] == REFERENCE_TYPE:\n&amp;gt;                   raise Py4JJavaError(\n                        &amp;quot;An error occurred while calling {0}{1}{2}.\\n&amp;quot;.\n                        format(target_id, &amp;quot;.&amp;quot;, name), value)\nE                   py4j.protocol.Py4JJavaError: An error occurred while calling o1087.count.\nE                   : org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3.0 failed 1 times, most recent failure: Lost task 0.0 in stage 3.0 (TID 3) (10.20.80.191 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\nE                       check_python_version(infile)\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\nE                       raise PySparkRuntimeError(\nE                   pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nE                   Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\nE                   \nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\nE                   \tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\nE                   \tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\nE                   \tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\nE                   \tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\nE                   \tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\nE                   \tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\nE                   \tat org.apache.spark.scheduler.Task.run(Task.scala:147)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\nE                   \tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\nE                   \tat java.base/java.lang.Thread.run(Thread.java:840)\nE                   \nE                   Driver stacktrace:\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$3(DAGScheduler.scala:3122)\nE                   \tat scala.Option.getOrElse(Option.scala:201)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:3122)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:3114)\nE                   \tat scala.collection.immutable.List.foreach(List.scala:323)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:3114)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1303)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1303)\nE                   \tat scala.Option.foreach(Option.scala:437)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1303)\nE                   \tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3397)\nE                   \tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3328)\nE                   \tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3317)\nE                   \tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:50)\nE                   Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\nE                       check_python_version(infile)\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\nE                       raise PySparkRuntimeError(\nE                   pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nE                   Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\nE                   \nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\nE                   \tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\nE                   \tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\nE                   \tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\nE                   \tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\nE                   \tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\nE                   \tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\nE                   \tat org.apache.spark.scheduler.Task.run(Task.scala:147)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\nE                   \tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\nE                   \tat java.base/java.lang.Thread.run(Thread.java:840)\n\nvenv/lib/python3.14/site-packages/py4j/protocol.py:327: Py4JJavaError\n\n----------------------------- Captured stderr call -----------------------------\n26/01/22 12:17:21 ERROR Executor: Exception in task 0.0 in stage 3.0 (TID 3)\norg.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\n    check_python_version(infile)\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\n    raise PySparkRuntimeError(\npyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nPlease check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n26/01/22 12:17:21 WARN TaskSetManager: Lost task 0.0 in stage 3.0 (TID 3) (10.20.80.191 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\n    check_python_version(infile)\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\n    raise PySparkRuntimeError(\npyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nPlease check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n\n26/01/22 12:17:21 ERROR TaskSetManager: Task 0 in stage 3.0 failed 1 times; aborting job\n&#34;}], &#34;tests/test_agents_transform.py::test_error_when_id_is_null&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/test_agents_transform.py::test_error_when_id_is_null&#34;, &#34;duration&#34;: &#34;232 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/test_agents_transform.py::test_error_when_id_is_null&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;232 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;spark = &amp;lt;pyspark.sql.session.SparkSession object at 0x10ca1de80&amp;gt;\n\n    def test_error_when_id_is_null(spark):\n        df = agent_details_df(\n            spark,\n            [make_agent(agent_id=None)]\n        )\n    \n        final_df, error_df = transform_agents(df, agents_df(spark, []))\n    \n&amp;gt;       assert final_df.count() == 0\n               ^^^^^^^^^^^^^^^^\n\ntests/test_agents_transform.py:162: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \nvenv/lib/python3.14/site-packages/pyspark/sql/classic/dataframe.py:439: in count\n    return int(self._jdf.count())\n               ^^^^^^^^^^^^^^^^^\nvenv/lib/python3.14/site-packages/py4j/java_gateway.py:1362: in __call__\n    return_value = get_return_value(\nvenv/lib/python3.14/site-packages/pyspark/errors/exceptions/captured.py:263: in deco\n    return f(*a, **kw)\n           ^^^^^^^^^^^\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nanswer = &amp;#x27;xro1471&amp;#x27;\ngateway_client = &amp;lt;py4j.clientserver.JavaClient object at 0x10ca1c6e0&amp;gt;\ntarget_id = &amp;#x27;o1470&amp;#x27;, name = &amp;#x27;count&amp;#x27;\n\n    def get_return_value(answer, gateway_client, target_id=None, name=None):\n        &amp;quot;&amp;quot;&amp;quot;Converts an answer received from the Java gateway into a Python object.\n    \n        For example, string representation of integers are converted to Python\n        integer, string representation of objects are converted to JavaObject\n        instances, etc.\n    \n        :param answer: the string returned by the Java gateway\n        :param gateway_client: the gateway client used to communicate with the Java\n            Gateway. Only necessary if the answer is a reference (e.g., object,\n            list, map)\n        :param target_id: the name of the object from which the answer comes from\n            (e.g., *object1* in `object1.hello()`). Optional.\n        :param name: the name of the member from which the answer comes from\n            (e.g., *hello* in `object1.hello()`). Optional.\n        &amp;quot;&amp;quot;&amp;quot;\n        if is_error(answer)[0]:\n            if len(answer) &amp;gt; 1:\n                type = answer[1]\n                value = OUTPUT_CONVERTER[type](answer[2:], gateway_client)\n                if answer[1] == REFERENCE_TYPE:\n&amp;gt;                   raise Py4JJavaError(\n                        &amp;quot;An error occurred while calling {0}{1}{2}.\\n&amp;quot;.\n                        format(target_id, &amp;quot;.&amp;quot;, name), value)\nE                   py4j.protocol.Py4JJavaError: An error occurred while calling o1470.count.\nE                   : org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 4.0 failed 1 times, most recent failure: Lost task 0.0 in stage 4.0 (TID 4) (10.20.80.191 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\nE                       check_python_version(infile)\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\nE                       raise PySparkRuntimeError(\nE                   pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nE                   Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\nE                   \nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\nE                   \tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\nE                   \tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\nE                   \tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\nE                   \tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\nE                   \tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\nE                   \tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\nE                   \tat org.apache.spark.scheduler.Task.run(Task.scala:147)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\nE                   \tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\nE                   \tat java.base/java.lang.Thread.run(Thread.java:840)\nE                   \nE                   Driver stacktrace:\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$3(DAGScheduler.scala:3122)\nE                   \tat scala.Option.getOrElse(Option.scala:201)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:3122)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:3114)\nE                   \tat scala.collection.immutable.List.foreach(List.scala:323)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:3114)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1303)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1303)\nE                   \tat scala.Option.foreach(Option.scala:437)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1303)\nE                   \tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3397)\nE                   \tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3328)\nE                   \tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3317)\nE                   \tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:50)\nE                   Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\nE                       check_python_version(infile)\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\nE                       raise PySparkRuntimeError(\nE                   pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nE                   Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\nE                   \nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\nE                   \tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\nE                   \tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\nE                   \tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\nE                   \tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\nE                   \tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\nE                   \tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\nE                   \tat org.apache.spark.scheduler.Task.run(Task.scala:147)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\nE                   \tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\nE                   \tat java.base/java.lang.Thread.run(Thread.java:840)\n\nvenv/lib/python3.14/site-packages/py4j/protocol.py:327: Py4JJavaError\n\n----------------------------- Captured stderr call -----------------------------\n26/01/22 12:17:21 ERROR Executor: Exception in task 0.0 in stage 4.0 (TID 4)\norg.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\n    check_python_version(infile)\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\n    raise PySparkRuntimeError(\npyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nPlease check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n26/01/22 12:17:21 WARN TaskSetManager: Lost task 0.0 in stage 4.0 (TID 4) (10.20.80.191 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\n    check_python_version(infile)\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\n    raise PySparkRuntimeError(\npyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nPlease check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n\n26/01/22 12:17:21 ERROR TaskSetManager: Task 0 in stage 4.0 failed 1 times; aborting job\n&#34;}], &#34;tests/test_agents_transform.py::test_error_when_email_is_null&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/test_agents_transform.py::test_error_when_email_is_null&#34;, &#34;duration&#34;: &#34;218 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/test_agents_transform.py::test_error_when_email_is_null&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;218 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;spark = &amp;lt;pyspark.sql.session.SparkSession object at 0x10ca1de80&amp;gt;\n\n    def test_error_when_email_is_null(spark):\n        df = agent_details_df(\n            spark,\n            [make_agent(agent_id=4, email=None)]\n        )\n    \n        _, error_df = transform_agents(df, agents_df(spark, []))\n&amp;gt;       assert error_df.count() == 1\n               ^^^^^^^^^^^^^^^^\n\ntests/test_agents_transform.py:177: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \nvenv/lib/python3.14/site-packages/pyspark/sql/classic/dataframe.py:439: in count\n    return int(self._jdf.count())\n               ^^^^^^^^^^^^^^^^^\nvenv/lib/python3.14/site-packages/py4j/java_gateway.py:1362: in __call__\n    return_value = get_return_value(\nvenv/lib/python3.14/site-packages/pyspark/errors/exceptions/captured.py:263: in deco\n    return f(*a, **kw)\n           ^^^^^^^^^^^\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nanswer = &amp;#x27;xro1745&amp;#x27;\ngateway_client = &amp;lt;py4j.clientserver.JavaClient object at 0x10ca1c6e0&amp;gt;\ntarget_id = &amp;#x27;o1639&amp;#x27;, name = &amp;#x27;count&amp;#x27;\n\n    def get_return_value(answer, gateway_client, target_id=None, name=None):\n        &amp;quot;&amp;quot;&amp;quot;Converts an answer received from the Java gateway into a Python object.\n    \n        For example, string representation of integers are converted to Python\n        integer, string representation of objects are converted to JavaObject\n        instances, etc.\n    \n        :param answer: the string returned by the Java gateway\n        :param gateway_client: the gateway client used to communicate with the Java\n            Gateway. Only necessary if the answer is a reference (e.g., object,\n            list, map)\n        :param target_id: the name of the object from which the answer comes from\n            (e.g., *object1* in `object1.hello()`). Optional.\n        :param name: the name of the member from which the answer comes from\n            (e.g., *hello* in `object1.hello()`). Optional.\n        &amp;quot;&amp;quot;&amp;quot;\n        if is_error(answer)[0]:\n            if len(answer) &amp;gt; 1:\n                type = answer[1]\n                value = OUTPUT_CONVERTER[type](answer[2:], gateway_client)\n                if answer[1] == REFERENCE_TYPE:\n&amp;gt;                   raise Py4JJavaError(\n                        &amp;quot;An error occurred while calling {0}{1}{2}.\\n&amp;quot;.\n                        format(target_id, &amp;quot;.&amp;quot;, name), value)\nE                   py4j.protocol.Py4JJavaError: An error occurred while calling o1639.count.\nE                   : org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 5.0 failed 1 times, most recent failure: Lost task 0.0 in stage 5.0 (TID 5) (10.20.80.191 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\nE                       check_python_version(infile)\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\nE                       raise PySparkRuntimeError(\nE                   pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nE                   Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\nE                   \nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\nE                   \tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\nE                   \tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\nE                   \tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\nE                   \tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\nE                   \tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\nE                   \tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\nE                   \tat org.apache.spark.scheduler.Task.run(Task.scala:147)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\nE                   \tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\nE                   \tat java.base/java.lang.Thread.run(Thread.java:840)\nE                   \nE                   Driver stacktrace:\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$3(DAGScheduler.scala:3122)\nE                   \tat scala.Option.getOrElse(Option.scala:201)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:3122)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:3114)\nE                   \tat scala.collection.immutable.List.foreach(List.scala:323)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:3114)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1303)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1303)\nE                   \tat scala.Option.foreach(Option.scala:437)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1303)\nE                   \tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3397)\nE                   \tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3328)\nE                   \tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3317)\nE                   \tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:50)\nE                   Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\nE                       check_python_version(infile)\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\nE                       raise PySparkRuntimeError(\nE                   pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nE                   Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\nE                   \nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\nE                   \tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\nE                   \tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\nE                   \tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\nE                   \tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\nE                   \tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\nE                   \tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\nE                   \tat org.apache.spark.scheduler.Task.run(Task.scala:147)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\nE                   \tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\nE                   \tat java.base/java.lang.Thread.run(Thread.java:840)\n\nvenv/lib/python3.14/site-packages/py4j/protocol.py:327: Py4JJavaError\n\n----------------------------- Captured stderr call -----------------------------\n26/01/22 12:17:21 ERROR Executor: Exception in task 0.0 in stage 5.0 (TID 5)\norg.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\n    check_python_version(infile)\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\n    raise PySparkRuntimeError(\npyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nPlease check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n26/01/22 12:17:21 WARN TaskSetManager: Lost task 0.0 in stage 5.0 (TID 5) (10.20.80.191 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\n    check_python_version(infile)\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\n    raise PySparkRuntimeError(\npyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nPlease check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n\n26/01/22 12:17:21 ERROR TaskSetManager: Task 0 in stage 5.0 failed 1 times; aborting job\n&#34;}], &#34;tests/test_agents_transform.py::test_error_when_name_is_null&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/test_agents_transform.py::test_error_when_name_is_null&#34;, &#34;duration&#34;: &#34;197 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/test_agents_transform.py::test_error_when_name_is_null&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;197 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;spark = &amp;lt;pyspark.sql.session.SparkSession object at 0x10ca1de80&amp;gt;\n\n    def test_error_when_name_is_null(spark):\n        df = agent_details_df(\n            spark,\n            [make_agent(agent_id=5, name=None)]\n        )\n    \n        _, error_df = transform_agents(df, agents_df(spark, []))\n&amp;gt;       assert error_df.count() == 1\n               ^^^^^^^^^^^^^^^^\n\ntests/test_agents_transform.py:190: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \nvenv/lib/python3.14/site-packages/pyspark/sql/classic/dataframe.py:439: in count\n    return int(self._jdf.count())\n               ^^^^^^^^^^^^^^^^^\nvenv/lib/python3.14/site-packages/py4j/java_gateway.py:1362: in __call__\n    return_value = get_return_value(\nvenv/lib/python3.14/site-packages/pyspark/errors/exceptions/captured.py:263: in deco\n    return f(*a, **kw)\n           ^^^^^^^^^^^\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nanswer = &amp;#x27;xro2019&amp;#x27;\ngateway_client = &amp;lt;py4j.clientserver.JavaClient object at 0x10ca1c6e0&amp;gt;\ntarget_id = &amp;#x27;o1913&amp;#x27;, name = &amp;#x27;count&amp;#x27;\n\n    def get_return_value(answer, gateway_client, target_id=None, name=None):\n        &amp;quot;&amp;quot;&amp;quot;Converts an answer received from the Java gateway into a Python object.\n    \n        For example, string representation of integers are converted to Python\n        integer, string representation of objects are converted to JavaObject\n        instances, etc.\n    \n        :param answer: the string returned by the Java gateway\n        :param gateway_client: the gateway client used to communicate with the Java\n            Gateway. Only necessary if the answer is a reference (e.g., object,\n            list, map)\n        :param target_id: the name of the object from which the answer comes from\n            (e.g., *object1* in `object1.hello()`). Optional.\n        :param name: the name of the member from which the answer comes from\n            (e.g., *hello* in `object1.hello()`). Optional.\n        &amp;quot;&amp;quot;&amp;quot;\n        if is_error(answer)[0]:\n            if len(answer) &amp;gt; 1:\n                type = answer[1]\n                value = OUTPUT_CONVERTER[type](answer[2:], gateway_client)\n                if answer[1] == REFERENCE_TYPE:\n&amp;gt;                   raise Py4JJavaError(\n                        &amp;quot;An error occurred while calling {0}{1}{2}.\\n&amp;quot;.\n                        format(target_id, &amp;quot;.&amp;quot;, name), value)\nE                   py4j.protocol.Py4JJavaError: An error occurred while calling o1913.count.\nE                   : org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 6.0 failed 1 times, most recent failure: Lost task 0.0 in stage 6.0 (TID 6) (10.20.80.191 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\nE                       check_python_version(infile)\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\nE                       raise PySparkRuntimeError(\nE                   pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nE                   Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\nE                   \nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\nE                   \tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\nE                   \tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\nE                   \tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\nE                   \tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\nE                   \tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\nE                   \tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\nE                   \tat org.apache.spark.scheduler.Task.run(Task.scala:147)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\nE                   \tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\nE                   \tat java.base/java.lang.Thread.run(Thread.java:840)\nE                   \nE                   Driver stacktrace:\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$3(DAGScheduler.scala:3122)\nE                   \tat scala.Option.getOrElse(Option.scala:201)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:3122)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:3114)\nE                   \tat scala.collection.immutable.List.foreach(List.scala:323)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:3114)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1303)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1303)\nE                   \tat scala.Option.foreach(Option.scala:437)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1303)\nE                   \tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3397)\nE                   \tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3328)\nE                   \tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3317)\nE                   \tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:50)\nE                   Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\nE                       check_python_version(infile)\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\nE                       raise PySparkRuntimeError(\nE                   pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nE                   Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\nE                   \nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\nE                   \tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\nE                   \tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\nE                   \tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\nE                   \tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\nE                   \tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\nE                   \tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\nE                   \tat org.apache.spark.scheduler.Task.run(Task.scala:147)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\nE                   \tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\nE                   \tat java.base/java.lang.Thread.run(Thread.java:840)\n\nvenv/lib/python3.14/site-packages/py4j/protocol.py:327: Py4JJavaError\n\n----------------------------- Captured stderr call -----------------------------\n26/01/22 12:17:21 ERROR Executor: Exception in task 0.0 in stage 6.0 (TID 6)\norg.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\n    check_python_version(infile)\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\n    raise PySparkRuntimeError(\npyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nPlease check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n26/01/22 12:17:21 WARN TaskSetManager: Lost task 0.0 in stage 6.0 (TID 6) (10.20.80.191 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\n    check_python_version(infile)\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\n    raise PySparkRuntimeError(\npyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nPlease check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n\n26/01/22 12:17:21 ERROR TaskSetManager: Task 0 in stage 6.0 failed 1 times; aborting job\n&#34;}], &#34;tests/test_agents_transform.py::test_empty_input&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/test_agents_transform.py::test_empty_input&#34;, &#34;duration&#34;: &#34;194 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/test_agents_transform.py::test_empty_input&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;194 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;spark = &amp;lt;pyspark.sql.session.SparkSession object at 0x10ca1de80&amp;gt;\n\n    def test_empty_input(spark):\n        empty_df = spark.createDataFrame([], schema=agent_schema)\n        final_df, error_df = transform_agents(empty_df, agents_df(spark, []))\n    \n&amp;gt;       assert final_df.count() == 0\n               ^^^^^^^^^^^^^^^^\n\ntests/test_agents_transform.py:200: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \nvenv/lib/python3.14/site-packages/pyspark/sql/classic/dataframe.py:439: in count\n    return int(self._jdf.count())\n               ^^^^^^^^^^^^^^^^^\nvenv/lib/python3.14/site-packages/py4j/java_gateway.py:1362: in __call__\n    return_value = get_return_value(\nvenv/lib/python3.14/site-packages/pyspark/errors/exceptions/captured.py:263: in deco\n    return f(*a, **kw)\n           ^^^^^^^^^^^\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nanswer = &amp;#x27;xro2293&amp;#x27;\ngateway_client = &amp;lt;py4j.clientserver.JavaClient object at 0x10ca1c6e0&amp;gt;\ntarget_id = &amp;#x27;o2292&amp;#x27;, name = &amp;#x27;count&amp;#x27;\n\n    def get_return_value(answer, gateway_client, target_id=None, name=None):\n        &amp;quot;&amp;quot;&amp;quot;Converts an answer received from the Java gateway into a Python object.\n    \n        For example, string representation of integers are converted to Python\n        integer, string representation of objects are converted to JavaObject\n        instances, etc.\n    \n        :param answer: the string returned by the Java gateway\n        :param gateway_client: the gateway client used to communicate with the Java\n            Gateway. Only necessary if the answer is a reference (e.g., object,\n            list, map)\n        :param target_id: the name of the object from which the answer comes from\n            (e.g., *object1* in `object1.hello()`). Optional.\n        :param name: the name of the member from which the answer comes from\n            (e.g., *hello* in `object1.hello()`). Optional.\n        &amp;quot;&amp;quot;&amp;quot;\n        if is_error(answer)[0]:\n            if len(answer) &amp;gt; 1:\n                type = answer[1]\n                value = OUTPUT_CONVERTER[type](answer[2:], gateway_client)\n                if answer[1] == REFERENCE_TYPE:\n&amp;gt;                   raise Py4JJavaError(\n                        &amp;quot;An error occurred while calling {0}{1}{2}.\\n&amp;quot;.\n                        format(target_id, &amp;quot;.&amp;quot;, name), value)\nE                   py4j.protocol.Py4JJavaError: An error occurred while calling o2292.count.\nE                   : org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 7.0 failed 1 times, most recent failure: Lost task 0.0 in stage 7.0 (TID 7) (10.20.80.191 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\nE                       check_python_version(infile)\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\nE                       raise PySparkRuntimeError(\nE                   pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nE                   Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\nE                   \nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\nE                   \tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\nE                   \tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\nE                   \tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\nE                   \tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\nE                   \tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\nE                   \tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\nE                   \tat org.apache.spark.scheduler.Task.run(Task.scala:147)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\nE                   \tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\nE                   \tat java.base/java.lang.Thread.run(Thread.java:840)\nE                   \nE                   Driver stacktrace:\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$3(DAGScheduler.scala:3122)\nE                   \tat scala.Option.getOrElse(Option.scala:201)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:3122)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:3114)\nE                   \tat scala.collection.immutable.List.foreach(List.scala:323)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:3114)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1303)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1303)\nE                   \tat scala.Option.foreach(Option.scala:437)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1303)\nE                   \tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3397)\nE                   \tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3328)\nE                   \tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3317)\nE                   \tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:50)\nE                   Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\nE                       check_python_version(infile)\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\nE                       raise PySparkRuntimeError(\nE                   pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nE                   Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\nE                   \nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\nE                   \tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\nE                   \tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\nE                   \tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\nE                   \tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\nE                   \tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\nE                   \tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\nE                   \tat org.apache.spark.scheduler.Task.run(Task.scala:147)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\nE                   \tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\nE                   \tat java.base/java.lang.Thread.run(Thread.java:840)\n\nvenv/lib/python3.14/site-packages/py4j/protocol.py:327: Py4JJavaError\n\n----------------------------- Captured stderr call -----------------------------\n26/01/22 12:17:22 ERROR Executor: Exception in task 0.0 in stage 7.0 (TID 7)\norg.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\n    check_python_version(infile)\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\n    raise PySparkRuntimeError(\npyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nPlease check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n26/01/22 12:17:22 WARN TaskSetManager: Lost task 0.0 in stage 7.0 (TID 7) (10.20.80.191 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\n    check_python_version(infile)\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\n    raise PySparkRuntimeError(\npyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nPlease check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n\n26/01/22 12:17:22 ERROR TaskSetManager: Task 0 in stage 7.0 failed 1 times; aborting job\n&#34;}], &#34;tests/test_agents_transform.py::test_error_when_id_is_non_numeric&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/test_agents_transform.py::test_error_when_id_is_non_numeric&#34;, &#34;duration&#34;: &#34;260 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/test_agents_transform.py::test_error_when_id_is_non_numeric&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;260 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;spark = &amp;lt;pyspark.sql.session.SparkSession object at 0x10ca1de80&amp;gt;\n\n    def test_error_when_id_is_non_numeric(spark):\n        df = spark.createDataFrame(\n            [Row(id=&amp;quot;abc&amp;quot;, contact=make_agent().contact)],\n            schema=StructType([\n                StructField(&amp;quot;id&amp;quot;, StringType(), True),\n                StructField(&amp;quot;contact&amp;quot;, contact_schema, True),\n            ])\n        )\n    \n        final_df, error_df = transform_agents(df, agents_df(spark, []))\n    \n&amp;gt;       assert final_df.count() == 0\n               ^^^^^^^^^^^^^^^^\n\ntests/test_agents_transform.py:219: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \nvenv/lib/python3.14/site-packages/pyspark/sql/classic/dataframe.py:439: in count\n    return int(self._jdf.count())\n               ^^^^^^^^^^^^^^^^^\nvenv/lib/python3.14/site-packages/py4j/java_gateway.py:1362: in __call__\n    return_value = get_return_value(\nvenv/lib/python3.14/site-packages/pyspark/errors/exceptions/captured.py:263: in deco\n    return f(*a, **kw)\n           ^^^^^^^^^^^\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nanswer = &amp;#x27;xro2570&amp;#x27;\ngateway_client = &amp;lt;py4j.clientserver.JavaClient object at 0x10ca1c6e0&amp;gt;\ntarget_id = &amp;#x27;o2569&amp;#x27;, name = &amp;#x27;count&amp;#x27;\n\n    def get_return_value(answer, gateway_client, target_id=None, name=None):\n        &amp;quot;&amp;quot;&amp;quot;Converts an answer received from the Java gateway into a Python object.\n    \n        For example, string representation of integers are converted to Python\n        integer, string representation of objects are converted to JavaObject\n        instances, etc.\n    \n        :param answer: the string returned by the Java gateway\n        :param gateway_client: the gateway client used to communicate with the Java\n            Gateway. Only necessary if the answer is a reference (e.g., object,\n            list, map)\n        :param target_id: the name of the object from which the answer comes from\n            (e.g., *object1* in `object1.hello()`). Optional.\n        :param name: the name of the member from which the answer comes from\n            (e.g., *hello* in `object1.hello()`). Optional.\n        &amp;quot;&amp;quot;&amp;quot;\n        if is_error(answer)[0]:\n            if len(answer) &amp;gt; 1:\n                type = answer[1]\n                value = OUTPUT_CONVERTER[type](answer[2:], gateway_client)\n                if answer[1] == REFERENCE_TYPE:\n&amp;gt;                   raise Py4JJavaError(\n                        &amp;quot;An error occurred while calling {0}{1}{2}.\\n&amp;quot;.\n                        format(target_id, &amp;quot;.&amp;quot;, name), value)\nE                   py4j.protocol.Py4JJavaError: An error occurred while calling o2569.count.\nE                   : org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 8.0 failed 1 times, most recent failure: Lost task 0.0 in stage 8.0 (TID 8) (10.20.80.191 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\nE                       check_python_version(infile)\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\nE                       raise PySparkRuntimeError(\nE                   pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nE                   Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\nE                   \nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\nE                   \tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\nE                   \tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\nE                   \tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\nE                   \tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\nE                   \tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\nE                   \tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\nE                   \tat org.apache.spark.scheduler.Task.run(Task.scala:147)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\nE                   \tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\nE                   \tat java.base/java.lang.Thread.run(Thread.java:840)\nE                   \nE                   Driver stacktrace:\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$3(DAGScheduler.scala:3122)\nE                   \tat scala.Option.getOrElse(Option.scala:201)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:3122)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:3114)\nE                   \tat scala.collection.immutable.List.foreach(List.scala:323)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:3114)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1303)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1303)\nE                   \tat scala.Option.foreach(Option.scala:437)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1303)\nE                   \tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3397)\nE                   \tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3328)\nE                   \tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3317)\nE                   \tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:50)\nE                   Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\nE                       check_python_version(infile)\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\nE                       raise PySparkRuntimeError(\nE                   pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nE                   Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\nE                   \nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\nE                   \tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\nE                   \tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\nE                   \tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\nE                   \tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\nE                   \tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\nE                   \tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\nE                   \tat org.apache.spark.scheduler.Task.run(Task.scala:147)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\nE                   \tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\nE                   \tat java.base/java.lang.Thread.run(Thread.java:840)\n\nvenv/lib/python3.14/site-packages/py4j/protocol.py:327: Py4JJavaError\n\n----------------------------- Captured stderr call -----------------------------\n26/01/22 12:17:22 ERROR Executor: Exception in task 0.0 in stage 8.0 (TID 8)\norg.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\n    check_python_version(infile)\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\n    raise PySparkRuntimeError(\npyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nPlease check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n26/01/22 12:17:22 WARN TaskSetManager: Lost task 0.0 in stage 8.0 (TID 8) (10.20.80.191 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\n    check_python_version(infile)\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\n    raise PySparkRuntimeError(\npyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nPlease check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n\n26/01/22 12:17:22 ERROR TaskSetManager: Task 0 in stage 8.0 failed 1 times; aborting job\n&#34;}], &#34;tests/test_agents_transform.py::test_extra_fields_are_ignored&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/test_agents_transform.py::test_extra_fields_are_ignored&#34;, &#34;duration&#34;: &#34;249 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/test_agents_transform.py::test_extra_fields_are_ignored&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;249 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;spark = &amp;lt;pyspark.sql.session.SparkSession object at 0x10ca1de80&amp;gt;\n\n    def test_extra_fields_are_ignored(spark):\n        df = spark.createDataFrame(\n            [\n                Row(\n                    id=20,\n                    contact=make_agent().contact,\n                    random_field=&amp;quot;junk&amp;quot;\n                )\n            ],\n            schema=StructType([\n                StructField(&amp;quot;id&amp;quot;, LongType(), True),\n                StructField(&amp;quot;contact&amp;quot;, contact_schema, True),\n                StructField(&amp;quot;random_field&amp;quot;, StringType(), True),\n            ])\n        )\n    \n        final_df, error_df = transform_agents(df, agents_df(spark, [20]))\n    \n&amp;gt;       assert final_df.count() == 1\n               ^^^^^^^^^^^^^^^^\n\ntests/test_agents_transform.py:245: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \nvenv/lib/python3.14/site-packages/pyspark/sql/classic/dataframe.py:439: in count\n    return int(self._jdf.count())\n               ^^^^^^^^^^^^^^^^^\nvenv/lib/python3.14/site-packages/py4j/java_gateway.py:1362: in __call__\n    return_value = get_return_value(\nvenv/lib/python3.14/site-packages/pyspark/errors/exceptions/captured.py:263: in deco\n    return f(*a, **kw)\n           ^^^^^^^^^^^\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nanswer = &amp;#x27;xro2847&amp;#x27;\ngateway_client = &amp;lt;py4j.clientserver.JavaClient object at 0x10ca1c6e0&amp;gt;\ntarget_id = &amp;#x27;o2846&amp;#x27;, name = &amp;#x27;count&amp;#x27;\n\n    def get_return_value(answer, gateway_client, target_id=None, name=None):\n        &amp;quot;&amp;quot;&amp;quot;Converts an answer received from the Java gateway into a Python object.\n    \n        For example, string representation of integers are converted to Python\n        integer, string representation of objects are converted to JavaObject\n        instances, etc.\n    \n        :param answer: the string returned by the Java gateway\n        :param gateway_client: the gateway client used to communicate with the Java\n            Gateway. Only necessary if the answer is a reference (e.g., object,\n            list, map)\n        :param target_id: the name of the object from which the answer comes from\n            (e.g., *object1* in `object1.hello()`). Optional.\n        :param name: the name of the member from which the answer comes from\n            (e.g., *hello* in `object1.hello()`). Optional.\n        &amp;quot;&amp;quot;&amp;quot;\n        if is_error(answer)[0]:\n            if len(answer) &amp;gt; 1:\n                type = answer[1]\n                value = OUTPUT_CONVERTER[type](answer[2:], gateway_client)\n                if answer[1] == REFERENCE_TYPE:\n&amp;gt;                   raise Py4JJavaError(\n                        &amp;quot;An error occurred while calling {0}{1}{2}.\\n&amp;quot;.\n                        format(target_id, &amp;quot;.&amp;quot;, name), value)\nE                   py4j.protocol.Py4JJavaError: An error occurred while calling o2846.count.\nE                   : org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 9.0 failed 1 times, most recent failure: Lost task 0.0 in stage 9.0 (TID 9) (10.20.80.191 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\nE                       check_python_version(infile)\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\nE                       raise PySparkRuntimeError(\nE                   pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nE                   Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\nE                   \nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\nE                   \tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\nE                   \tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\nE                   \tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\nE                   \tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\nE                   \tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\nE                   \tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\nE                   \tat org.apache.spark.scheduler.Task.run(Task.scala:147)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\nE                   \tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\nE                   \tat java.base/java.lang.Thread.run(Thread.java:840)\nE                   \nE                   Driver stacktrace:\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$3(DAGScheduler.scala:3122)\nE                   \tat scala.Option.getOrElse(Option.scala:201)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:3122)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:3114)\nE                   \tat scala.collection.immutable.List.foreach(List.scala:323)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:3114)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1303)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1303)\nE                   \tat scala.Option.foreach(Option.scala:437)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1303)\nE                   \tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3397)\nE                   \tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3328)\nE                   \tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3317)\nE                   \tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:50)\nE                   Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\nE                       check_python_version(infile)\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\nE                       raise PySparkRuntimeError(\nE                   pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nE                   Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\nE                   \nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\nE                   \tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\nE                   \tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\nE                   \tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\nE                   \tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\nE                   \tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\nE                   \tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\nE                   \tat org.apache.spark.scheduler.Task.run(Task.scala:147)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\nE                   \tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\nE                   \tat java.base/java.lang.Thread.run(Thread.java:840)\n\nvenv/lib/python3.14/site-packages/py4j/protocol.py:327: Py4JJavaError\n\n----------------------------- Captured stderr call -----------------------------\n26/01/22 12:17:22 ERROR Executor: Exception in task 0.0 in stage 9.0 (TID 9)\norg.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\n    check_python_version(infile)\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\n    raise PySparkRuntimeError(\npyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nPlease check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n26/01/22 12:17:22 WARN TaskSetManager: Lost task 0.0 in stage 9.0 (TID 9) (10.20.80.191 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\n    check_python_version(infile)\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\n    raise PySparkRuntimeError(\npyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nPlease check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n\n26/01/22 12:17:22 ERROR TaskSetManager: Task 0 in stage 9.0 failed 1 times; aborting job\n&#34;}], &#34;tests/test_agents_transform.py::test_optional_field_type_mismatch_does_not_fail&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/test_agents_transform.py::test_optional_field_type_mismatch_does_not_fail&#34;, &#34;duration&#34;: &#34;195 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/test_agents_transform.py::test_optional_field_type_mismatch_does_not_fail&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;195 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;spark = &amp;lt;pyspark.sql.session.SparkSession object at 0x10ca1de80&amp;gt;\n\n    def test_optional_field_type_mismatch_does_not_fail(spark):\n        df = spark.createDataFrame(\n            [\n                Row(\n                    id=30,\n                    contact=make_agent().contact,\n                    available=&amp;quot;yes&amp;quot;\n                )\n            ],\n            schema=StructType([\n                StructField(&amp;quot;id&amp;quot;, LongType(), True),\n                StructField(&amp;quot;contact&amp;quot;, contact_schema, True),\n                StructField(&amp;quot;available&amp;quot;, StringType(), True),\n            ])\n        )\n    \n        final_df, error_df = transform_agents(df, agents_df(spark, [30]))\n    \n&amp;gt;       assert final_df.count() == 1\n               ^^^^^^^^^^^^^^^^\n\ntests/test_agents_transform.py:269: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \nvenv/lib/python3.14/site-packages/pyspark/sql/classic/dataframe.py:439: in count\n    return int(self._jdf.count())\n               ^^^^^^^^^^^^^^^^^\nvenv/lib/python3.14/site-packages/py4j/java_gateway.py:1362: in __call__\n    return_value = get_return_value(\nvenv/lib/python3.14/site-packages/pyspark/errors/exceptions/captured.py:263: in deco\n    return f(*a, **kw)\n           ^^^^^^^^^^^\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nanswer = &amp;#x27;xro3121&amp;#x27;\ngateway_client = &amp;lt;py4j.clientserver.JavaClient object at 0x10ca1c6e0&amp;gt;\ntarget_id = &amp;#x27;o3120&amp;#x27;, name = &amp;#x27;count&amp;#x27;\n\n    def get_return_value(answer, gateway_client, target_id=None, name=None):\n        &amp;quot;&amp;quot;&amp;quot;Converts an answer received from the Java gateway into a Python object.\n    \n        For example, string representation of integers are converted to Python\n        integer, string representation of objects are converted to JavaObject\n        instances, etc.\n    \n        :param answer: the string returned by the Java gateway\n        :param gateway_client: the gateway client used to communicate with the Java\n            Gateway. Only necessary if the answer is a reference (e.g., object,\n            list, map)\n        :param target_id: the name of the object from which the answer comes from\n            (e.g., *object1* in `object1.hello()`). Optional.\n        :param name: the name of the member from which the answer comes from\n            (e.g., *hello* in `object1.hello()`). Optional.\n        &amp;quot;&amp;quot;&amp;quot;\n        if is_error(answer)[0]:\n            if len(answer) &amp;gt; 1:\n                type = answer[1]\n                value = OUTPUT_CONVERTER[type](answer[2:], gateway_client)\n                if answer[1] == REFERENCE_TYPE:\n&amp;gt;                   raise Py4JJavaError(\n                        &amp;quot;An error occurred while calling {0}{1}{2}.\\n&amp;quot;.\n                        format(target_id, &amp;quot;.&amp;quot;, name), value)\nE                   py4j.protocol.Py4JJavaError: An error occurred while calling o3120.count.\nE                   : org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 10.0 failed 1 times, most recent failure: Lost task 0.0 in stage 10.0 (TID 10) (10.20.80.191 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\nE                       check_python_version(infile)\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\nE                       raise PySparkRuntimeError(\nE                   pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nE                   Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\nE                   \nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\nE                   \tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\nE                   \tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\nE                   \tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\nE                   \tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\nE                   \tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\nE                   \tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\nE                   \tat org.apache.spark.scheduler.Task.run(Task.scala:147)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\nE                   \tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\nE                   \tat java.base/java.lang.Thread.run(Thread.java:840)\nE                   \nE                   Driver stacktrace:\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$3(DAGScheduler.scala:3122)\nE                   \tat scala.Option.getOrElse(Option.scala:201)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:3122)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:3114)\nE                   \tat scala.collection.immutable.List.foreach(List.scala:323)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:3114)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1303)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1303)\nE                   \tat scala.Option.foreach(Option.scala:437)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1303)\nE                   \tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3397)\nE                   \tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3328)\nE                   \tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3317)\nE                   \tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:50)\nE                   Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\nE                       check_python_version(infile)\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\nE                       raise PySparkRuntimeError(\nE                   pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nE                   Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\nE                   \nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\nE                   \tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\nE                   \tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\nE                   \tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\nE                   \tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\nE                   \tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\nE                   \tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\nE                   \tat org.apache.spark.scheduler.Task.run(Task.scala:147)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\nE                   \tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\nE                   \tat java.base/java.lang.Thread.run(Thread.java:840)\n\nvenv/lib/python3.14/site-packages/py4j/protocol.py:327: Py4JJavaError\n\n----------------------------- Captured stderr call -----------------------------\n26/01/22 12:17:22 ERROR Executor: Exception in task 0.0 in stage 10.0 (TID 10)\norg.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\n    check_python_version(infile)\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\n    raise PySparkRuntimeError(\npyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nPlease check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n26/01/22 12:17:22 WARN TaskSetManager: Lost task 0.0 in stage 10.0 (TID 10) (10.20.80.191 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\n    check_python_version(infile)\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\n    raise PySparkRuntimeError(\npyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nPlease check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n\n26/01/22 12:17:22 ERROR TaskSetManager: Task 0 in stage 10.0 failed 1 times; aborting job\n&#34;}], &#34;tests/test_agents_transform.py::test_safe_col_flat_column_present&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/test_agents_transform.py::test_safe_col_flat_column_present&#34;, &#34;duration&#34;: &#34;213 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/test_agents_transform.py::test_safe_col_flat_column_present&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;213 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;spark = &amp;lt;pyspark.sql.session.SparkSession object at 0x10ca1de80&amp;gt;\n\n    def test_safe_col_flat_column_present(spark):\n        df = spark.createDataFrame(\n            [\n                Row(\n                    id=40,\n                    contact=make_agent().contact,\n                    signature=&amp;quot;my-signature&amp;quot;\n                )\n            ],\n            schema=StructType([\n                StructField(&amp;quot;id&amp;quot;, LongType(), True),\n                StructField(&amp;quot;contact&amp;quot;, contact_schema, True),\n                StructField(&amp;quot;signature&amp;quot;, StringType(), True),\n            ])\n        )\n    \n        final_df, error_df = transform_agents(df, agents_df(spark, [40]))\n    \n&amp;gt;       assert final_df.count() == 1\n               ^^^^^^^^^^^^^^^^\n\ntests/test_agents_transform.py:293: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \nvenv/lib/python3.14/site-packages/pyspark/sql/classic/dataframe.py:439: in count\n    return int(self._jdf.count())\n               ^^^^^^^^^^^^^^^^^\nvenv/lib/python3.14/site-packages/py4j/java_gateway.py:1362: in __call__\n    return_value = get_return_value(\nvenv/lib/python3.14/site-packages/pyspark/errors/exceptions/captured.py:263: in deco\n    return f(*a, **kw)\n           ^^^^^^^^^^^\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nanswer = &amp;#x27;xro3395&amp;#x27;\ngateway_client = &amp;lt;py4j.clientserver.JavaClient object at 0x10ca1c6e0&amp;gt;\ntarget_id = &amp;#x27;o3394&amp;#x27;, name = &amp;#x27;count&amp;#x27;\n\n    def get_return_value(answer, gateway_client, target_id=None, name=None):\n        &amp;quot;&amp;quot;&amp;quot;Converts an answer received from the Java gateway into a Python object.\n    \n        For example, string representation of integers are converted to Python\n        integer, string representation of objects are converted to JavaObject\n        instances, etc.\n    \n        :param answer: the string returned by the Java gateway\n        :param gateway_client: the gateway client used to communicate with the Java\n            Gateway. Only necessary if the answer is a reference (e.g., object,\n            list, map)\n        :param target_id: the name of the object from which the answer comes from\n            (e.g., *object1* in `object1.hello()`). Optional.\n        :param name: the name of the member from which the answer comes from\n            (e.g., *hello* in `object1.hello()`). Optional.\n        &amp;quot;&amp;quot;&amp;quot;\n        if is_error(answer)[0]:\n            if len(answer) &amp;gt; 1:\n                type = answer[1]\n                value = OUTPUT_CONVERTER[type](answer[2:], gateway_client)\n                if answer[1] == REFERENCE_TYPE:\n&amp;gt;                   raise Py4JJavaError(\n                        &amp;quot;An error occurred while calling {0}{1}{2}.\\n&amp;quot;.\n                        format(target_id, &amp;quot;.&amp;quot;, name), value)\nE                   py4j.protocol.Py4JJavaError: An error occurred while calling o3394.count.\nE                   : org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 11.0 failed 1 times, most recent failure: Lost task 0.0 in stage 11.0 (TID 11) (10.20.80.191 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\nE                       check_python_version(infile)\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\nE                       raise PySparkRuntimeError(\nE                   pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nE                   Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\nE                   \nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\nE                   \tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\nE                   \tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\nE                   \tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\nE                   \tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\nE                   \tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\nE                   \tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\nE                   \tat org.apache.spark.scheduler.Task.run(Task.scala:147)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\nE                   \tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\nE                   \tat java.base/java.lang.Thread.run(Thread.java:840)\nE                   \nE                   Driver stacktrace:\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$3(DAGScheduler.scala:3122)\nE                   \tat scala.Option.getOrElse(Option.scala:201)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:3122)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:3114)\nE                   \tat scala.collection.immutable.List.foreach(List.scala:323)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:3114)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1303)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1303)\nE                   \tat scala.Option.foreach(Option.scala:437)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1303)\nE                   \tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3397)\nE                   \tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3328)\nE                   \tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3317)\nE                   \tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:50)\nE                   Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\nE                       check_python_version(infile)\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\nE                       raise PySparkRuntimeError(\nE                   pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nE                   Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\nE                   \nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\nE                   \tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\nE                   \tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\nE                   \tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\nE                   \tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\nE                   \tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\nE                   \tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\nE                   \tat org.apache.spark.scheduler.Task.run(Task.scala:147)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\nE                   \tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\nE                   \tat java.base/java.lang.Thread.run(Thread.java:840)\n\nvenv/lib/python3.14/site-packages/py4j/protocol.py:327: Py4JJavaError\n\n----------------------------- Captured stderr call -----------------------------\n26/01/22 12:17:23 ERROR Executor: Exception in task 0.0 in stage 11.0 (TID 11)\norg.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\n    check_python_version(infile)\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\n    raise PySparkRuntimeError(\npyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nPlease check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n26/01/22 12:17:23 WARN TaskSetManager: Lost task 0.0 in stage 11.0 (TID 11) (10.20.80.191 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\n    check_python_version(infile)\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\n    raise PySparkRuntimeError(\npyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nPlease check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n\n26/01/22 12:17:23 ERROR TaskSetManager: Task 0 in stage 11.0 failed 1 times; aborting job\n&#34;}], &#34;tests/test_agents_transform.py::test_safe_col_flat_column_missing&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/test_agents_transform.py::test_safe_col_flat_column_missing&#34;, &#34;duration&#34;: &#34;205 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/test_agents_transform.py::test_safe_col_flat_column_missing&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;205 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;spark = &amp;lt;pyspark.sql.session.SparkSession object at 0x10ca1de80&amp;gt;\n\n    def test_safe_col_flat_column_missing(spark):\n        df = spark.createDataFrame(\n            [\n                Row(\n                    id=41,\n                    contact=make_agent().contact\n                )\n            ],\n            schema=StructType([\n                StructField(&amp;quot;id&amp;quot;, LongType(), True),\n                StructField(&amp;quot;contact&amp;quot;, contact_schema, True),\n                # signature column missing\n            ])\n        )\n    \n        final_df, error_df = transform_agents(df, agents_df(spark, [41]))\n    \n&amp;gt;       assert final_df.count() == 1\n               ^^^^^^^^^^^^^^^^\n\ntests/test_agents_transform.py:318: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \nvenv/lib/python3.14/site-packages/pyspark/sql/classic/dataframe.py:439: in count\n    return int(self._jdf.count())\n               ^^^^^^^^^^^^^^^^^\nvenv/lib/python3.14/site-packages/py4j/java_gateway.py:1362: in __call__\n    return_value = get_return_value(\nvenv/lib/python3.14/site-packages/pyspark/errors/exceptions/captured.py:263: in deco\n    return f(*a, **kw)\n           ^^^^^^^^^^^\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nanswer = &amp;#x27;xro3672&amp;#x27;\ngateway_client = &amp;lt;py4j.clientserver.JavaClient object at 0x10ca1c6e0&amp;gt;\ntarget_id = &amp;#x27;o3671&amp;#x27;, name = &amp;#x27;count&amp;#x27;\n\n    def get_return_value(answer, gateway_client, target_id=None, name=None):\n        &amp;quot;&amp;quot;&amp;quot;Converts an answer received from the Java gateway into a Python object.\n    \n        For example, string representation of integers are converted to Python\n        integer, string representation of objects are converted to JavaObject\n        instances, etc.\n    \n        :param answer: the string returned by the Java gateway\n        :param gateway_client: the gateway client used to communicate with the Java\n            Gateway. Only necessary if the answer is a reference (e.g., object,\n            list, map)\n        :param target_id: the name of the object from which the answer comes from\n            (e.g., *object1* in `object1.hello()`). Optional.\n        :param name: the name of the member from which the answer comes from\n            (e.g., *hello* in `object1.hello()`). Optional.\n        &amp;quot;&amp;quot;&amp;quot;\n        if is_error(answer)[0]:\n            if len(answer) &amp;gt; 1:\n                type = answer[1]\n                value = OUTPUT_CONVERTER[type](answer[2:], gateway_client)\n                if answer[1] == REFERENCE_TYPE:\n&amp;gt;                   raise Py4JJavaError(\n                        &amp;quot;An error occurred while calling {0}{1}{2}.\\n&amp;quot;.\n                        format(target_id, &amp;quot;.&amp;quot;, name), value)\nE                   py4j.protocol.Py4JJavaError: An error occurred while calling o3671.count.\nE                   : org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 12.0 failed 1 times, most recent failure: Lost task 0.0 in stage 12.0 (TID 12) (10.20.80.191 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\nE                       check_python_version(infile)\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\nE                       raise PySparkRuntimeError(\nE                   pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nE                   Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\nE                   \nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\nE                   \tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\nE                   \tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\nE                   \tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\nE                   \tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\nE                   \tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\nE                   \tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\nE                   \tat org.apache.spark.scheduler.Task.run(Task.scala:147)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\nE                   \tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\nE                   \tat java.base/java.lang.Thread.run(Thread.java:840)\nE                   \nE                   Driver stacktrace:\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$3(DAGScheduler.scala:3122)\nE                   \tat scala.Option.getOrElse(Option.scala:201)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:3122)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:3114)\nE                   \tat scala.collection.immutable.List.foreach(List.scala:323)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:3114)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1303)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1303)\nE                   \tat scala.Option.foreach(Option.scala:437)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1303)\nE                   \tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3397)\nE                   \tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3328)\nE                   \tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3317)\nE                   \tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:50)\nE                   Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\nE                       check_python_version(infile)\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\nE                       raise PySparkRuntimeError(\nE                   pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nE                   Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\nE                   \nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\nE                   \tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\nE                   \tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\nE                   \tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\nE                   \tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\nE                   \tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\nE                   \tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\nE                   \tat org.apache.spark.scheduler.Task.run(Task.scala:147)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\nE                   \tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\nE                   \tat java.base/java.lang.Thread.run(Thread.java:840)\n\nvenv/lib/python3.14/site-packages/py4j/protocol.py:327: Py4JJavaError\n\n----------------------------- Captured stderr call -----------------------------\n26/01/22 12:17:23 ERROR Executor: Exception in task 0.0 in stage 12.0 (TID 12)\norg.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\n    check_python_version(infile)\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\n    raise PySparkRuntimeError(\npyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nPlease check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n26/01/22 12:17:23 WARN TaskSetManager: Lost task 0.0 in stage 12.0 (TID 12) (10.20.80.191 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\n    check_python_version(infile)\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\n    raise PySparkRuntimeError(\npyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nPlease check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n\n26/01/22 12:17:23 ERROR TaskSetManager: Task 0 in stage 12.0 failed 1 times; aborting job\n&#34;}], &#34;tests/test_agents_transform.py::test_safe_nested_col_struct_missing&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/test_agents_transform.py::test_safe_nested_col_struct_missing&#34;, &#34;duration&#34;: &#34;380 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/test_agents_transform.py::test_safe_nested_col_struct_missing&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;380 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;spark = &amp;lt;pyspark.sql.session.SparkSession object at 0x10ca1de80&amp;gt;\n\n    def test_safe_nested_col_struct_missing(spark):\n        df = spark.createDataFrame(\n            [\n                Row(id=42)\n            ],\n            schema=StructType([\n                StructField(&amp;quot;id&amp;quot;, LongType(), True),\n                # contact struct missing\n            ])\n        )\n    \n        final_df, error_df = transform_agents(df, agents_df(spark, [42]))\n    \n        # required fields missing \u2192 error expected\n        assert final_df.count() == 0\n&amp;gt;       assert error_df.count() == 1\n               ^^^^^^^^^^^^^^^^\n\ntests/test_agents_transform.py:342: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \nvenv/lib/python3.14/site-packages/pyspark/sql/classic/dataframe.py:439: in count\n    return int(self._jdf.count())\n               ^^^^^^^^^^^^^^^^^\nvenv/lib/python3.14/site-packages/py4j/java_gateway.py:1362: in __call__\n    return_value = get_return_value(\nvenv/lib/python3.14/site-packages/pyspark/errors/exceptions/captured.py:263: in deco\n    return f(*a, **kw)\n           ^^^^^^^^^^^\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nanswer = &amp;#x27;xro3941&amp;#x27;\ngateway_client = &amp;lt;py4j.clientserver.JavaClient object at 0x10ca1c6e0&amp;gt;\ntarget_id = &amp;#x27;o3832&amp;#x27;, name = &amp;#x27;count&amp;#x27;\n\n    def get_return_value(answer, gateway_client, target_id=None, name=None):\n        &amp;quot;&amp;quot;&amp;quot;Converts an answer received from the Java gateway into a Python object.\n    \n        For example, string representation of integers are converted to Python\n        integer, string representation of objects are converted to JavaObject\n        instances, etc.\n    \n        :param answer: the string returned by the Java gateway\n        :param gateway_client: the gateway client used to communicate with the Java\n            Gateway. Only necessary if the answer is a reference (e.g., object,\n            list, map)\n        :param target_id: the name of the object from which the answer comes from\n            (e.g., *object1* in `object1.hello()`). Optional.\n        :param name: the name of the member from which the answer comes from\n            (e.g., *hello* in `object1.hello()`). Optional.\n        &amp;quot;&amp;quot;&amp;quot;\n        if is_error(answer)[0]:\n            if len(answer) &amp;gt; 1:\n                type = answer[1]\n                value = OUTPUT_CONVERTER[type](answer[2:], gateway_client)\n                if answer[1] == REFERENCE_TYPE:\n&amp;gt;                   raise Py4JJavaError(\n                        &amp;quot;An error occurred while calling {0}{1}{2}.\\n&amp;quot;.\n                        format(target_id, &amp;quot;.&amp;quot;, name), value)\nE                   py4j.protocol.Py4JJavaError: An error occurred while calling o3832.count.\nE                   : org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 15.0 failed 1 times, most recent failure: Lost task 0.0 in stage 15.0 (TID 14) (10.20.80.191 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\nE                       check_python_version(infile)\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\nE                       raise PySparkRuntimeError(\nE                   pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nE                   Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\nE                   \nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\nE                   \tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\nE                   \tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\nE                   \tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\nE                   \tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\nE                   \tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\nE                   \tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\nE                   \tat org.apache.spark.scheduler.Task.run(Task.scala:147)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\nE                   \tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\nE                   \tat java.base/java.lang.Thread.run(Thread.java:840)\nE                   \nE                   Driver stacktrace:\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$3(DAGScheduler.scala:3122)\nE                   \tat scala.Option.getOrElse(Option.scala:201)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:3122)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:3114)\nE                   \tat scala.collection.immutable.List.foreach(List.scala:323)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:3114)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1303)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1303)\nE                   \tat scala.Option.foreach(Option.scala:437)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1303)\nE                   \tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3397)\nE                   \tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3328)\nE                   \tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3317)\nE                   \tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:50)\nE                   Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\nE                       check_python_version(infile)\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\nE                       raise PySparkRuntimeError(\nE                   pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nE                   Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\nE                   \nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\nE                   \tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\nE                   \tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\nE                   \tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\nE                   \tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\nE                   \tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\nE                   \tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\nE                   \tat org.apache.spark.scheduler.Task.run(Task.scala:147)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\nE                   \tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\nE                   \tat java.base/java.lang.Thread.run(Thread.java:840)\n\nvenv/lib/python3.14/site-packages/py4j/protocol.py:327: Py4JJavaError\n\n----------------------------- Captured stderr call -----------------------------\n\r                                                                                \r26/01/22 12:17:23 ERROR Executor: Exception in task 0.0 in stage 15.0 (TID 14)\norg.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\n    check_python_version(infile)\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\n    raise PySparkRuntimeError(\npyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nPlease check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n26/01/22 12:17:23 WARN TaskSetManager: Lost task 0.0 in stage 15.0 (TID 14) (10.20.80.191 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\n    check_python_version(infile)\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\n    raise PySparkRuntimeError(\npyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nPlease check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n\n26/01/22 12:17:23 ERROR TaskSetManager: Task 0 in stage 15.0 failed 1 times; aborting job\n&#34;}], &#34;tests/test_agents_transform.py::test_safe_nested_col_nested_field_missing&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/test_agents_transform.py::test_safe_nested_col_nested_field_missing&#34;, &#34;duration&#34;: &#34;277 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/test_agents_transform.py::test_safe_nested_col_nested_field_missing&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;277 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;spark = &amp;lt;pyspark.sql.session.SparkSession object at 0x10ca1de80&amp;gt;\n\n    def test_safe_nested_col_nested_field_missing(spark):\n        partial_contact_schema = StructType([\n            StructField(&amp;quot;email&amp;quot;, StringType(), True),\n            # name field missing\n        ])\n    \n        df = spark.createDataFrame(\n            [\n                Row(\n                    id=43,\n                    contact=Row(email=&amp;quot;partial@test.com&amp;quot;)\n                )\n            ],\n            schema=StructType([\n                StructField(&amp;quot;id&amp;quot;, LongType(), True),\n                StructField(&amp;quot;contact&amp;quot;, partial_contact_schema, True),\n            ])\n        )\n    \n        final_df, error_df = transform_agents(df, agents_df(spark, [43]))\n    \n        # name is required \u2192 error expected\n        assert final_df.count() == 0\n&amp;gt;       assert error_df.count() == 1\n               ^^^^^^^^^^^^^^^^\n\ntests/test_agents_transform.py:373: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \nvenv/lib/python3.14/site-packages/pyspark/sql/classic/dataframe.py:439: in count\n    return int(self._jdf.count())\n               ^^^^^^^^^^^^^^^^^\nvenv/lib/python3.14/site-packages/py4j/java_gateway.py:1362: in __call__\n    return_value = get_return_value(\nvenv/lib/python3.14/site-packages/pyspark/errors/exceptions/captured.py:263: in deco\n    return f(*a, **kw)\n           ^^^^^^^^^^^\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nanswer = &amp;#x27;xro4211&amp;#x27;\ngateway_client = &amp;lt;py4j.clientserver.JavaClient object at 0x10ca1c6e0&amp;gt;\ntarget_id = &amp;#x27;o4102&amp;#x27;, name = &amp;#x27;count&amp;#x27;\n\n    def get_return_value(answer, gateway_client, target_id=None, name=None):\n        &amp;quot;&amp;quot;&amp;quot;Converts an answer received from the Java gateway into a Python object.\n    \n        For example, string representation of integers are converted to Python\n        integer, string representation of objects are converted to JavaObject\n        instances, etc.\n    \n        :param answer: the string returned by the Java gateway\n        :param gateway_client: the gateway client used to communicate with the Java\n            Gateway. Only necessary if the answer is a reference (e.g., object,\n            list, map)\n        :param target_id: the name of the object from which the answer comes from\n            (e.g., *object1* in `object1.hello()`). Optional.\n        :param name: the name of the member from which the answer comes from\n            (e.g., *hello* in `object1.hello()`). Optional.\n        &amp;quot;&amp;quot;&amp;quot;\n        if is_error(answer)[0]:\n            if len(answer) &amp;gt; 1:\n                type = answer[1]\n                value = OUTPUT_CONVERTER[type](answer[2:], gateway_client)\n                if answer[1] == REFERENCE_TYPE:\n&amp;gt;                   raise Py4JJavaError(\n                        &amp;quot;An error occurred while calling {0}{1}{2}.\\n&amp;quot;.\n                        format(target_id, &amp;quot;.&amp;quot;, name), value)\nE                   py4j.protocol.Py4JJavaError: An error occurred while calling o4102.count.\nE                   : org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 18.0 failed 1 times, most recent failure: Lost task 0.0 in stage 18.0 (TID 16) (10.20.80.191 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\nE                       check_python_version(infile)\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\nE                       raise PySparkRuntimeError(\nE                   pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nE                   Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\nE                   \nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\nE                   \tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\nE                   \tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\nE                   \tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\nE                   \tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\nE                   \tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\nE                   \tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\nE                   \tat org.apache.spark.scheduler.Task.run(Task.scala:147)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\nE                   \tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\nE                   \tat java.base/java.lang.Thread.run(Thread.java:840)\nE                   \nE                   Driver stacktrace:\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$3(DAGScheduler.scala:3122)\nE                   \tat scala.Option.getOrElse(Option.scala:201)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:3122)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:3114)\nE                   \tat scala.collection.immutable.List.foreach(List.scala:323)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:3114)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1303)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1303)\nE                   \tat scala.Option.foreach(Option.scala:437)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1303)\nE                   \tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3397)\nE                   \tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3328)\nE                   \tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3317)\nE                   \tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:50)\nE                   Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\nE                       check_python_version(infile)\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\nE                       raise PySparkRuntimeError(\nE                   pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nE                   Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\nE                   \nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\nE                   \tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\nE                   \tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\nE                   \tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\nE                   \tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\nE                   \tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\nE                   \tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\nE                   \tat org.apache.spark.scheduler.Task.run(Task.scala:147)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\nE                   \tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\nE                   \tat java.base/java.lang.Thread.run(Thread.java:840)\n\nvenv/lib/python3.14/site-packages/py4j/protocol.py:327: Py4JJavaError\n\n----------------------------- Captured stderr call -----------------------------\n26/01/22 12:17:24 ERROR Executor: Exception in task 0.0 in stage 18.0 (TID 16)\norg.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\n    check_python_version(infile)\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\n    raise PySparkRuntimeError(\npyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nPlease check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n26/01/22 12:17:24 WARN TaskSetManager: Lost task 0.0 in stage 18.0 (TID 16) (10.20.80.191 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\n    check_python_version(infile)\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\n    raise PySparkRuntimeError(\npyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nPlease check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n\n26/01/22 12:17:24 ERROR TaskSetManager: Task 0 in stage 18.0 failed 1 times; aborting job\n&#34;}], &#34;tests/test_budgets_transform.py::test_transform_budgets_valid_single&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/test_budgets_transform.py::test_transform_budgets_valid_single&#34;, &#34;duration&#34;: &#34;334 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/test_budgets_transform.py::test_transform_budgets_valid_single&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;334 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;spark = &amp;lt;pyspark.sql.session.SparkSession object at 0x10ca1de80&amp;gt;\nbudgets_schema = StructType([StructField(&amp;#x27;results&amp;#x27;, ArrayType(StructType([StructField(&amp;#x27;id&amp;#x27;, StringType(), True), StructField(&amp;#x27;uuid&amp;#x27;, St...tructField(&amp;#x27;pending&amp;#x27;, DoubleType(), True), StructField(&amp;#x27;total&amp;#x27;, DoubleType(), True)]), True)]), True)]), True), True)])\n\n    def test_transform_budgets_valid_single(spark, budgets_schema):\n        raw_data = [{\n            &amp;quot;results&amp;quot;: [{\n                &amp;quot;id&amp;quot;: &amp;quot;b1&amp;quot;,\n                &amp;quot;uuid&amp;quot;: &amp;quot;u1&amp;quot;,\n                &amp;quot;name&amp;quot;: &amp;quot;AWS&amp;quot;,\n                &amp;quot;description&amp;quot;: &amp;quot;Cloud&amp;quot;,\n                &amp;quot;retired&amp;quot;: False,\n                &amp;quot;startDate&amp;quot;: &amp;quot;2024-01-01&amp;quot;,\n                &amp;quot;recurringInterval&amp;quot;: &amp;quot;monthly&amp;quot;,\n                &amp;quot;timezone&amp;quot;: &amp;quot;UTC&amp;quot;,\n                &amp;quot;currentPeriod&amp;quot;: {\n                    &amp;quot;limit&amp;quot;: 1000.0,\n                    &amp;quot;overspendBuffer&amp;quot;: 100.0,\n                    &amp;quot;assigned&amp;quot;: 900.0,\n                    &amp;quot;spent&amp;quot;: {\n                        &amp;quot;cleared&amp;quot;: 200.0,\n                        &amp;quot;pending&amp;quot;: 50.0,\n                        &amp;quot;total&amp;quot;: 250.0\n                    }\n                }\n            }]\n        }]\n    \n        raw_df = spark.createDataFrame(raw_data, budgets_schema)\n    \n        valid_df, error_df = transform_budgets(raw_df, day=&amp;quot;2024-01-10&amp;quot;)\n    \n&amp;gt;       assert valid_df.count() == 1\n               ^^^^^^^^^^^^^^^^\n\ntests/test_budgets_transform.py:80: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \nvenv/lib/python3.14/site-packages/pyspark/sql/classic/dataframe.py:439: in count\n    return int(self._jdf.count())\n               ^^^^^^^^^^^^^^^^^\nvenv/lib/python3.14/site-packages/py4j/java_gateway.py:1362: in __call__\n    return_value = get_return_value(\nvenv/lib/python3.14/site-packages/pyspark/errors/exceptions/captured.py:263: in deco\n    return f(*a, **kw)\n           ^^^^^^^^^^^\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nanswer = &amp;#x27;xro4403&amp;#x27;\ngateway_client = &amp;lt;py4j.clientserver.JavaClient object at 0x10ca1c6e0&amp;gt;\ntarget_id = &amp;#x27;o4402&amp;#x27;, name = &amp;#x27;count&amp;#x27;\n\n    def get_return_value(answer, gateway_client, target_id=None, name=None):\n        &amp;quot;&amp;quot;&amp;quot;Converts an answer received from the Java gateway into a Python object.\n    \n        For example, string representation of integers are converted to Python\n        integer, string representation of objects are converted to JavaObject\n        instances, etc.\n    \n        :param answer: the string returned by the Java gateway\n        :param gateway_client: the gateway client used to communicate with the Java\n            Gateway. Only necessary if the answer is a reference (e.g., object,\n            list, map)\n        :param target_id: the name of the object from which the answer comes from\n            (e.g., *object1* in `object1.hello()`). Optional.\n        :param name: the name of the member from which the answer comes from\n            (e.g., *hello* in `object1.hello()`). Optional.\n        &amp;quot;&amp;quot;&amp;quot;\n        if is_error(answer)[0]:\n            if len(answer) &amp;gt; 1:\n                type = answer[1]\n                value = OUTPUT_CONVERTER[type](answer[2:], gateway_client)\n                if answer[1] == REFERENCE_TYPE:\n&amp;gt;                   raise Py4JJavaError(\n                        &amp;quot;An error occurred while calling {0}{1}{2}.\\n&amp;quot;.\n                        format(target_id, &amp;quot;.&amp;quot;, name), value)\nE                   py4j.protocol.Py4JJavaError: An error occurred while calling o4402.count.\nE                   : org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 19.0 failed 1 times, most recent failure: Lost task 0.0 in stage 19.0 (TID 17) (10.20.80.191 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\nE                       check_python_version(infile)\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\nE                       raise PySparkRuntimeError(\nE                   pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nE                   Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\nE                   \nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\nE                   \tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\nE                   \tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\nE                   \tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\nE                   \tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\nE                   \tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\nE                   \tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\nE                   \tat org.apache.spark.scheduler.Task.run(Task.scala:147)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\nE                   \tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\nE                   \tat java.base/java.lang.Thread.run(Thread.java:840)\nE                   \nE                   Driver stacktrace:\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$3(DAGScheduler.scala:3122)\nE                   \tat scala.Option.getOrElse(Option.scala:201)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:3122)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:3114)\nE                   \tat scala.collection.immutable.List.foreach(List.scala:323)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:3114)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1303)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1303)\nE                   \tat scala.Option.foreach(Option.scala:437)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1303)\nE                   \tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3397)\nE                   \tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3328)\nE                   \tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3317)\nE                   \tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:50)\nE                   Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\nE                       check_python_version(infile)\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\nE                       raise PySparkRuntimeError(\nE                   pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nE                   Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\nE                   \nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\nE                   \tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\nE                   \tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\nE                   \tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\nE                   \tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\nE                   \tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\nE                   \tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\nE                   \tat org.apache.spark.scheduler.Task.run(Task.scala:147)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\nE                   \tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\nE                   \tat java.base/java.lang.Thread.run(Thread.java:840)\n\nvenv/lib/python3.14/site-packages/py4j/protocol.py:327: Py4JJavaError\n\n---------------------------- Captured stderr setup -----------------------------\n26/01/22 12:17:24 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n\n----------------------------- Captured stderr call -----------------------------\n26/01/22 12:17:24 ERROR Executor: Exception in task 0.0 in stage 19.0 (TID 17)\norg.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\n    check_python_version(infile)\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\n    raise PySparkRuntimeError(\npyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nPlease check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n26/01/22 12:17:24 WARN TaskSetManager: Lost task 0.0 in stage 19.0 (TID 17) (10.20.80.191 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\n    check_python_version(infile)\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\n    raise PySparkRuntimeError(\npyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nPlease check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n\n26/01/22 12:17:24 ERROR TaskSetManager: Task 0 in stage 19.0 failed 1 times; aborting job\n&#34;}], &#34;tests/test_budgets_transform.py::test_transform_budgets_multiple_results&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/test_budgets_transform.py::test_transform_budgets_multiple_results&#34;, &#34;duration&#34;: &#34;548 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/test_budgets_transform.py::test_transform_budgets_multiple_results&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;548 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;spark = &amp;lt;pyspark.sql.session.SparkSession object at 0x10ca1de80&amp;gt;\nbudgets_schema = StructType([StructField(&amp;#x27;results&amp;#x27;, ArrayType(StructType([StructField(&amp;#x27;id&amp;#x27;, StringType(), True), StructField(&amp;#x27;uuid&amp;#x27;, St...tructField(&amp;#x27;pending&amp;#x27;, DoubleType(), True), StructField(&amp;#x27;total&amp;#x27;, DoubleType(), True)]), True)]), True)]), True), True)])\n\n    def test_transform_budgets_multiple_results(spark, budgets_schema):\n        raw_data = [{\n            &amp;quot;results&amp;quot;: [\n                {\n                    &amp;quot;id&amp;quot;: &amp;quot;b1&amp;quot;,\n                    &amp;quot;uuid&amp;quot;: &amp;quot;u1&amp;quot;,\n                    &amp;quot;name&amp;quot;: &amp;quot;AWS&amp;quot;,\n                    &amp;quot;description&amp;quot;: &amp;quot;Cloud&amp;quot;,\n                    &amp;quot;retired&amp;quot;: False,\n                    &amp;quot;startDate&amp;quot;: &amp;quot;2024-01-01&amp;quot;,\n                    &amp;quot;recurringInterval&amp;quot;: &amp;quot;monthly&amp;quot;,\n                    &amp;quot;timezone&amp;quot;: &amp;quot;UTC&amp;quot;,\n                    &amp;quot;currentPeriod&amp;quot;: {\n                        &amp;quot;limit&amp;quot;: 1000.0,\n                        &amp;quot;overspendBuffer&amp;quot;: 100.0,\n                        &amp;quot;assigned&amp;quot;: 900.0,\n                        &amp;quot;spent&amp;quot;: {&amp;quot;cleared&amp;quot;: 200.0, &amp;quot;pending&amp;quot;: 50.0, &amp;quot;total&amp;quot;: 250.0}\n                    }\n                },\n                {\n                    &amp;quot;id&amp;quot;: &amp;quot;b2&amp;quot;,\n                    &amp;quot;uuid&amp;quot;: &amp;quot;u2&amp;quot;,\n                    &amp;quot;name&amp;quot;: &amp;quot;GCP&amp;quot;,\n                    &amp;quot;description&amp;quot;: &amp;quot;Infra&amp;quot;,\n                    &amp;quot;retired&amp;quot;: False,\n                    &amp;quot;startDate&amp;quot;: &amp;quot;2024-02-01&amp;quot;,\n                    &amp;quot;recurringInterval&amp;quot;: &amp;quot;monthly&amp;quot;,\n                    &amp;quot;timezone&amp;quot;: &amp;quot;UTC&amp;quot;,\n                    &amp;quot;currentPeriod&amp;quot;: {\n                        &amp;quot;limit&amp;quot;: 2000.0,\n                        &amp;quot;overspendBuffer&amp;quot;: 200.0,\n                        &amp;quot;assigned&amp;quot;: 1800.0,\n                        &amp;quot;spent&amp;quot;: {&amp;quot;cleared&amp;quot;: 300.0, &amp;quot;pending&amp;quot;: 20.0, &amp;quot;total&amp;quot;: 320.0}\n                    }\n                }\n            ]\n        }]\n    \n        raw_df = spark.createDataFrame(raw_data, budgets_schema)\n        valid_df, error_df = transform_budgets(raw_df, day=&amp;quot;2024-01-10&amp;quot;)\n    \n&amp;gt;       assert valid_df.count() == 2\n               ^^^^^^^^^^^^^^^^\n\ntests/test_budgets_transform.py:133: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \nvenv/lib/python3.14/site-packages/pyspark/sql/classic/dataframe.py:439: in count\n    return int(self._jdf.count())\n               ^^^^^^^^^^^^^^^^^\nvenv/lib/python3.14/site-packages/py4j/java_gateway.py:1362: in __call__\n    return_value = get_return_value(\nvenv/lib/python3.14/site-packages/pyspark/errors/exceptions/captured.py:263: in deco\n    return f(*a, **kw)\n           ^^^^^^^^^^^\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nanswer = &amp;#x27;xro4593&amp;#x27;\ngateway_client = &amp;lt;py4j.clientserver.JavaClient object at 0x10ca1c6e0&amp;gt;\ntarget_id = &amp;#x27;o4592&amp;#x27;, name = &amp;#x27;count&amp;#x27;\n\n    def get_return_value(answer, gateway_client, target_id=None, name=None):\n        &amp;quot;&amp;quot;&amp;quot;Converts an answer received from the Java gateway into a Python object.\n    \n        For example, string representation of integers are converted to Python\n        integer, string representation of objects are converted to JavaObject\n        instances, etc.\n    \n        :param answer: the string returned by the Java gateway\n        :param gateway_client: the gateway client used to communicate with the Java\n            Gateway. Only necessary if the answer is a reference (e.g., object,\n            list, map)\n        :param target_id: the name of the object from which the answer comes from\n            (e.g., *object1* in `object1.hello()`). Optional.\n        :param name: the name of the member from which the answer comes from\n            (e.g., *hello* in `object1.hello()`). Optional.\n        &amp;quot;&amp;quot;&amp;quot;\n        if is_error(answer)[0]:\n            if len(answer) &amp;gt; 1:\n                type = answer[1]\n                value = OUTPUT_CONVERTER[type](answer[2:], gateway_client)\n                if answer[1] == REFERENCE_TYPE:\n&amp;gt;                   raise Py4JJavaError(\n                        &amp;quot;An error occurred while calling {0}{1}{2}.\\n&amp;quot;.\n                        format(target_id, &amp;quot;.&amp;quot;, name), value)\nE                   py4j.protocol.Py4JJavaError: An error occurred while calling o4592.count.\nE                   : org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 20.0 failed 1 times, most recent failure: Lost task 0.0 in stage 20.0 (TID 18) (10.20.80.191 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\nE                       check_python_version(infile)\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\nE                       raise PySparkRuntimeError(\nE                   pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nE                   Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\nE                   \nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\nE                   \tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\nE                   \tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\nE                   \tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\nE                   \tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\nE                   \tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\nE                   \tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\nE                   \tat org.apache.spark.scheduler.Task.run(Task.scala:147)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\nE                   \tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\nE                   \tat java.base/java.lang.Thread.run(Thread.java:840)\nE                   \nE                   Driver stacktrace:\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$3(DAGScheduler.scala:3122)\nE                   \tat scala.Option.getOrElse(Option.scala:201)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:3122)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:3114)\nE                   \tat scala.collection.immutable.List.foreach(List.scala:323)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:3114)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1303)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1303)\nE                   \tat scala.Option.foreach(Option.scala:437)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1303)\nE                   \tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3397)\nE                   \tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3328)\nE                   \tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3317)\nE                   \tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:50)\nE                   Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\nE                       check_python_version(infile)\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\nE                       raise PySparkRuntimeError(\nE                   pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nE                   Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\nE                   \nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\nE                   \tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\nE                   \tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\nE                   \tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\nE                   \tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\nE                   \tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\nE                   \tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\nE                   \tat org.apache.spark.scheduler.Task.run(Task.scala:147)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\nE                   \tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\nE                   \tat java.base/java.lang.Thread.run(Thread.java:840)\n\nvenv/lib/python3.14/site-packages/py4j/protocol.py:327: Py4JJavaError\n\n----------------------------- Captured stderr call -----------------------------\n26/01/22 12:17:25 ERROR Executor: Exception in task 0.0 in stage 20.0 (TID 18)\norg.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\n    check_python_version(infile)\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\n    raise PySparkRuntimeError(\npyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nPlease check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n26/01/22 12:17:25 WARN TaskSetManager: Lost task 0.0 in stage 20.0 (TID 18) (10.20.80.191 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\n    check_python_version(infile)\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\n    raise PySparkRuntimeError(\npyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nPlease check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n\n26/01/22 12:17:25 ERROR TaskSetManager: Task 0 in stage 20.0 failed 1 times; aborting job\n&#34;}], &#34;tests/test_budgets_transform.py::test_transform_budgets_null_budget_id&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/test_budgets_transform.py::test_transform_budgets_null_budget_id&#34;, &#34;duration&#34;: &#34;309 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/test_budgets_transform.py::test_transform_budgets_null_budget_id&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;309 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;spark = &amp;lt;pyspark.sql.session.SparkSession object at 0x10ca1de80&amp;gt;\nbudgets_schema = StructType([StructField(&amp;#x27;results&amp;#x27;, ArrayType(StructType([StructField(&amp;#x27;id&amp;#x27;, StringType(), True), StructField(&amp;#x27;uuid&amp;#x27;, St...tructField(&amp;#x27;pending&amp;#x27;, DoubleType(), True), StructField(&amp;#x27;total&amp;#x27;, DoubleType(), True)]), True)]), True)]), True), True)])\n\n    def test_transform_budgets_null_budget_id(spark, budgets_schema):\n        raw_data = [{\n            &amp;quot;results&amp;quot;: [{\n                &amp;quot;id&amp;quot;: None,\n                &amp;quot;uuid&amp;quot;: &amp;quot;u1&amp;quot;,\n                &amp;quot;name&amp;quot;: &amp;quot;AWS&amp;quot;,\n                &amp;quot;description&amp;quot;: &amp;quot;Cloud&amp;quot;,\n                &amp;quot;retired&amp;quot;: False,\n                &amp;quot;startDate&amp;quot;: &amp;quot;2024-01-01&amp;quot;,\n                &amp;quot;recurringInterval&amp;quot;: &amp;quot;monthly&amp;quot;,\n                &amp;quot;timezone&amp;quot;: &amp;quot;UTC&amp;quot;,\n                &amp;quot;currentPeriod&amp;quot;: {\n                    &amp;quot;limit&amp;quot;: 1000.0,\n                    &amp;quot;overspendBuffer&amp;quot;: 100.0,\n                    &amp;quot;assigned&amp;quot;: 900.0,\n                    &amp;quot;spent&amp;quot;: {&amp;quot;cleared&amp;quot;: 200.0, &amp;quot;pending&amp;quot;: 50.0, &amp;quot;total&amp;quot;: 250.0}\n                }\n            }]\n        }]\n    \n        raw_df = spark.createDataFrame(raw_data, budgets_schema)\n        valid_df, error_df = transform_budgets(raw_df, day=&amp;quot;2024-01-10&amp;quot;)\n    \n&amp;gt;       assert valid_df.count() == 0\n               ^^^^^^^^^^^^^^^^\n\ntests/test_budgets_transform.py:163: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \nvenv/lib/python3.14/site-packages/pyspark/sql/classic/dataframe.py:439: in count\n    return int(self._jdf.count())\n               ^^^^^^^^^^^^^^^^^\nvenv/lib/python3.14/site-packages/py4j/java_gateway.py:1362: in __call__\n    return_value = get_return_value(\nvenv/lib/python3.14/site-packages/pyspark/errors/exceptions/captured.py:263: in deco\n    return f(*a, **kw)\n           ^^^^^^^^^^^\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nanswer = &amp;#x27;xro4783&amp;#x27;\ngateway_client = &amp;lt;py4j.clientserver.JavaClient object at 0x10ca1c6e0&amp;gt;\ntarget_id = &amp;#x27;o4782&amp;#x27;, name = &amp;#x27;count&amp;#x27;\n\n    def get_return_value(answer, gateway_client, target_id=None, name=None):\n        &amp;quot;&amp;quot;&amp;quot;Converts an answer received from the Java gateway into a Python object.\n    \n        For example, string representation of integers are converted to Python\n        integer, string representation of objects are converted to JavaObject\n        instances, etc.\n    \n        :param answer: the string returned by the Java gateway\n        :param gateway_client: the gateway client used to communicate with the Java\n            Gateway. Only necessary if the answer is a reference (e.g., object,\n            list, map)\n        :param target_id: the name of the object from which the answer comes from\n            (e.g., *object1* in `object1.hello()`). Optional.\n        :param name: the name of the member from which the answer comes from\n            (e.g., *hello* in `object1.hello()`). Optional.\n        &amp;quot;&amp;quot;&amp;quot;\n        if is_error(answer)[0]:\n            if len(answer) &amp;gt; 1:\n                type = answer[1]\n                value = OUTPUT_CONVERTER[type](answer[2:], gateway_client)\n                if answer[1] == REFERENCE_TYPE:\n&amp;gt;                   raise Py4JJavaError(\n                        &amp;quot;An error occurred while calling {0}{1}{2}.\\n&amp;quot;.\n                        format(target_id, &amp;quot;.&amp;quot;, name), value)\nE                   py4j.protocol.Py4JJavaError: An error occurred while calling o4782.count.\nE                   : org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 21.0 failed 1 times, most recent failure: Lost task 0.0 in stage 21.0 (TID 19) (10.20.80.191 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\nE                       check_python_version(infile)\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\nE                       raise PySparkRuntimeError(\nE                   pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nE                   Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\nE                   \nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\nE                   \tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\nE                   \tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\nE                   \tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\nE                   \tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\nE                   \tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\nE                   \tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\nE                   \tat org.apache.spark.scheduler.Task.run(Task.scala:147)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\nE                   \tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\nE                   \tat java.base/java.lang.Thread.run(Thread.java:840)\nE                   \nE                   Driver stacktrace:\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$3(DAGScheduler.scala:3122)\nE                   \tat scala.Option.getOrElse(Option.scala:201)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:3122)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:3114)\nE                   \tat scala.collection.immutable.List.foreach(List.scala:323)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:3114)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1303)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1303)\nE                   \tat scala.Option.foreach(Option.scala:437)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1303)\nE                   \tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3397)\nE                   \tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3328)\nE                   \tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3317)\nE                   \tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:50)\nE                   Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\nE                       check_python_version(infile)\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\nE                       raise PySparkRuntimeError(\nE                   pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nE                   Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\nE                   \nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\nE                   \tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\nE                   \tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\nE                   \tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\nE                   \tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\nE                   \tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\nE                   \tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\nE                   \tat org.apache.spark.scheduler.Task.run(Task.scala:147)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\nE                   \tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\nE                   \tat java.base/java.lang.Thread.run(Thread.java:840)\n\nvenv/lib/python3.14/site-packages/py4j/protocol.py:327: Py4JJavaError\n\n----------------------------- Captured stderr call -----------------------------\n26/01/22 12:17:25 ERROR Executor: Exception in task 0.0 in stage 21.0 (TID 19)\norg.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\n    check_python_version(infile)\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\n    raise PySparkRuntimeError(\npyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nPlease check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n26/01/22 12:17:25 WARN TaskSetManager: Lost task 0.0 in stage 21.0 (TID 19) (10.20.80.191 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\n    check_python_version(infile)\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\n    raise PySparkRuntimeError(\npyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nPlease check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n\n26/01/22 12:17:25 ERROR TaskSetManager: Task 0 in stage 21.0 failed 1 times; aborting job\n&#34;}], &#34;tests/test_budgets_transform.py::test_transform_budgets_null_limit_amount&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/test_budgets_transform.py::test_transform_budgets_null_limit_amount&#34;, &#34;duration&#34;: &#34;166 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/test_budgets_transform.py::test_transform_budgets_null_limit_amount&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;166 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;spark = &amp;lt;pyspark.sql.session.SparkSession object at 0x10ca1de80&amp;gt;\nbudgets_schema = StructType([StructField(&amp;#x27;results&amp;#x27;, ArrayType(StructType([StructField(&amp;#x27;id&amp;#x27;, StringType(), True), StructField(&amp;#x27;uuid&amp;#x27;, St...tructField(&amp;#x27;pending&amp;#x27;, DoubleType(), True), StructField(&amp;#x27;total&amp;#x27;, DoubleType(), True)]), True)]), True)]), True), True)])\n\n    def test_transform_budgets_null_limit_amount(spark, budgets_schema):\n        raw_data = [{\n            &amp;quot;results&amp;quot;: [{\n                &amp;quot;id&amp;quot;: &amp;quot;b1&amp;quot;,\n                &amp;quot;uuid&amp;quot;: &amp;quot;u1&amp;quot;,\n                &amp;quot;name&amp;quot;: &amp;quot;AWS&amp;quot;,\n                &amp;quot;description&amp;quot;: &amp;quot;Cloud&amp;quot;,\n                &amp;quot;retired&amp;quot;: False,\n                &amp;quot;startDate&amp;quot;: &amp;quot;2024-01-01&amp;quot;,\n                &amp;quot;recurringInterval&amp;quot;: &amp;quot;monthly&amp;quot;,\n                &amp;quot;timezone&amp;quot;: &amp;quot;UTC&amp;quot;,\n                &amp;quot;currentPeriod&amp;quot;: {\n                    &amp;quot;limit&amp;quot;: None,\n                    &amp;quot;overspendBuffer&amp;quot;: 100.0,\n                    &amp;quot;assigned&amp;quot;: 900.0,\n                    &amp;quot;spent&amp;quot;: {&amp;quot;cleared&amp;quot;: 200.0, &amp;quot;pending&amp;quot;: 50.0, &amp;quot;total&amp;quot;: 250.0}\n                }\n            }]\n        }]\n    \n        raw_df = spark.createDataFrame(raw_data, budgets_schema)\n        valid_df, error_df = transform_budgets(raw_df, day=&amp;quot;2024-01-10&amp;quot;)\n    \n&amp;gt;       assert valid_df.count() == 0\n               ^^^^^^^^^^^^^^^^\n\ntests/test_budgets_transform.py:196: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \nvenv/lib/python3.14/site-packages/pyspark/sql/classic/dataframe.py:439: in count\n    return int(self._jdf.count())\n               ^^^^^^^^^^^^^^^^^\nvenv/lib/python3.14/site-packages/py4j/java_gateway.py:1362: in __call__\n    return_value = get_return_value(\nvenv/lib/python3.14/site-packages/pyspark/errors/exceptions/captured.py:263: in deco\n    return f(*a, **kw)\n           ^^^^^^^^^^^\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nanswer = &amp;#x27;xro4973&amp;#x27;\ngateway_client = &amp;lt;py4j.clientserver.JavaClient object at 0x10ca1c6e0&amp;gt;\ntarget_id = &amp;#x27;o4972&amp;#x27;, name = &amp;#x27;count&amp;#x27;\n\n    def get_return_value(answer, gateway_client, target_id=None, name=None):\n        &amp;quot;&amp;quot;&amp;quot;Converts an answer received from the Java gateway into a Python object.\n    \n        For example, string representation of integers are converted to Python\n        integer, string representation of objects are converted to JavaObject\n        instances, etc.\n    \n        :param answer: the string returned by the Java gateway\n        :param gateway_client: the gateway client used to communicate with the Java\n            Gateway. Only necessary if the answer is a reference (e.g., object,\n            list, map)\n        :param target_id: the name of the object from which the answer comes from\n            (e.g., *object1* in `object1.hello()`). Optional.\n        :param name: the name of the member from which the answer comes from\n            (e.g., *hello* in `object1.hello()`). Optional.\n        &amp;quot;&amp;quot;&amp;quot;\n        if is_error(answer)[0]:\n            if len(answer) &amp;gt; 1:\n                type = answer[1]\n                value = OUTPUT_CONVERTER[type](answer[2:], gateway_client)\n                if answer[1] == REFERENCE_TYPE:\n&amp;gt;                   raise Py4JJavaError(\n                        &amp;quot;An error occurred while calling {0}{1}{2}.\\n&amp;quot;.\n                        format(target_id, &amp;quot;.&amp;quot;, name), value)\nE                   py4j.protocol.Py4JJavaError: An error occurred while calling o4972.count.\nE                   : org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 22.0 failed 1 times, most recent failure: Lost task 0.0 in stage 22.0 (TID 20) (10.20.80.191 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\nE                       check_python_version(infile)\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\nE                       raise PySparkRuntimeError(\nE                   pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nE                   Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\nE                   \nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\nE                   \tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\nE                   \tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\nE                   \tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\nE                   \tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\nE                   \tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\nE                   \tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\nE                   \tat org.apache.spark.scheduler.Task.run(Task.scala:147)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\nE                   \tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\nE                   \tat java.base/java.lang.Thread.run(Thread.java:840)\nE                   \nE                   Driver stacktrace:\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$3(DAGScheduler.scala:3122)\nE                   \tat scala.Option.getOrElse(Option.scala:201)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:3122)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:3114)\nE                   \tat scala.collection.immutable.List.foreach(List.scala:323)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:3114)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1303)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1303)\nE                   \tat scala.Option.foreach(Option.scala:437)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1303)\nE                   \tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3397)\nE                   \tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3328)\nE                   \tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3317)\nE                   \tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:50)\nE                   Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\nE                       check_python_version(infile)\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\nE                       raise PySparkRuntimeError(\nE                   pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nE                   Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\nE                   \nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\nE                   \tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\nE                   \tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\nE                   \tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\nE                   \tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\nE                   \tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\nE                   \tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\nE                   \tat org.apache.spark.scheduler.Task.run(Task.scala:147)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\nE                   \tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\nE                   \tat java.base/java.lang.Thread.run(Thread.java:840)\n\nvenv/lib/python3.14/site-packages/py4j/protocol.py:327: Py4JJavaError\n\n----------------------------- Captured stderr call -----------------------------\n26/01/22 12:17:25 ERROR Executor: Exception in task 0.0 in stage 22.0 (TID 20)\norg.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\n    check_python_version(infile)\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\n    raise PySparkRuntimeError(\npyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nPlease check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n26/01/22 12:17:25 WARN TaskSetManager: Lost task 0.0 in stage 22.0 (TID 20) (10.20.80.191 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\n    check_python_version(infile)\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\n    raise PySparkRuntimeError(\npyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nPlease check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n\n26/01/22 12:17:25 ERROR TaskSetManager: Task 0 in stage 22.0 failed 1 times; aborting job\n&#34;}], &#34;tests/test_budgets_transform.py::test_transform_budgets_mixed_valid_invalid&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/test_budgets_transform.py::test_transform_budgets_mixed_valid_invalid&#34;, &#34;duration&#34;: &#34;212 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/test_budgets_transform.py::test_transform_budgets_mixed_valid_invalid&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;212 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;spark = &amp;lt;pyspark.sql.session.SparkSession object at 0x10ca1de80&amp;gt;\nbudgets_schema = StructType([StructField(&amp;#x27;results&amp;#x27;, ArrayType(StructType([StructField(&amp;#x27;id&amp;#x27;, StringType(), True), StructField(&amp;#x27;uuid&amp;#x27;, St...tructField(&amp;#x27;pending&amp;#x27;, DoubleType(), True), StructField(&amp;#x27;total&amp;#x27;, DoubleType(), True)]), True)]), True)]), True), True)])\n\n    def test_transform_budgets_mixed_valid_invalid(spark, budgets_schema):\n        raw_data = [{\n            &amp;quot;results&amp;quot;: [\n                {\n                    &amp;quot;id&amp;quot;: &amp;quot;b1&amp;quot;,\n                    &amp;quot;uuid&amp;quot;: &amp;quot;u1&amp;quot;,\n                    &amp;quot;name&amp;quot;: &amp;quot;AWS&amp;quot;,\n                    &amp;quot;description&amp;quot;: &amp;quot;Cloud&amp;quot;,\n                    &amp;quot;retired&amp;quot;: False,\n                    &amp;quot;startDate&amp;quot;: &amp;quot;2024-01-01&amp;quot;,\n                    &amp;quot;recurringInterval&amp;quot;: &amp;quot;monthly&amp;quot;,\n                    &amp;quot;timezone&amp;quot;: &amp;quot;UTC&amp;quot;,\n                    &amp;quot;currentPeriod&amp;quot;: {\n                        &amp;quot;limit&amp;quot;: 1000.0,\n                        &amp;quot;overspendBuffer&amp;quot;: 100.0,\n                        &amp;quot;assigned&amp;quot;: 900.0,\n                        &amp;quot;spent&amp;quot;: {&amp;quot;cleared&amp;quot;: 200.0, &amp;quot;pending&amp;quot;: 50.0, &amp;quot;total&amp;quot;: 250.0}\n                    }\n                },\n                {\n                    &amp;quot;id&amp;quot;: None,\n                    &amp;quot;uuid&amp;quot;: &amp;quot;u2&amp;quot;,\n                    &amp;quot;name&amp;quot;: &amp;quot;GCP&amp;quot;,\n                    &amp;quot;description&amp;quot;: &amp;quot;Infra&amp;quot;,\n                    &amp;quot;retired&amp;quot;: False,\n                    &amp;quot;startDate&amp;quot;: &amp;quot;2024-02-01&amp;quot;,\n                    &amp;quot;recurringInterval&amp;quot;: &amp;quot;monthly&amp;quot;,\n                    &amp;quot;timezone&amp;quot;: &amp;quot;UTC&amp;quot;,\n                    &amp;quot;currentPeriod&amp;quot;: {\n                        &amp;quot;limit&amp;quot;: 2000.0,\n                        &amp;quot;overspendBuffer&amp;quot;: 200.0,\n                        &amp;quot;assigned&amp;quot;: 1800.0,\n                        &amp;quot;spent&amp;quot;: {&amp;quot;cleared&amp;quot;: 300.0, &amp;quot;pending&amp;quot;: 20.0, &amp;quot;total&amp;quot;: 320.0}\n                    }\n                }\n            ]\n        }]\n    \n        raw_df = spark.createDataFrame(raw_data, budgets_schema)\n        valid_df, error_df = transform_budgets(raw_df, day=&amp;quot;2024-01-10&amp;quot;)\n    \n&amp;gt;       assert valid_df.count() == 1\n               ^^^^^^^^^^^^^^^^\n\ntests/test_budgets_transform.py:244: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \nvenv/lib/python3.14/site-packages/pyspark/sql/classic/dataframe.py:439: in count\n    return int(self._jdf.count())\n               ^^^^^^^^^^^^^^^^^\nvenv/lib/python3.14/site-packages/py4j/java_gateway.py:1362: in __call__\n    return_value = get_return_value(\nvenv/lib/python3.14/site-packages/pyspark/errors/exceptions/captured.py:263: in deco\n    return f(*a, **kw)\n           ^^^^^^^^^^^\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nanswer = &amp;#x27;xro5163&amp;#x27;\ngateway_client = &amp;lt;py4j.clientserver.JavaClient object at 0x10ca1c6e0&amp;gt;\ntarget_id = &amp;#x27;o5162&amp;#x27;, name = &amp;#x27;count&amp;#x27;\n\n    def get_return_value(answer, gateway_client, target_id=None, name=None):\n        &amp;quot;&amp;quot;&amp;quot;Converts an answer received from the Java gateway into a Python object.\n    \n        For example, string representation of integers are converted to Python\n        integer, string representation of objects are converted to JavaObject\n        instances, etc.\n    \n        :param answer: the string returned by the Java gateway\n        :param gateway_client: the gateway client used to communicate with the Java\n            Gateway. Only necessary if the answer is a reference (e.g., object,\n            list, map)\n        :param target_id: the name of the object from which the answer comes from\n            (e.g., *object1* in `object1.hello()`). Optional.\n        :param name: the name of the member from which the answer comes from\n            (e.g., *hello* in `object1.hello()`). Optional.\n        &amp;quot;&amp;quot;&amp;quot;\n        if is_error(answer)[0]:\n            if len(answer) &amp;gt; 1:\n                type = answer[1]\n                value = OUTPUT_CONVERTER[type](answer[2:], gateway_client)\n                if answer[1] == REFERENCE_TYPE:\n&amp;gt;                   raise Py4JJavaError(\n                        &amp;quot;An error occurred while calling {0}{1}{2}.\\n&amp;quot;.\n                        format(target_id, &amp;quot;.&amp;quot;, name), value)\nE                   py4j.protocol.Py4JJavaError: An error occurred while calling o5162.count.\nE                   : org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 23.0 failed 1 times, most recent failure: Lost task 0.0 in stage 23.0 (TID 21) (10.20.80.191 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\nE                       check_python_version(infile)\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\nE                       raise PySparkRuntimeError(\nE                   pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nE                   Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\nE                   \nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\nE                   \tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\nE                   \tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\nE                   \tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\nE                   \tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\nE                   \tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\nE                   \tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\nE                   \tat org.apache.spark.scheduler.Task.run(Task.scala:147)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\nE                   \tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\nE                   \tat java.base/java.lang.Thread.run(Thread.java:840)\nE                   \nE                   Driver stacktrace:\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$3(DAGScheduler.scala:3122)\nE                   \tat scala.Option.getOrElse(Option.scala:201)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:3122)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:3114)\nE                   \tat scala.collection.immutable.List.foreach(List.scala:323)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:3114)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1303)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1303)\nE                   \tat scala.Option.foreach(Option.scala:437)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1303)\nE                   \tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3397)\nE                   \tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3328)\nE                   \tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3317)\nE                   \tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:50)\nE                   Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\nE                       check_python_version(infile)\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\nE                       raise PySparkRuntimeError(\nE                   pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nE                   Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\nE                   \nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\nE                   \tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\nE                   \tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\nE                   \tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\nE                   \tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\nE                   \tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\nE                   \tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\nE                   \tat org.apache.spark.scheduler.Task.run(Task.scala:147)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\nE                   \tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\nE                   \tat java.base/java.lang.Thread.run(Thread.java:840)\n\nvenv/lib/python3.14/site-packages/py4j/protocol.py:327: Py4JJavaError\n\n----------------------------- Captured stderr call -----------------------------\n26/01/22 12:17:26 ERROR Executor: Exception in task 0.0 in stage 23.0 (TID 21)\norg.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\n    check_python_version(infile)\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\n    raise PySparkRuntimeError(\npyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nPlease check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n26/01/22 12:17:26 WARN TaskSetManager: Lost task 0.0 in stage 23.0 (TID 21) (10.20.80.191 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\n    check_python_version(infile)\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\n    raise PySparkRuntimeError(\npyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nPlease check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n\n26/01/22 12:17:26 ERROR TaskSetManager: Task 0 in stage 23.0 failed 1 times; aborting job\n&#34;}], &#34;tests/test_budgets_transform.py::test_transform_budgets_deduplication&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/test_budgets_transform.py::test_transform_budgets_deduplication&#34;, &#34;duration&#34;: &#34;153 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/test_budgets_transform.py::test_transform_budgets_deduplication&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;153 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;spark = &amp;lt;pyspark.sql.session.SparkSession object at 0x10ca1de80&amp;gt;\nbudgets_schema = StructType([StructField(&amp;#x27;results&amp;#x27;, ArrayType(StructType([StructField(&amp;#x27;id&amp;#x27;, StringType(), True), StructField(&amp;#x27;uuid&amp;#x27;, St...tructField(&amp;#x27;pending&amp;#x27;, DoubleType(), True), StructField(&amp;#x27;total&amp;#x27;, DoubleType(), True)]), True)]), True)]), True), True)])\n\n    def test_transform_budgets_deduplication(spark, budgets_schema):\n        raw_data = [{\n            &amp;quot;results&amp;quot;: [\n                {\n                    &amp;quot;id&amp;quot;: &amp;quot;b1&amp;quot;,\n                    &amp;quot;uuid&amp;quot;: &amp;quot;u1&amp;quot;,\n                    &amp;quot;name&amp;quot;: &amp;quot;AWS&amp;quot;,\n                    &amp;quot;description&amp;quot;: &amp;quot;Cloud&amp;quot;,\n                    &amp;quot;retired&amp;quot;: False,\n                    &amp;quot;startDate&amp;quot;: &amp;quot;2024-01-01&amp;quot;,\n                    &amp;quot;recurringInterval&amp;quot;: &amp;quot;monthly&amp;quot;,\n                    &amp;quot;timezone&amp;quot;: &amp;quot;UTC&amp;quot;,\n                    &amp;quot;currentPeriod&amp;quot;: {\n                        &amp;quot;limit&amp;quot;: 1000.0,\n                        &amp;quot;overspendBuffer&amp;quot;: 100.0,\n                        &amp;quot;assigned&amp;quot;: 900.0,\n                        &amp;quot;spent&amp;quot;: {&amp;quot;cleared&amp;quot;: 200.0, &amp;quot;pending&amp;quot;: 50.0, &amp;quot;total&amp;quot;: 250.0}\n                    }\n                },\n                {\n                    &amp;quot;id&amp;quot;: &amp;quot;b1&amp;quot;,\n                    &amp;quot;uuid&amp;quot;: &amp;quot;u1-dup&amp;quot;,\n                    &amp;quot;name&amp;quot;: &amp;quot;AWS Duplicate&amp;quot;,\n                    &amp;quot;description&amp;quot;: &amp;quot;Cloud&amp;quot;,\n                    &amp;quot;retired&amp;quot;: False,\n                    &amp;quot;startDate&amp;quot;: &amp;quot;2024-01-01&amp;quot;,\n                    &amp;quot;recurringInterval&amp;quot;: &amp;quot;monthly&amp;quot;,\n                    &amp;quot;timezone&amp;quot;: &amp;quot;UTC&amp;quot;,\n                    &amp;quot;currentPeriod&amp;quot;: {\n                        &amp;quot;limit&amp;quot;: 1000.0,\n                        &amp;quot;overspendBuffer&amp;quot;: 100.0,\n                        &amp;quot;assigned&amp;quot;: 900.0,\n                        &amp;quot;spent&amp;quot;: {&amp;quot;cleared&amp;quot;: 200.0, &amp;quot;pending&amp;quot;: 50.0, &amp;quot;total&amp;quot;: 250.0}\n                    }\n                }\n            ]\n        }]\n    \n        raw_df = spark.createDataFrame(raw_data, budgets_schema)\n        valid_df, error_df = transform_budgets(raw_df, day=&amp;quot;2024-01-10&amp;quot;)\n    \n&amp;gt;       assert valid_df.count() == 1\n               ^^^^^^^^^^^^^^^^\n\ntests/test_budgets_transform.py:292: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \nvenv/lib/python3.14/site-packages/pyspark/sql/classic/dataframe.py:439: in count\n    return int(self._jdf.count())\n               ^^^^^^^^^^^^^^^^^\nvenv/lib/python3.14/site-packages/py4j/java_gateway.py:1362: in __call__\n    return_value = get_return_value(\nvenv/lib/python3.14/site-packages/pyspark/errors/exceptions/captured.py:263: in deco\n    return f(*a, **kw)\n           ^^^^^^^^^^^\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nanswer = &amp;#x27;xro5353&amp;#x27;\ngateway_client = &amp;lt;py4j.clientserver.JavaClient object at 0x10ca1c6e0&amp;gt;\ntarget_id = &amp;#x27;o5352&amp;#x27;, name = &amp;#x27;count&amp;#x27;\n\n    def get_return_value(answer, gateway_client, target_id=None, name=None):\n        &amp;quot;&amp;quot;&amp;quot;Converts an answer received from the Java gateway into a Python object.\n    \n        For example, string representation of integers are converted to Python\n        integer, string representation of objects are converted to JavaObject\n        instances, etc.\n    \n        :param answer: the string returned by the Java gateway\n        :param gateway_client: the gateway client used to communicate with the Java\n            Gateway. Only necessary if the answer is a reference (e.g., object,\n            list, map)\n        :param target_id: the name of the object from which the answer comes from\n            (e.g., *object1* in `object1.hello()`). Optional.\n        :param name: the name of the member from which the answer comes from\n            (e.g., *hello* in `object1.hello()`). Optional.\n        &amp;quot;&amp;quot;&amp;quot;\n        if is_error(answer)[0]:\n            if len(answer) &amp;gt; 1:\n                type = answer[1]\n                value = OUTPUT_CONVERTER[type](answer[2:], gateway_client)\n                if answer[1] == REFERENCE_TYPE:\n&amp;gt;                   raise Py4JJavaError(\n                        &amp;quot;An error occurred while calling {0}{1}{2}.\\n&amp;quot;.\n                        format(target_id, &amp;quot;.&amp;quot;, name), value)\nE                   py4j.protocol.Py4JJavaError: An error occurred while calling o5352.count.\nE                   : org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 24.0 failed 1 times, most recent failure: Lost task 0.0 in stage 24.0 (TID 22) (10.20.80.191 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\nE                       check_python_version(infile)\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\nE                       raise PySparkRuntimeError(\nE                   pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nE                   Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\nE                   \nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\nE                   \tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\nE                   \tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\nE                   \tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\nE                   \tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\nE                   \tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\nE                   \tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\nE                   \tat org.apache.spark.scheduler.Task.run(Task.scala:147)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\nE                   \tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\nE                   \tat java.base/java.lang.Thread.run(Thread.java:840)\nE                   \nE                   Driver stacktrace:\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$3(DAGScheduler.scala:3122)\nE                   \tat scala.Option.getOrElse(Option.scala:201)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:3122)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:3114)\nE                   \tat scala.collection.immutable.List.foreach(List.scala:323)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:3114)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1303)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1303)\nE                   \tat scala.Option.foreach(Option.scala:437)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1303)\nE                   \tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3397)\nE                   \tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3328)\nE                   \tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3317)\nE                   \tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:50)\nE                   Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\nE                       check_python_version(infile)\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\nE                       raise PySparkRuntimeError(\nE                   pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nE                   Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\nE                   \nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\nE                   \tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\nE                   \tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\nE                   \tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\nE                   \tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\nE                   \tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\nE                   \tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\nE                   \tat org.apache.spark.scheduler.Task.run(Task.scala:147)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\nE                   \tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\nE                   \tat java.base/java.lang.Thread.run(Thread.java:840)\n\nvenv/lib/python3.14/site-packages/py4j/protocol.py:327: Py4JJavaError\n\n----------------------------- Captured stderr call -----------------------------\n26/01/22 12:17:26 ERROR Executor: Exception in task 0.0 in stage 24.0 (TID 22)\norg.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\n    check_python_version(infile)\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\n    raise PySparkRuntimeError(\npyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nPlease check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n26/01/22 12:17:26 WARN TaskSetManager: Lost task 0.0 in stage 24.0 (TID 22) (10.20.80.191 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\n    check_python_version(infile)\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\n    raise PySparkRuntimeError(\npyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nPlease check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n\n26/01/22 12:17:26 ERROR TaskSetManager: Task 0 in stage 24.0 failed 1 times; aborting job\n&#34;}], &#34;tests/test_budgets_transform.py::test_transform_budgets_empty_results&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/test_budgets_transform.py::test_transform_budgets_empty_results&#34;, &#34;duration&#34;: &#34;173 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/test_budgets_transform.py::test_transform_budgets_empty_results&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;173 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;spark = &amp;lt;pyspark.sql.session.SparkSession object at 0x10ca1de80&amp;gt;\nbudgets_schema = StructType([StructField(&amp;#x27;results&amp;#x27;, ArrayType(StructType([StructField(&amp;#x27;id&amp;#x27;, StringType(), True), StructField(&amp;#x27;uuid&amp;#x27;, St...tructField(&amp;#x27;pending&amp;#x27;, DoubleType(), True), StructField(&amp;#x27;total&amp;#x27;, DoubleType(), True)]), True)]), True)]), True), True)])\n\n    def test_transform_budgets_empty_results(spark, budgets_schema):\n        raw_data = [{&amp;quot;results&amp;quot;: []}]\n        raw_df = spark.createDataFrame(raw_data, budgets_schema)\n    \n        valid_df, error_df = transform_budgets(raw_df, day=&amp;quot;2024-01-10&amp;quot;)\n    \n&amp;gt;       assert valid_df.count() == 0\n               ^^^^^^^^^^^^^^^^\n\ntests/test_budgets_transform.py:305: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \nvenv/lib/python3.14/site-packages/pyspark/sql/classic/dataframe.py:439: in count\n    return int(self._jdf.count())\n               ^^^^^^^^^^^^^^^^^\nvenv/lib/python3.14/site-packages/py4j/java_gateway.py:1362: in __call__\n    return_value = get_return_value(\nvenv/lib/python3.14/site-packages/pyspark/errors/exceptions/captured.py:263: in deco\n    return f(*a, **kw)\n           ^^^^^^^^^^^\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nanswer = &amp;#x27;xro5543&amp;#x27;\ngateway_client = &amp;lt;py4j.clientserver.JavaClient object at 0x10ca1c6e0&amp;gt;\ntarget_id = &amp;#x27;o5542&amp;#x27;, name = &amp;#x27;count&amp;#x27;\n\n    def get_return_value(answer, gateway_client, target_id=None, name=None):\n        &amp;quot;&amp;quot;&amp;quot;Converts an answer received from the Java gateway into a Python object.\n    \n        For example, string representation of integers are converted to Python\n        integer, string representation of objects are converted to JavaObject\n        instances, etc.\n    \n        :param answer: the string returned by the Java gateway\n        :param gateway_client: the gateway client used to communicate with the Java\n            Gateway. Only necessary if the answer is a reference (e.g., object,\n            list, map)\n        :param target_id: the name of the object from which the answer comes from\n            (e.g., *object1* in `object1.hello()`). Optional.\n        :param name: the name of the member from which the answer comes from\n            (e.g., *hello* in `object1.hello()`). Optional.\n        &amp;quot;&amp;quot;&amp;quot;\n        if is_error(answer)[0]:\n            if len(answer) &amp;gt; 1:\n                type = answer[1]\n                value = OUTPUT_CONVERTER[type](answer[2:], gateway_client)\n                if answer[1] == REFERENCE_TYPE:\n&amp;gt;                   raise Py4JJavaError(\n                        &amp;quot;An error occurred while calling {0}{1}{2}.\\n&amp;quot;.\n                        format(target_id, &amp;quot;.&amp;quot;, name), value)\nE                   py4j.protocol.Py4JJavaError: An error occurred while calling o5542.count.\nE                   : org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 25.0 failed 1 times, most recent failure: Lost task 0.0 in stage 25.0 (TID 23) (10.20.80.191 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\nE                       check_python_version(infile)\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\nE                       raise PySparkRuntimeError(\nE                   pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nE                   Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\nE                   \nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\nE                   \tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\nE                   \tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\nE                   \tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\nE                   \tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\nE                   \tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\nE                   \tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\nE                   \tat org.apache.spark.scheduler.Task.run(Task.scala:147)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\nE                   \tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\nE                   \tat java.base/java.lang.Thread.run(Thread.java:840)\nE                   \nE                   Driver stacktrace:\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$3(DAGScheduler.scala:3122)\nE                   \tat scala.Option.getOrElse(Option.scala:201)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:3122)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:3114)\nE                   \tat scala.collection.immutable.List.foreach(List.scala:323)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:3114)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1303)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1303)\nE                   \tat scala.Option.foreach(Option.scala:437)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1303)\nE                   \tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3397)\nE                   \tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3328)\nE                   \tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3317)\nE                   \tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:50)\nE                   Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\nE                       check_python_version(infile)\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\nE                       raise PySparkRuntimeError(\nE                   pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nE                   Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\nE                   \nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\nE                   \tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\nE                   \tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\nE                   \tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\nE                   \tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\nE                   \tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\nE                   \tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\nE                   \tat org.apache.spark.scheduler.Task.run(Task.scala:147)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\nE                   \tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\nE                   \tat java.base/java.lang.Thread.run(Thread.java:840)\n\nvenv/lib/python3.14/site-packages/py4j/protocol.py:327: Py4JJavaError\n\n----------------------------- Captured stderr call -----------------------------\n26/01/22 12:17:26 ERROR Executor: Exception in task 0.0 in stage 25.0 (TID 23)\norg.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\n    check_python_version(infile)\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\n    raise PySparkRuntimeError(\npyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nPlease check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n26/01/22 12:17:26 WARN TaskSetManager: Lost task 0.0 in stage 25.0 (TID 23) (10.20.80.191 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\n    check_python_version(infile)\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\n    raise PySparkRuntimeError(\npyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nPlease check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n\n26/01/22 12:17:26 ERROR TaskSetManager: Task 0 in stage 25.0 failed 1 times; aborting job\n&#34;}], &#34;tests/test_budgets_transform.py::test_transform_budgets_null_results&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/test_budgets_transform.py::test_transform_budgets_null_results&#34;, &#34;duration&#34;: &#34;175 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/test_budgets_transform.py::test_transform_budgets_null_results&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;175 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;spark = &amp;lt;pyspark.sql.session.SparkSession object at 0x10ca1de80&amp;gt;\nbudgets_schema = StructType([StructField(&amp;#x27;results&amp;#x27;, ArrayType(StructType([StructField(&amp;#x27;id&amp;#x27;, StringType(), True), StructField(&amp;#x27;uuid&amp;#x27;, St...tructField(&amp;#x27;pending&amp;#x27;, DoubleType(), True), StructField(&amp;#x27;total&amp;#x27;, DoubleType(), True)]), True)]), True)]), True), True)])\n\n    def test_transform_budgets_null_results(spark, budgets_schema):\n        raw_data = [{&amp;quot;results&amp;quot;: None}]\n        raw_df = spark.createDataFrame(raw_data, budgets_schema)\n    \n        valid_df, error_df = transform_budgets(raw_df, day=&amp;quot;2024-01-10&amp;quot;)\n    \n&amp;gt;       assert valid_df.count() == 0\n               ^^^^^^^^^^^^^^^^\n\ntests/test_budgets_transform.py:318: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \nvenv/lib/python3.14/site-packages/pyspark/sql/classic/dataframe.py:439: in count\n    return int(self._jdf.count())\n               ^^^^^^^^^^^^^^^^^\nvenv/lib/python3.14/site-packages/py4j/java_gateway.py:1362: in __call__\n    return_value = get_return_value(\nvenv/lib/python3.14/site-packages/pyspark/errors/exceptions/captured.py:263: in deco\n    return f(*a, **kw)\n           ^^^^^^^^^^^\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nanswer = &amp;#x27;xro5733&amp;#x27;\ngateway_client = &amp;lt;py4j.clientserver.JavaClient object at 0x10ca1c6e0&amp;gt;\ntarget_id = &amp;#x27;o5732&amp;#x27;, name = &amp;#x27;count&amp;#x27;\n\n    def get_return_value(answer, gateway_client, target_id=None, name=None):\n        &amp;quot;&amp;quot;&amp;quot;Converts an answer received from the Java gateway into a Python object.\n    \n        For example, string representation of integers are converted to Python\n        integer, string representation of objects are converted to JavaObject\n        instances, etc.\n    \n        :param answer: the string returned by the Java gateway\n        :param gateway_client: the gateway client used to communicate with the Java\n            Gateway. Only necessary if the answer is a reference (e.g., object,\n            list, map)\n        :param target_id: the name of the object from which the answer comes from\n            (e.g., *object1* in `object1.hello()`). Optional.\n        :param name: the name of the member from which the answer comes from\n            (e.g., *hello* in `object1.hello()`). Optional.\n        &amp;quot;&amp;quot;&amp;quot;\n        if is_error(answer)[0]:\n            if len(answer) &amp;gt; 1:\n                type = answer[1]\n                value = OUTPUT_CONVERTER[type](answer[2:], gateway_client)\n                if answer[1] == REFERENCE_TYPE:\n&amp;gt;                   raise Py4JJavaError(\n                        &amp;quot;An error occurred while calling {0}{1}{2}.\\n&amp;quot;.\n                        format(target_id, &amp;quot;.&amp;quot;, name), value)\nE                   py4j.protocol.Py4JJavaError: An error occurred while calling o5732.count.\nE                   : org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 26.0 failed 1 times, most recent failure: Lost task 0.0 in stage 26.0 (TID 24) (10.20.80.191 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\nE                       check_python_version(infile)\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\nE                       raise PySparkRuntimeError(\nE                   pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nE                   Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\nE                   \nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\nE                   \tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\nE                   \tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\nE                   \tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\nE                   \tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\nE                   \tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\nE                   \tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\nE                   \tat org.apache.spark.scheduler.Task.run(Task.scala:147)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\nE                   \tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\nE                   \tat java.base/java.lang.Thread.run(Thread.java:840)\nE                   \nE                   Driver stacktrace:\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$3(DAGScheduler.scala:3122)\nE                   \tat scala.Option.getOrElse(Option.scala:201)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:3122)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:3114)\nE                   \tat scala.collection.immutable.List.foreach(List.scala:323)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:3114)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1303)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1303)\nE                   \tat scala.Option.foreach(Option.scala:437)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1303)\nE                   \tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3397)\nE                   \tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3328)\nE                   \tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3317)\nE                   \tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:50)\nE                   Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\nE                       check_python_version(infile)\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\nE                       raise PySparkRuntimeError(\nE                   pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nE                   Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\nE                   \nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\nE                   \tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\nE                   \tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\nE                   \tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\nE                   \tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\nE                   \tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\nE                   \tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\nE                   \tat org.apache.spark.scheduler.Task.run(Task.scala:147)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\nE                   \tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\nE                   \tat java.base/java.lang.Thread.run(Thread.java:840)\n\nvenv/lib/python3.14/site-packages/py4j/protocol.py:327: Py4JJavaError\n\n----------------------------- Captured stderr call -----------------------------\n26/01/22 12:17:26 ERROR Executor: Exception in task 0.0 in stage 26.0 (TID 24)\norg.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\n    check_python_version(infile)\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\n    raise PySparkRuntimeError(\npyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nPlease check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n26/01/22 12:17:26 WARN TaskSetManager: Lost task 0.0 in stage 26.0 (TID 24) (10.20.80.191 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\n    check_python_version(infile)\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\n    raise PySparkRuntimeError(\npyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nPlease check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n\n26/01/22 12:17:26 ERROR TaskSetManager: Task 0 in stage 26.0 failed 1 times; aborting job\n&#34;}], &#34;tests/test_budgets_transform.py::test_transform_budgets_start_date_parsing&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/test_budgets_transform.py::test_transform_budgets_start_date_parsing&#34;, &#34;duration&#34;: &#34;268 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/test_budgets_transform.py::test_transform_budgets_start_date_parsing&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;268 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;spark = &amp;lt;pyspark.sql.session.SparkSession object at 0x10ca1de80&amp;gt;\nbudgets_schema = StructType([StructField(&amp;#x27;results&amp;#x27;, ArrayType(StructType([StructField(&amp;#x27;id&amp;#x27;, StringType(), True), StructField(&amp;#x27;uuid&amp;#x27;, St...tructField(&amp;#x27;pending&amp;#x27;, DoubleType(), True), StructField(&amp;#x27;total&amp;#x27;, DoubleType(), True)]), True)]), True)]), True), True)])\n\n    def test_transform_budgets_start_date_parsing(spark, budgets_schema):\n        raw_data = [{\n            &amp;quot;results&amp;quot;: [{\n                &amp;quot;id&amp;quot;: &amp;quot;b1&amp;quot;,\n                &amp;quot;uuid&amp;quot;: &amp;quot;u1&amp;quot;,\n                &amp;quot;name&amp;quot;: &amp;quot;AWS&amp;quot;,\n                &amp;quot;description&amp;quot;: &amp;quot;Cloud&amp;quot;,\n                &amp;quot;retired&amp;quot;: False,\n                &amp;quot;startDate&amp;quot;: &amp;quot;2024-05-15&amp;quot;,\n                &amp;quot;recurringInterval&amp;quot;: &amp;quot;monthly&amp;quot;,\n                &amp;quot;timezone&amp;quot;: &amp;quot;UTC&amp;quot;,\n                &amp;quot;currentPeriod&amp;quot;: {\n                    &amp;quot;limit&amp;quot;: 1000.0,\n                    &amp;quot;overspendBuffer&amp;quot;: 100.0,\n                    &amp;quot;assigned&amp;quot;: 900.0,\n                    &amp;quot;spent&amp;quot;: {&amp;quot;cleared&amp;quot;: 200.0, &amp;quot;pending&amp;quot;: 50.0, &amp;quot;total&amp;quot;: 250.0}\n                }\n            }]\n        }]\n    \n        raw_df = spark.createDataFrame(raw_data, budgets_schema)\n        valid_df, _ = transform_budgets(raw_df, day=&amp;quot;2024-01-10&amp;quot;)\n    \n&amp;gt;       row = valid_df.collect()[0]\n              ^^^^^^^^^^^^^^^^^^\n\ntests/test_budgets_transform.py:348: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \nvenv/lib/python3.14/site-packages/pyspark/sql/classic/dataframe.py:443: in collect\n    sock_info = self._jdf.collectToPython()\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^\nvenv/lib/python3.14/site-packages/py4j/java_gateway.py:1362: in __call__\n    return_value = get_return_value(\nvenv/lib/python3.14/site-packages/pyspark/errors/exceptions/captured.py:263: in deco\n    return f(*a, **kw)\n           ^^^^^^^^^^^\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nanswer = &amp;#x27;xro5923&amp;#x27;\ngateway_client = &amp;lt;py4j.clientserver.JavaClient object at 0x10ca1c6e0&amp;gt;\ntarget_id = &amp;#x27;o5922&amp;#x27;, name = &amp;#x27;collectToPython&amp;#x27;\n\n    def get_return_value(answer, gateway_client, target_id=None, name=None):\n        &amp;quot;&amp;quot;&amp;quot;Converts an answer received from the Java gateway into a Python object.\n    \n        For example, string representation of integers are converted to Python\n        integer, string representation of objects are converted to JavaObject\n        instances, etc.\n    \n        :param answer: the string returned by the Java gateway\n        :param gateway_client: the gateway client used to communicate with the Java\n            Gateway. Only necessary if the answer is a reference (e.g., object,\n            list, map)\n        :param target_id: the name of the object from which the answer comes from\n            (e.g., *object1* in `object1.hello()`). Optional.\n        :param name: the name of the member from which the answer comes from\n            (e.g., *hello* in `object1.hello()`). Optional.\n        &amp;quot;&amp;quot;&amp;quot;\n        if is_error(answer)[0]:\n            if len(answer) &amp;gt; 1:\n                type = answer[1]\n                value = OUTPUT_CONVERTER[type](answer[2:], gateway_client)\n                if answer[1] == REFERENCE_TYPE:\n&amp;gt;                   raise Py4JJavaError(\n                        &amp;quot;An error occurred while calling {0}{1}{2}.\\n&amp;quot;.\n                        format(target_id, &amp;quot;.&amp;quot;, name), value)\nE                   py4j.protocol.Py4JJavaError: An error occurred while calling o5922.collectToPython.\nE                   : org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 27.0 failed 1 times, most recent failure: Lost task 0.0 in stage 27.0 (TID 25) (10.20.80.191 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\nE                       check_python_version(infile)\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\nE                       raise PySparkRuntimeError(\nE                   pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nE                   Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\nE                   \nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\nE                   \tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\nE                   \tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.sort_addToSorter_0$(Unknown Source)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\nE                   \tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\nE                   \tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\nE                   \tat org.apache.spark.sql.execution.aggregate.SortAggregateExec.$anonfun$doExecute$1(SortAggregateExec.scala:63)\nE                   \tat org.apache.spark.sql.execution.aggregate.SortAggregateExec.$anonfun$doExecute$1$adapted(SortAggregateExec.scala:60)\nE                   \tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:888)\nE                   \tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:888)\nE                   \tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\nE                   \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\nE                   \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\nE                   \tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\nE                   \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\nE                   \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\nE                   \tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\nE                   \tat org.apache.spark.scheduler.Task.run(Task.scala:147)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\nE                   \tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\nE                   \tat java.base/java.lang.Thread.run(Thread.java:840)\nE                   \nE                   Driver stacktrace:\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$3(DAGScheduler.scala:3122)\nE                   \tat scala.Option.getOrElse(Option.scala:201)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:3122)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:3114)\nE                   \tat scala.collection.immutable.List.foreach(List.scala:323)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:3114)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1303)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1303)\nE                   \tat scala.Option.foreach(Option.scala:437)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1303)\nE                   \tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3397)\nE                   \tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3328)\nE                   \tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3317)\nE                   \tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:50)\nE                   Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\nE                       check_python_version(infile)\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\nE                       raise PySparkRuntimeError(\nE                   pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nE                   Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\nE                   \nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\nE                   \tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\nE                   \tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.sort_addToSorter_0$(Unknown Source)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\nE                   \tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\nE                   \tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\nE                   \tat org.apache.spark.sql.execution.aggregate.SortAggregateExec.$anonfun$doExecute$1(SortAggregateExec.scala:63)\nE                   \tat org.apache.spark.sql.execution.aggregate.SortAggregateExec.$anonfun$doExecute$1$adapted(SortAggregateExec.scala:60)\nE                   \tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:888)\nE                   \tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:888)\nE                   \tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\nE                   \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\nE                   \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\nE                   \tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\nE                   \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\nE                   \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\nE                   \tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\nE                   \tat org.apache.spark.scheduler.Task.run(Task.scala:147)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\nE                   \tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\nE                   \tat java.base/java.lang.Thread.run(Thread.java:840)\n\nvenv/lib/python3.14/site-packages/py4j/protocol.py:327: Py4JJavaError\n\n----------------------------- Captured stderr call -----------------------------\n26/01/22 12:17:26 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting &amp;#x27;spark.sql.debug.maxToStringFields&amp;#x27;.\n26/01/22 12:17:27 ERROR Executor: Exception in task 0.0 in stage 27.0 (TID 25)\norg.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\n    check_python_version(infile)\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\n    raise PySparkRuntimeError(\npyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nPlease check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.sort_addToSorter_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n\tat org.apache.spark.sql.execution.aggregate.SortAggregateExec.$anonfun$doExecute$1(SortAggregateExec.scala:63)\n\tat org.apache.spark.sql.execution.aggregate.SortAggregateExec.$anonfun$doExecute$1$adapted(SortAggregateExec.scala:60)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:888)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:888)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n26/01/22 12:17:27 WARN TaskSetManager: Lost task 0.0 in stage 27.0 (TID 25) (10.20.80.191 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\n    check_python_version(infile)\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\n    raise PySparkRuntimeError(\npyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nPlease check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.sort_addToSorter_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n\tat org.apache.spark.sql.execution.aggregate.SortAggregateExec.$anonfun$doExecute$1(SortAggregateExec.scala:63)\n\tat org.apache.spark.sql.execution.aggregate.SortAggregateExec.$anonfun$doExecute$1$adapted(SortAggregateExec.scala:60)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:888)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:888)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n\n26/01/22 12:17:27 ERROR TaskSetManager: Task 0 in stage 27.0 failed 1 times; aborting job\n&#34;}], &#34;tests/test_budgets_transform.py::test_transform_budgets_schema&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Passed&#34;, &#34;testId&#34;: &#34;tests/test_budgets_transform.py::test_transform_budgets_schema&#34;, &#34;duration&#34;: &#34;91 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Passed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/test_budgets_transform.py::test_transform_budgets_schema&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;91 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;No log output captured.&#34;}], &#34;tests/test_budgets_transform.py::test_transform_budgets_non_nested_input&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/test_budgets_transform.py::test_transform_budgets_non_nested_input&#34;, &#34;duration&#34;: &#34;161 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/test_budgets_transform.py::test_transform_budgets_non_nested_input&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;161 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;spark = &amp;lt;pyspark.sql.session.SparkSession object at 0x10ca1de80&amp;gt;\n\n    def test_transform_budgets_non_nested_input(spark):\n        schema = StructType([\n            StructField(&amp;quot;id&amp;quot;, StringType(), True),\n            StructField(&amp;quot;uuid&amp;quot;, StringType(), True),\n            StructField(&amp;quot;name&amp;quot;, StringType(), True),\n            StructField(&amp;quot;description&amp;quot;, StringType(), True),\n            StructField(&amp;quot;retired&amp;quot;, BooleanType(), True),\n            StructField(&amp;quot;startDate&amp;quot;, StringType(), True),\n            StructField(&amp;quot;recurringInterval&amp;quot;, StringType(), True),\n            StructField(&amp;quot;timezone&amp;quot;, StringType(), True),\n            StructField(&amp;quot;currentPeriod&amp;quot;, StructType([\n                StructField(&amp;quot;limit&amp;quot;, DoubleType(), True),\n                StructField(&amp;quot;overspendBuffer&amp;quot;, DoubleType(), True),\n                StructField(&amp;quot;assigned&amp;quot;, DoubleType(), True),\n                StructField(&amp;quot;spent&amp;quot;, StructType([\n                    StructField(&amp;quot;cleared&amp;quot;, DoubleType(), True),\n                    StructField(&amp;quot;pending&amp;quot;, DoubleType(), True),\n                    StructField(&amp;quot;total&amp;quot;, DoubleType(), True),\n                ]), True),\n            ]), True),\n        ])\n    \n        raw_data = [{\n            &amp;quot;id&amp;quot;: &amp;quot;b1&amp;quot;,\n            &amp;quot;uuid&amp;quot;: &amp;quot;u1&amp;quot;,\n            &amp;quot;name&amp;quot;: &amp;quot;AWS&amp;quot;,\n            &amp;quot;description&amp;quot;: &amp;quot;Cloud&amp;quot;,\n            &amp;quot;retired&amp;quot;: False,\n            &amp;quot;startDate&amp;quot;: &amp;quot;2024-01-01&amp;quot;,\n            &amp;quot;recurringInterval&amp;quot;: &amp;quot;monthly&amp;quot;,\n            &amp;quot;timezone&amp;quot;: &amp;quot;UTC&amp;quot;,\n            &amp;quot;currentPeriod&amp;quot;: {\n                &amp;quot;limit&amp;quot;: 1000.0,\n                &amp;quot;overspendBuffer&amp;quot;: 100.0,\n                &amp;quot;assigned&amp;quot;: 900.0,\n                &amp;quot;spent&amp;quot;: {\n                    &amp;quot;cleared&amp;quot;: 200.0,\n                    &amp;quot;pending&amp;quot;: 50.0,\n                    &amp;quot;total&amp;quot;: 250.0\n                }\n            }\n        }]\n    \n        raw_df = spark.createDataFrame(raw_data, schema)\n    \n        # IMPORTANT: wrap row into &amp;quot;budget&amp;quot; struct\n        raw_df = raw_df.select(struct(*raw_df.columns).alias(&amp;quot;budget&amp;quot;))\n    \n        valid_df, error_df = transform_budgets(raw_df, day=&amp;quot;2024-01-10&amp;quot;)\n    \n&amp;gt;       assert valid_df.count() == 1\n               ^^^^^^^^^^^^^^^^\n\ntests/test_budgets_transform.py:443: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \nvenv/lib/python3.14/site-packages/pyspark/sql/classic/dataframe.py:439: in count\n    return int(self._jdf.count())\n               ^^^^^^^^^^^^^^^^^\nvenv/lib/python3.14/site-packages/py4j/java_gateway.py:1362: in __call__\n    return_value = get_return_value(\nvenv/lib/python3.14/site-packages/pyspark/errors/exceptions/captured.py:263: in deco\n    return f(*a, **kw)\n           ^^^^^^^^^^^\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nanswer = &amp;#x27;xro6291&amp;#x27;\ngateway_client = &amp;lt;py4j.clientserver.JavaClient object at 0x10ca1c6e0&amp;gt;\ntarget_id = &amp;#x27;o6290&amp;#x27;, name = &amp;#x27;count&amp;#x27;\n\n    def get_return_value(answer, gateway_client, target_id=None, name=None):\n        &amp;quot;&amp;quot;&amp;quot;Converts an answer received from the Java gateway into a Python object.\n    \n        For example, string representation of integers are converted to Python\n        integer, string representation of objects are converted to JavaObject\n        instances, etc.\n    \n        :param answer: the string returned by the Java gateway\n        :param gateway_client: the gateway client used to communicate with the Java\n            Gateway. Only necessary if the answer is a reference (e.g., object,\n            list, map)\n        :param target_id: the name of the object from which the answer comes from\n            (e.g., *object1* in `object1.hello()`). Optional.\n        :param name: the name of the member from which the answer comes from\n            (e.g., *hello* in `object1.hello()`). Optional.\n        &amp;quot;&amp;quot;&amp;quot;\n        if is_error(answer)[0]:\n            if len(answer) &amp;gt; 1:\n                type = answer[1]\n                value = OUTPUT_CONVERTER[type](answer[2:], gateway_client)\n                if answer[1] == REFERENCE_TYPE:\n&amp;gt;                   raise Py4JJavaError(\n                        &amp;quot;An error occurred while calling {0}{1}{2}.\\n&amp;quot;.\n                        format(target_id, &amp;quot;.&amp;quot;, name), value)\nE                   py4j.protocol.Py4JJavaError: An error occurred while calling o6290.count.\nE                   : org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 28.0 failed 1 times, most recent failure: Lost task 0.0 in stage 28.0 (TID 26) (10.20.80.191 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\nE                       check_python_version(infile)\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\nE                       raise PySparkRuntimeError(\nE                   pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nE                   Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\nE                   \nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\nE                   \tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\nE                   \tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\nE                   \tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\nE                   \tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\nE                   \tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\nE                   \tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\nE                   \tat org.apache.spark.scheduler.Task.run(Task.scala:147)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\nE                   \tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\nE                   \tat java.base/java.lang.Thread.run(Thread.java:840)\nE                   \nE                   Driver stacktrace:\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$3(DAGScheduler.scala:3122)\nE                   \tat scala.Option.getOrElse(Option.scala:201)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:3122)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:3114)\nE                   \tat scala.collection.immutable.List.foreach(List.scala:323)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:3114)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1303)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1303)\nE                   \tat scala.Option.foreach(Option.scala:437)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1303)\nE                   \tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3397)\nE                   \tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3328)\nE                   \tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3317)\nE                   \tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:50)\nE                   Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\nE                       check_python_version(infile)\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\nE                       raise PySparkRuntimeError(\nE                   pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nE                   Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\nE                   \nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\nE                   \tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\nE                   \tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\nE                   \tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\nE                   \tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\nE                   \tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\nE                   \tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\nE                   \tat org.apache.spark.scheduler.Task.run(Task.scala:147)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\nE                   \tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\nE                   \tat java.base/java.lang.Thread.run(Thread.java:840)\n\nvenv/lib/python3.14/site-packages/py4j/protocol.py:327: Py4JJavaError\n\n----------------------------- Captured stderr call -----------------------------\n26/01/22 12:17:27 ERROR Executor: Exception in task 0.0 in stage 28.0 (TID 26)\norg.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\n    check_python_version(infile)\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\n    raise PySparkRuntimeError(\npyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nPlease check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n26/01/22 12:17:27 WARN TaskSetManager: Lost task 0.0 in stage 28.0 (TID 26) (10.20.80.191 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\n    check_python_version(infile)\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\n    raise PySparkRuntimeError(\npyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nPlease check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n\n26/01/22 12:17:27 ERROR TaskSetManager: Task 0 in stage 28.0 failed 1 times; aborting job\n&#34;}], &#34;tests/test_cards_transform.py::test_transform_cards_valid_record&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/test_cards_transform.py::test_transform_cards_valid_record&#34;, &#34;duration&#34;: &#34;310 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/test_cards_transform.py::test_transform_cards_valid_record&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;310 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;spark = &amp;lt;pyspark.sql.session.SparkSession object at 0x10ca1de80&amp;gt;\n\n    def test_transform_cards_valid_record(spark):\n        raw_df = create_raw_cards_df(spark, [{\n            &amp;quot;id&amp;quot;: &amp;quot;c1&amp;quot;,\n            &amp;quot;uuid&amp;quot;: &amp;quot;u1&amp;quot;,\n            &amp;quot;name&amp;quot;: &amp;quot;Card 1&amp;quot;,\n            &amp;quot;userId&amp;quot;: &amp;quot;user1&amp;quot;,\n            &amp;quot;userUuid&amp;quot;: &amp;quot;uu1&amp;quot;,\n            &amp;quot;budgetId&amp;quot;: &amp;quot;b1&amp;quot;,\n            &amp;quot;budgetUuid&amp;quot;: &amp;quot;bu1&amp;quot;,\n            &amp;quot;lastFour&amp;quot;: &amp;quot;1234&amp;quot;,\n            &amp;quot;validThru&amp;quot;: &amp;quot;12/25&amp;quot;,\n            &amp;quot;status&amp;quot;: &amp;quot;active&amp;quot;,\n            &amp;quot;type&amp;quot;: &amp;quot;virtual&amp;quot;,\n            &amp;quot;shareBudgetFunds&amp;quot;: &amp;quot;true&amp;quot;,\n            &amp;quot;recurring&amp;quot;: &amp;quot;false&amp;quot;,\n            &amp;quot;recurringLimit&amp;quot;: 100.0,\n            &amp;quot;currentPeriod&amp;quot;: {&amp;quot;limit&amp;quot;: 500.0, &amp;quot;spent&amp;quot;: 200.0},\n            &amp;quot;createdTime&amp;quot;: &amp;quot;2024-01-01T10:00:00&amp;quot;,\n            &amp;quot;updatedTime&amp;quot;: &amp;quot;2024-01-02T10:00:00&amp;quot;\n        }])\n    \n        valid_df, error_df = transform_cards(raw_df, &amp;quot;day1&amp;quot;)\n    \n&amp;gt;       assert valid_df.count() == 1\n               ^^^^^^^^^^^^^^^^\n\ntests/test_cards_transform.py:94: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \nvenv/lib/python3.14/site-packages/pyspark/sql/classic/dataframe.py:439: in count\n    return int(self._jdf.count())\n               ^^^^^^^^^^^^^^^^^\nvenv/lib/python3.14/site-packages/py4j/java_gateway.py:1362: in __call__\n    return_value = get_return_value(\nvenv/lib/python3.14/site-packages/pyspark/errors/exceptions/captured.py:263: in deco\n    return f(*a, **kw)\n           ^^^^^^^^^^^\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nanswer = &amp;#x27;xro6578&amp;#x27;\ngateway_client = &amp;lt;py4j.clientserver.JavaClient object at 0x10ca1c6e0&amp;gt;\ntarget_id = &amp;#x27;o6577&amp;#x27;, name = &amp;#x27;count&amp;#x27;\n\n    def get_return_value(answer, gateway_client, target_id=None, name=None):\n        &amp;quot;&amp;quot;&amp;quot;Converts an answer received from the Java gateway into a Python object.\n    \n        For example, string representation of integers are converted to Python\n        integer, string representation of objects are converted to JavaObject\n        instances, etc.\n    \n        :param answer: the string returned by the Java gateway\n        :param gateway_client: the gateway client used to communicate with the Java\n            Gateway. Only necessary if the answer is a reference (e.g., object,\n            list, map)\n        :param target_id: the name of the object from which the answer comes from\n            (e.g., *object1* in `object1.hello()`). Optional.\n        :param name: the name of the member from which the answer comes from\n            (e.g., *hello* in `object1.hello()`). Optional.\n        &amp;quot;&amp;quot;&amp;quot;\n        if is_error(answer)[0]:\n            if len(answer) &amp;gt; 1:\n                type = answer[1]\n                value = OUTPUT_CONVERTER[type](answer[2:], gateway_client)\n                if answer[1] == REFERENCE_TYPE:\n&amp;gt;                   raise Py4JJavaError(\n                        &amp;quot;An error occurred while calling {0}{1}{2}.\\n&amp;quot;.\n                        format(target_id, &amp;quot;.&amp;quot;, name), value)\nE                   py4j.protocol.Py4JJavaError: An error occurred while calling o6577.count.\nE                   : org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 29.0 failed 1 times, most recent failure: Lost task 0.0 in stage 29.0 (TID 27) (10.20.80.191 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\nE                       check_python_version(infile)\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\nE                       raise PySparkRuntimeError(\nE                   pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nE                   Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\nE                   \nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\nE                   \tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\nE                   \tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\nE                   \tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\nE                   \tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\nE                   \tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\nE                   \tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\nE                   \tat org.apache.spark.scheduler.Task.run(Task.scala:147)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\nE                   \tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\nE                   \tat java.base/java.lang.Thread.run(Thread.java:840)\nE                   \nE                   Driver stacktrace:\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$3(DAGScheduler.scala:3122)\nE                   \tat scala.Option.getOrElse(Option.scala:201)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:3122)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:3114)\nE                   \tat scala.collection.immutable.List.foreach(List.scala:323)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:3114)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1303)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1303)\nE                   \tat scala.Option.foreach(Option.scala:437)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1303)\nE                   \tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3397)\nE                   \tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3328)\nE                   \tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3317)\nE                   \tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:50)\nE                   Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\nE                       check_python_version(infile)\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\nE                       raise PySparkRuntimeError(\nE                   pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nE                   Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\nE                   \nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\nE                   \tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\nE                   \tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\nE                   \tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\nE                   \tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\nE                   \tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\nE                   \tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\nE                   \tat org.apache.spark.scheduler.Task.run(Task.scala:147)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\nE                   \tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\nE                   \tat java.base/java.lang.Thread.run(Thread.java:840)\n\nvenv/lib/python3.14/site-packages/py4j/protocol.py:327: Py4JJavaError\n\n---------------------------- Captured stderr setup -----------------------------\n26/01/22 12:17:27 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n\n----------------------------- Captured stderr call -----------------------------\n26/01/22 12:17:27 ERROR Executor: Exception in task 0.0 in stage 29.0 (TID 27)\norg.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\n    check_python_version(infile)\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\n    raise PySparkRuntimeError(\npyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nPlease check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n26/01/22 12:17:27 WARN TaskSetManager: Lost task 0.0 in stage 29.0 (TID 27) (10.20.80.191 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\n    check_python_version(infile)\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\n    raise PySparkRuntimeError(\npyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nPlease check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n\n26/01/22 12:17:27 ERROR TaskSetManager: Task 0 in stage 29.0 failed 1 times; aborting job\n&#34;}], &#34;tests/test_cards_transform.py::test_transform_cards_null_card_id&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/test_cards_transform.py::test_transform_cards_null_card_id&#34;, &#34;duration&#34;: &#34;236 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/test_cards_transform.py::test_transform_cards_null_card_id&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;236 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;spark = &amp;lt;pyspark.sql.session.SparkSession object at 0x10ca1de80&amp;gt;\n\n    def test_transform_cards_null_card_id(spark):\n        raw_df = create_raw_cards_df(spark, [{\n            &amp;quot;id&amp;quot;: None,\n            &amp;quot;status&amp;quot;: &amp;quot;active&amp;quot;,\n            &amp;quot;type&amp;quot;: &amp;quot;virtual&amp;quot;,\n            &amp;quot;createdTime&amp;quot;: &amp;quot;2024-01-01T10:00:00&amp;quot;,\n            &amp;quot;currentPeriod&amp;quot;: {&amp;quot;limit&amp;quot;: 100.0, &amp;quot;spent&amp;quot;: 10.0}\n        }])\n    \n        valid_df, error_df = transform_cards(raw_df, &amp;quot;day1&amp;quot;)\n    \n&amp;gt;       assert valid_df.count() == 0\n               ^^^^^^^^^^^^^^^^\n\ntests/test_cards_transform.py:112: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \nvenv/lib/python3.14/site-packages/pyspark/sql/classic/dataframe.py:439: in count\n    return int(self._jdf.count())\n               ^^^^^^^^^^^^^^^^^\nvenv/lib/python3.14/site-packages/py4j/java_gateway.py:1362: in __call__\n    return_value = get_return_value(\nvenv/lib/python3.14/site-packages/pyspark/errors/exceptions/captured.py:263: in deco\n    return f(*a, **kw)\n           ^^^^^^^^^^^\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nanswer = &amp;#x27;xro6863&amp;#x27;\ngateway_client = &amp;lt;py4j.clientserver.JavaClient object at 0x10ca1c6e0&amp;gt;\ntarget_id = &amp;#x27;o6862&amp;#x27;, name = &amp;#x27;count&amp;#x27;\n\n    def get_return_value(answer, gateway_client, target_id=None, name=None):\n        &amp;quot;&amp;quot;&amp;quot;Converts an answer received from the Java gateway into a Python object.\n    \n        For example, string representation of integers are converted to Python\n        integer, string representation of objects are converted to JavaObject\n        instances, etc.\n    \n        :param answer: the string returned by the Java gateway\n        :param gateway_client: the gateway client used to communicate with the Java\n            Gateway. Only necessary if the answer is a reference (e.g., object,\n            list, map)\n        :param target_id: the name of the object from which the answer comes from\n            (e.g., *object1* in `object1.hello()`). Optional.\n        :param name: the name of the member from which the answer comes from\n            (e.g., *hello* in `object1.hello()`). Optional.\n        &amp;quot;&amp;quot;&amp;quot;\n        if is_error(answer)[0]:\n            if len(answer) &amp;gt; 1:\n                type = answer[1]\n                value = OUTPUT_CONVERTER[type](answer[2:], gateway_client)\n                if answer[1] == REFERENCE_TYPE:\n&amp;gt;                   raise Py4JJavaError(\n                        &amp;quot;An error occurred while calling {0}{1}{2}.\\n&amp;quot;.\n                        format(target_id, &amp;quot;.&amp;quot;, name), value)\nE                   py4j.protocol.Py4JJavaError: An error occurred while calling o6862.count.\nE                   : org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 30.0 failed 1 times, most recent failure: Lost task 0.0 in stage 30.0 (TID 28) (10.20.80.191 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\nE                       check_python_version(infile)\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\nE                       raise PySparkRuntimeError(\nE                   pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nE                   Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\nE                   \nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\nE                   \tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\nE                   \tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\nE                   \tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\nE                   \tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\nE                   \tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\nE                   \tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\nE                   \tat org.apache.spark.scheduler.Task.run(Task.scala:147)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\nE                   \tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\nE                   \tat java.base/java.lang.Thread.run(Thread.java:840)\nE                   \nE                   Driver stacktrace:\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$3(DAGScheduler.scala:3122)\nE                   \tat scala.Option.getOrElse(Option.scala:201)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:3122)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:3114)\nE                   \tat scala.collection.immutable.List.foreach(List.scala:323)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:3114)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1303)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1303)\nE                   \tat scala.Option.foreach(Option.scala:437)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1303)\nE                   \tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3397)\nE                   \tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3328)\nE                   \tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3317)\nE                   \tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:50)\nE                   Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\nE                       check_python_version(infile)\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\nE                       raise PySparkRuntimeError(\nE                   pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nE                   Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\nE                   \nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\nE                   \tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\nE                   \tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\nE                   \tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\nE                   \tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\nE                   \tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\nE                   \tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\nE                   \tat org.apache.spark.scheduler.Task.run(Task.scala:147)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\nE                   \tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\nE                   \tat java.base/java.lang.Thread.run(Thread.java:840)\n\nvenv/lib/python3.14/site-packages/py4j/protocol.py:327: Py4JJavaError\n\n----------------------------- Captured stderr call -----------------------------\n26/01/22 12:17:27 ERROR Executor: Exception in task 0.0 in stage 30.0 (TID 28)\norg.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\n    check_python_version(infile)\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\n    raise PySparkRuntimeError(\npyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nPlease check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n26/01/22 12:17:27 WARN TaskSetManager: Lost task 0.0 in stage 30.0 (TID 28) (10.20.80.191 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\n    check_python_version(infile)\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\n    raise PySparkRuntimeError(\npyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nPlease check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n\n26/01/22 12:17:27 ERROR TaskSetManager: Task 0 in stage 30.0 failed 1 times; aborting job\n&#34;}], &#34;tests/test_cards_transform.py::test_transform_cards_negative_values&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/test_cards_transform.py::test_transform_cards_negative_values&#34;, &#34;duration&#34;: &#34;211 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/test_cards_transform.py::test_transform_cards_negative_values&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;211 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;spark = &amp;lt;pyspark.sql.session.SparkSession object at 0x10ca1de80&amp;gt;\n\n    def test_transform_cards_negative_values(spark):\n        raw_df = create_raw_cards_df(spark, [{\n            &amp;quot;id&amp;quot;: &amp;quot;c2&amp;quot;,\n            &amp;quot;status&amp;quot;: &amp;quot;active&amp;quot;,\n            &amp;quot;type&amp;quot;: &amp;quot;physical&amp;quot;,\n            &amp;quot;recurringLimit&amp;quot;: -10.0,\n            &amp;quot;currentPeriod&amp;quot;: {&amp;quot;limit&amp;quot;: -100.0, &amp;quot;spent&amp;quot;: -20.0},\n            &amp;quot;createdTime&amp;quot;: &amp;quot;2024-01-01T10:00:00&amp;quot;\n        }])\n    \n        valid_df, error_df = transform_cards(raw_df, &amp;quot;day1&amp;quot;)\n    \n&amp;gt;       assert valid_df.count() == 0\n               ^^^^^^^^^^^^^^^^\n\ntests/test_cards_transform.py:131: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \nvenv/lib/python3.14/site-packages/pyspark/sql/classic/dataframe.py:439: in count\n    return int(self._jdf.count())\n               ^^^^^^^^^^^^^^^^^\nvenv/lib/python3.14/site-packages/py4j/java_gateway.py:1362: in __call__\n    return_value = get_return_value(\nvenv/lib/python3.14/site-packages/pyspark/errors/exceptions/captured.py:263: in deco\n    return f(*a, **kw)\n           ^^^^^^^^^^^\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nanswer = &amp;#x27;xro7148&amp;#x27;\ngateway_client = &amp;lt;py4j.clientserver.JavaClient object at 0x10ca1c6e0&amp;gt;\ntarget_id = &amp;#x27;o7147&amp;#x27;, name = &amp;#x27;count&amp;#x27;\n\n    def get_return_value(answer, gateway_client, target_id=None, name=None):\n        &amp;quot;&amp;quot;&amp;quot;Converts an answer received from the Java gateway into a Python object.\n    \n        For example, string representation of integers are converted to Python\n        integer, string representation of objects are converted to JavaObject\n        instances, etc.\n    \n        :param answer: the string returned by the Java gateway\n        :param gateway_client: the gateway client used to communicate with the Java\n            Gateway. Only necessary if the answer is a reference (e.g., object,\n            list, map)\n        :param target_id: the name of the object from which the answer comes from\n            (e.g., *object1* in `object1.hello()`). Optional.\n        :param name: the name of the member from which the answer comes from\n            (e.g., *hello* in `object1.hello()`). Optional.\n        &amp;quot;&amp;quot;&amp;quot;\n        if is_error(answer)[0]:\n            if len(answer) &amp;gt; 1:\n                type = answer[1]\n                value = OUTPUT_CONVERTER[type](answer[2:], gateway_client)\n                if answer[1] == REFERENCE_TYPE:\n&amp;gt;                   raise Py4JJavaError(\n                        &amp;quot;An error occurred while calling {0}{1}{2}.\\n&amp;quot;.\n                        format(target_id, &amp;quot;.&amp;quot;, name), value)\nE                   py4j.protocol.Py4JJavaError: An error occurred while calling o7147.count.\nE                   : org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 31.0 failed 1 times, most recent failure: Lost task 0.0 in stage 31.0 (TID 29) (10.20.80.191 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\nE                       check_python_version(infile)\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\nE                       raise PySparkRuntimeError(\nE                   pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nE                   Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\nE                   \nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\nE                   \tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\nE                   \tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\nE                   \tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\nE                   \tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\nE                   \tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\nE                   \tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\nE                   \tat org.apache.spark.scheduler.Task.run(Task.scala:147)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\nE                   \tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\nE                   \tat java.base/java.lang.Thread.run(Thread.java:840)\nE                   \nE                   Driver stacktrace:\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$3(DAGScheduler.scala:3122)\nE                   \tat scala.Option.getOrElse(Option.scala:201)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:3122)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:3114)\nE                   \tat scala.collection.immutable.List.foreach(List.scala:323)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:3114)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1303)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1303)\nE                   \tat scala.Option.foreach(Option.scala:437)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1303)\nE                   \tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3397)\nE                   \tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3328)\nE                   \tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3317)\nE                   \tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:50)\nE                   Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\nE                       check_python_version(infile)\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\nE                       raise PySparkRuntimeError(\nE                   pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nE                   Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\nE                   \nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\nE                   \tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\nE                   \tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\nE                   \tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\nE                   \tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\nE                   \tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\nE                   \tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\nE                   \tat org.apache.spark.scheduler.Task.run(Task.scala:147)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\nE                   \tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\nE                   \tat java.base/java.lang.Thread.run(Thread.java:840)\n\nvenv/lib/python3.14/site-packages/py4j/protocol.py:327: Py4JJavaError\n\n----------------------------- Captured stderr call -----------------------------\n26/01/22 12:17:28 ERROR Executor: Exception in task 0.0 in stage 31.0 (TID 29)\norg.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\n    check_python_version(infile)\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\n    raise PySparkRuntimeError(\npyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nPlease check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n26/01/22 12:17:28 WARN TaskSetManager: Lost task 0.0 in stage 31.0 (TID 29) (10.20.80.191 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\n    check_python_version(infile)\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\n    raise PySparkRuntimeError(\npyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nPlease check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n\n26/01/22 12:17:28 ERROR TaskSetManager: Task 0 in stage 31.0 failed 1 times; aborting job\n&#34;}], &#34;tests/test_cards_transform.py::test_transform_cards_null_optional_numbers&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/test_cards_transform.py::test_transform_cards_null_optional_numbers&#34;, &#34;duration&#34;: &#34;177 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/test_cards_transform.py::test_transform_cards_null_optional_numbers&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;177 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;spark = &amp;lt;pyspark.sql.session.SparkSession object at 0x10ca1de80&amp;gt;\n\n    def test_transform_cards_null_optional_numbers(spark):\n        raw_df = create_raw_cards_df(spark, [{\n            &amp;quot;id&amp;quot;: &amp;quot;c3&amp;quot;,\n            &amp;quot;status&amp;quot;: &amp;quot;inactive&amp;quot;,\n            &amp;quot;type&amp;quot;: &amp;quot;virtual&amp;quot;,\n            &amp;quot;recurringLimit&amp;quot;: None,\n            &amp;quot;currentPeriod&amp;quot;: {&amp;quot;limit&amp;quot;: None, &amp;quot;spent&amp;quot;: None},\n            &amp;quot;createdTime&amp;quot;: &amp;quot;2024-01-01T10:00:00&amp;quot;\n        }])\n    \n        valid_df, error_df = transform_cards(raw_df, &amp;quot;day1&amp;quot;)\n    \n&amp;gt;       assert valid_df.count() == 1\n               ^^^^^^^^^^^^^^^^\n\ntests/test_cards_transform.py:150: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \nvenv/lib/python3.14/site-packages/pyspark/sql/classic/dataframe.py:439: in count\n    return int(self._jdf.count())\n               ^^^^^^^^^^^^^^^^^\nvenv/lib/python3.14/site-packages/py4j/java_gateway.py:1362: in __call__\n    return_value = get_return_value(\nvenv/lib/python3.14/site-packages/pyspark/errors/exceptions/captured.py:263: in deco\n    return f(*a, **kw)\n           ^^^^^^^^^^^\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nanswer = &amp;#x27;xro7433&amp;#x27;\ngateway_client = &amp;lt;py4j.clientserver.JavaClient object at 0x10ca1c6e0&amp;gt;\ntarget_id = &amp;#x27;o7432&amp;#x27;, name = &amp;#x27;count&amp;#x27;\n\n    def get_return_value(answer, gateway_client, target_id=None, name=None):\n        &amp;quot;&amp;quot;&amp;quot;Converts an answer received from the Java gateway into a Python object.\n    \n        For example, string representation of integers are converted to Python\n        integer, string representation of objects are converted to JavaObject\n        instances, etc.\n    \n        :param answer: the string returned by the Java gateway\n        :param gateway_client: the gateway client used to communicate with the Java\n            Gateway. Only necessary if the answer is a reference (e.g., object,\n            list, map)\n        :param target_id: the name of the object from which the answer comes from\n            (e.g., *object1* in `object1.hello()`). Optional.\n        :param name: the name of the member from which the answer comes from\n            (e.g., *hello* in `object1.hello()`). Optional.\n        &amp;quot;&amp;quot;&amp;quot;\n        if is_error(answer)[0]:\n            if len(answer) &amp;gt; 1:\n                type = answer[1]\n                value = OUTPUT_CONVERTER[type](answer[2:], gateway_client)\n                if answer[1] == REFERENCE_TYPE:\n&amp;gt;                   raise Py4JJavaError(\n                        &amp;quot;An error occurred while calling {0}{1}{2}.\\n&amp;quot;.\n                        format(target_id, &amp;quot;.&amp;quot;, name), value)\nE                   py4j.protocol.Py4JJavaError: An error occurred while calling o7432.count.\nE                   : org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 32.0 failed 1 times, most recent failure: Lost task 0.0 in stage 32.0 (TID 30) (10.20.80.191 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\nE                       check_python_version(infile)\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\nE                       raise PySparkRuntimeError(\nE                   pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nE                   Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\nE                   \nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\nE                   \tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\nE                   \tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\nE                   \tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\nE                   \tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\nE                   \tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\nE                   \tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\nE                   \tat org.apache.spark.scheduler.Task.run(Task.scala:147)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\nE                   \tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\nE                   \tat java.base/java.lang.Thread.run(Thread.java:840)\nE                   \nE                   Driver stacktrace:\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$3(DAGScheduler.scala:3122)\nE                   \tat scala.Option.getOrElse(Option.scala:201)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:3122)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:3114)\nE                   \tat scala.collection.immutable.List.foreach(List.scala:323)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:3114)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1303)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1303)\nE                   \tat scala.Option.foreach(Option.scala:437)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1303)\nE                   \tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3397)\nE                   \tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3328)\nE                   \tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3317)\nE                   \tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:50)\nE                   Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\nE                       check_python_version(infile)\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\nE                       raise PySparkRuntimeError(\nE                   pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nE                   Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\nE                   \nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\nE                   \tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\nE                   \tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\nE                   \tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\nE                   \tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\nE                   \tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\nE                   \tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\nE                   \tat org.apache.spark.scheduler.Task.run(Task.scala:147)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\nE                   \tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\nE                   \tat java.base/java.lang.Thread.run(Thread.java:840)\n\nvenv/lib/python3.14/site-packages/py4j/protocol.py:327: Py4JJavaError\n\n----------------------------- Captured stderr call -----------------------------\n26/01/22 12:17:28 ERROR Executor: Exception in task 0.0 in stage 32.0 (TID 30)\norg.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\n    check_python_version(infile)\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\n    raise PySparkRuntimeError(\npyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nPlease check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n26/01/22 12:17:28 WARN TaskSetManager: Lost task 0.0 in stage 32.0 (TID 30) (10.20.80.191 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\n    check_python_version(infile)\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\n    raise PySparkRuntimeError(\npyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nPlease check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n\n26/01/22 12:17:28 ERROR TaskSetManager: Task 0 in stage 32.0 failed 1 times; aborting job\n&#34;}], &#34;tests/test_cards_transform.py::test_transform_cards_deduplication&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/test_cards_transform.py::test_transform_cards_deduplication&#34;, &#34;duration&#34;: &#34;215 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/test_cards_transform.py::test_transform_cards_deduplication&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;215 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;spark = &amp;lt;pyspark.sql.session.SparkSession object at 0x10ca1de80&amp;gt;\n\n    def test_transform_cards_deduplication(spark):\n        raw_df = create_raw_cards_df(spark, [\n            {\n                &amp;quot;id&amp;quot;: &amp;quot;c4&amp;quot;,\n                &amp;quot;status&amp;quot;: &amp;quot;active&amp;quot;,\n                &amp;quot;type&amp;quot;: &amp;quot;virtual&amp;quot;,\n                &amp;quot;currentPeriod&amp;quot;: {&amp;quot;limit&amp;quot;: 100.0, &amp;quot;spent&amp;quot;: 10.0},\n                &amp;quot;createdTime&amp;quot;: &amp;quot;2024-01-01T10:00:00&amp;quot;\n            },\n            {\n                &amp;quot;id&amp;quot;: &amp;quot;c4&amp;quot;,\n                &amp;quot;status&amp;quot;: &amp;quot;active&amp;quot;,\n                &amp;quot;type&amp;quot;: &amp;quot;virtual&amp;quot;,\n                &amp;quot;currentPeriod&amp;quot;: {&amp;quot;limit&amp;quot;: 100.0, &amp;quot;spent&amp;quot;: 10.0},\n                &amp;quot;createdTime&amp;quot;: &amp;quot;2024-01-01T10:00:00&amp;quot;\n            }\n        ])\n    \n        valid_df, error_df = transform_cards(raw_df, &amp;quot;day1&amp;quot;)\n    \n&amp;gt;       assert valid_df.count() == 1\n               ^^^^^^^^^^^^^^^^\n\ntests/test_cards_transform.py:177: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \nvenv/lib/python3.14/site-packages/pyspark/sql/classic/dataframe.py:439: in count\n    return int(self._jdf.count())\n               ^^^^^^^^^^^^^^^^^\nvenv/lib/python3.14/site-packages/py4j/java_gateway.py:1362: in __call__\n    return_value = get_return_value(\nvenv/lib/python3.14/site-packages/pyspark/errors/exceptions/captured.py:263: in deco\n    return f(*a, **kw)\n           ^^^^^^^^^^^\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nanswer = &amp;#x27;xro7718&amp;#x27;\ngateway_client = &amp;lt;py4j.clientserver.JavaClient object at 0x10ca1c6e0&amp;gt;\ntarget_id = &amp;#x27;o7717&amp;#x27;, name = &amp;#x27;count&amp;#x27;\n\n    def get_return_value(answer, gateway_client, target_id=None, name=None):\n        &amp;quot;&amp;quot;&amp;quot;Converts an answer received from the Java gateway into a Python object.\n    \n        For example, string representation of integers are converted to Python\n        integer, string representation of objects are converted to JavaObject\n        instances, etc.\n    \n        :param answer: the string returned by the Java gateway\n        :param gateway_client: the gateway client used to communicate with the Java\n            Gateway. Only necessary if the answer is a reference (e.g., object,\n            list, map)\n        :param target_id: the name of the object from which the answer comes from\n            (e.g., *object1* in `object1.hello()`). Optional.\n        :param name: the name of the member from which the answer comes from\n            (e.g., *hello* in `object1.hello()`). Optional.\n        &amp;quot;&amp;quot;&amp;quot;\n        if is_error(answer)[0]:\n            if len(answer) &amp;gt; 1:\n                type = answer[1]\n                value = OUTPUT_CONVERTER[type](answer[2:], gateway_client)\n                if answer[1] == REFERENCE_TYPE:\n&amp;gt;                   raise Py4JJavaError(\n                        &amp;quot;An error occurred while calling {0}{1}{2}.\\n&amp;quot;.\n                        format(target_id, &amp;quot;.&amp;quot;, name), value)\nE                   py4j.protocol.Py4JJavaError: An error occurred while calling o7717.count.\nE                   : org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 33.0 failed 1 times, most recent failure: Lost task 0.0 in stage 33.0 (TID 31) (10.20.80.191 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\nE                       check_python_version(infile)\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\nE                       raise PySparkRuntimeError(\nE                   pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nE                   Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\nE                   \nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\nE                   \tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\nE                   \tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\nE                   \tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\nE                   \tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\nE                   \tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\nE                   \tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\nE                   \tat org.apache.spark.scheduler.Task.run(Task.scala:147)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\nE                   \tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\nE                   \tat java.base/java.lang.Thread.run(Thread.java:840)\nE                   \nE                   Driver stacktrace:\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$3(DAGScheduler.scala:3122)\nE                   \tat scala.Option.getOrElse(Option.scala:201)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:3122)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:3114)\nE                   \tat scala.collection.immutable.List.foreach(List.scala:323)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:3114)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1303)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1303)\nE                   \tat scala.Option.foreach(Option.scala:437)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1303)\nE                   \tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3397)\nE                   \tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3328)\nE                   \tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3317)\nE                   \tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:50)\nE                   Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\nE                       check_python_version(infile)\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\nE                       raise PySparkRuntimeError(\nE                   pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nE                   Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\nE                   \nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\nE                   \tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\nE                   \tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\nE                   \tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\nE                   \tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\nE                   \tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\nE                   \tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\nE                   \tat org.apache.spark.scheduler.Task.run(Task.scala:147)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\nE                   \tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\nE                   \tat java.base/java.lang.Thread.run(Thread.java:840)\n\nvenv/lib/python3.14/site-packages/py4j/protocol.py:327: Py4JJavaError\n\n----------------------------- Captured stderr call -----------------------------\n26/01/22 12:17:28 ERROR Executor: Exception in task 0.0 in stage 33.0 (TID 31)\norg.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\n    check_python_version(infile)\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\n    raise PySparkRuntimeError(\npyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nPlease check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n26/01/22 12:17:28 WARN TaskSetManager: Lost task 0.0 in stage 33.0 (TID 31) (10.20.80.191 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\n    check_python_version(infile)\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\n    raise PySparkRuntimeError(\npyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nPlease check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n\n26/01/22 12:17:28 ERROR TaskSetManager: Task 0 in stage 33.0 failed 1 times; aborting job\n&#34;}], &#34;tests/test_cards_transform.py::test_transform_cards_flat_input_valid&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/test_cards_transform.py::test_transform_cards_flat_input_valid&#34;, &#34;duration&#34;: &#34;269 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/test_cards_transform.py::test_transform_cards_flat_input_valid&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;269 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;spark = &amp;lt;pyspark.sql.session.SparkSession object at 0x10ca1de80&amp;gt;\n\n    def test_transform_cards_flat_input_valid(spark):\n        raw_df = create_flat_cards_df(spark, [{\n            &amp;quot;id&amp;quot;: &amp;quot;cf1&amp;quot;,\n            &amp;quot;status&amp;quot;: &amp;quot;active&amp;quot;,\n            &amp;quot;type&amp;quot;: &amp;quot;virtual&amp;quot;,\n            &amp;quot;currentPeriod&amp;quot;: {&amp;quot;limit&amp;quot;: 300.0, &amp;quot;spent&amp;quot;: 100.0},\n            &amp;quot;createdTime&amp;quot;: &amp;quot;2024-01-01T10:00:00&amp;quot;\n        }])\n    \n        valid_df, error_df = transform_cards(raw_df, &amp;quot;day_flat&amp;quot;)\n    \n&amp;gt;       assert valid_df.count() == 1\n               ^^^^^^^^^^^^^^^^\n\ntests/test_cards_transform.py:195: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \nvenv/lib/python3.14/site-packages/pyspark/sql/classic/dataframe.py:439: in count\n    return int(self._jdf.count())\n               ^^^^^^^^^^^^^^^^^\nvenv/lib/python3.14/site-packages/py4j/java_gateway.py:1362: in __call__\n    return_value = get_return_value(\nvenv/lib/python3.14/site-packages/pyspark/errors/exceptions/captured.py:263: in deco\n    return f(*a, **kw)\n           ^^^^^^^^^^^\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nanswer = &amp;#x27;xro8014&amp;#x27;\ngateway_client = &amp;lt;py4j.clientserver.JavaClient object at 0x10ca1c6e0&amp;gt;\ntarget_id = &amp;#x27;o8013&amp;#x27;, name = &amp;#x27;count&amp;#x27;\n\n    def get_return_value(answer, gateway_client, target_id=None, name=None):\n        &amp;quot;&amp;quot;&amp;quot;Converts an answer received from the Java gateway into a Python object.\n    \n        For example, string representation of integers are converted to Python\n        integer, string representation of objects are converted to JavaObject\n        instances, etc.\n    \n        :param answer: the string returned by the Java gateway\n        :param gateway_client: the gateway client used to communicate with the Java\n            Gateway. Only necessary if the answer is a reference (e.g., object,\n            list, map)\n        :param target_id: the name of the object from which the answer comes from\n            (e.g., *object1* in `object1.hello()`). Optional.\n        :param name: the name of the member from which the answer comes from\n            (e.g., *hello* in `object1.hello()`). Optional.\n        &amp;quot;&amp;quot;&amp;quot;\n        if is_error(answer)[0]:\n            if len(answer) &amp;gt; 1:\n                type = answer[1]\n                value = OUTPUT_CONVERTER[type](answer[2:], gateway_client)\n                if answer[1] == REFERENCE_TYPE:\n&amp;gt;                   raise Py4JJavaError(\n                        &amp;quot;An error occurred while calling {0}{1}{2}.\\n&amp;quot;.\n                        format(target_id, &amp;quot;.&amp;quot;, name), value)\nE                   py4j.protocol.Py4JJavaError: An error occurred while calling o8013.count.\nE                   : org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 34.0 failed 1 times, most recent failure: Lost task 0.0 in stage 34.0 (TID 32) (10.20.80.191 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\nE                       check_python_version(infile)\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\nE                       raise PySparkRuntimeError(\nE                   pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nE                   Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\nE                   \nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\nE                   \tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\nE                   \tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\nE                   \tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\nE                   \tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\nE                   \tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\nE                   \tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\nE                   \tat org.apache.spark.scheduler.Task.run(Task.scala:147)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\nE                   \tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\nE                   \tat java.base/java.lang.Thread.run(Thread.java:840)\nE                   \nE                   Driver stacktrace:\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$3(DAGScheduler.scala:3122)\nE                   \tat scala.Option.getOrElse(Option.scala:201)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:3122)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:3114)\nE                   \tat scala.collection.immutable.List.foreach(List.scala:323)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:3114)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1303)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1303)\nE                   \tat scala.Option.foreach(Option.scala:437)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1303)\nE                   \tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3397)\nE                   \tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3328)\nE                   \tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3317)\nE                   \tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:50)\nE                   Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\nE                       check_python_version(infile)\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\nE                       raise PySparkRuntimeError(\nE                   pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nE                   Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\nE                   \nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\nE                   \tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\nE                   \tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\nE                   \tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\nE                   \tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\nE                   \tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\nE                   \tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\nE                   \tat org.apache.spark.scheduler.Task.run(Task.scala:147)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\nE                   \tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\nE                   \tat java.base/java.lang.Thread.run(Thread.java:840)\n\nvenv/lib/python3.14/site-packages/py4j/protocol.py:327: Py4JJavaError\n\n----------------------------- Captured stderr call -----------------------------\n26/01/22 12:17:29 ERROR Executor: Exception in task 0.0 in stage 34.0 (TID 32)\norg.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\n    check_python_version(infile)\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\n    raise PySparkRuntimeError(\npyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nPlease check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n26/01/22 12:17:29 WARN TaskSetManager: Lost task 0.0 in stage 34.0 (TID 32) (10.20.80.191 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\n    check_python_version(infile)\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\n    raise PySparkRuntimeError(\npyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nPlease check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n\n26/01/22 12:17:29 ERROR TaskSetManager: Task 0 in stage 34.0 failed 1 times; aborting job\n&#34;}], &#34;tests/test_cards_transform.py::test_transform_cards_flat_input_deduplication&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/test_cards_transform.py::test_transform_cards_flat_input_deduplication&#34;, &#34;duration&#34;: &#34;159 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/test_cards_transform.py::test_transform_cards_flat_input_deduplication&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;159 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;spark = &amp;lt;pyspark.sql.session.SparkSession object at 0x10ca1de80&amp;gt;\n\n    def test_transform_cards_flat_input_deduplication(spark):\n        raw_df = create_flat_cards_df(spark, [\n            {\n                &amp;quot;id&amp;quot;: &amp;quot;cf2&amp;quot;,\n                &amp;quot;status&amp;quot;: &amp;quot;active&amp;quot;,\n                &amp;quot;type&amp;quot;: &amp;quot;virtual&amp;quot;,\n                &amp;quot;currentPeriod&amp;quot;: {&amp;quot;limit&amp;quot;: 100.0, &amp;quot;spent&amp;quot;: 10.0},\n                &amp;quot;createdTime&amp;quot;: &amp;quot;2024-01-01T10:00:00&amp;quot;\n            },\n            {\n                &amp;quot;id&amp;quot;: &amp;quot;cf2&amp;quot;,\n                &amp;quot;status&amp;quot;: &amp;quot;active&amp;quot;,\n                &amp;quot;type&amp;quot;: &amp;quot;virtual&amp;quot;,\n                &amp;quot;currentPeriod&amp;quot;: {&amp;quot;limit&amp;quot;: 100.0, &amp;quot;spent&amp;quot;: 10.0},\n                &amp;quot;createdTime&amp;quot;: &amp;quot;2024-01-01T10:00:00&amp;quot;\n            }\n        ])\n    \n        valid_df, error_df = transform_cards(raw_df, &amp;quot;day_flat&amp;quot;)\n    \n&amp;gt;       assert valid_df.count() == 1\n               ^^^^^^^^^^^^^^^^\n\ntests/test_cards_transform.py:225: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \nvenv/lib/python3.14/site-packages/pyspark/sql/classic/dataframe.py:439: in count\n    return int(self._jdf.count())\n               ^^^^^^^^^^^^^^^^^\nvenv/lib/python3.14/site-packages/py4j/java_gateway.py:1362: in __call__\n    return_value = get_return_value(\nvenv/lib/python3.14/site-packages/pyspark/errors/exceptions/captured.py:263: in deco\n    return f(*a, **kw)\n           ^^^^^^^^^^^\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nanswer = &amp;#x27;xro8310&amp;#x27;\ngateway_client = &amp;lt;py4j.clientserver.JavaClient object at 0x10ca1c6e0&amp;gt;\ntarget_id = &amp;#x27;o8309&amp;#x27;, name = &amp;#x27;count&amp;#x27;\n\n    def get_return_value(answer, gateway_client, target_id=None, name=None):\n        &amp;quot;&amp;quot;&amp;quot;Converts an answer received from the Java gateway into a Python object.\n    \n        For example, string representation of integers are converted to Python\n        integer, string representation of objects are converted to JavaObject\n        instances, etc.\n    \n        :param answer: the string returned by the Java gateway\n        :param gateway_client: the gateway client used to communicate with the Java\n            Gateway. Only necessary if the answer is a reference (e.g., object,\n            list, map)\n        :param target_id: the name of the object from which the answer comes from\n            (e.g., *object1* in `object1.hello()`). Optional.\n        :param name: the name of the member from which the answer comes from\n            (e.g., *hello* in `object1.hello()`). Optional.\n        &amp;quot;&amp;quot;&amp;quot;\n        if is_error(answer)[0]:\n            if len(answer) &amp;gt; 1:\n                type = answer[1]\n                value = OUTPUT_CONVERTER[type](answer[2:], gateway_client)\n                if answer[1] == REFERENCE_TYPE:\n&amp;gt;                   raise Py4JJavaError(\n                        &amp;quot;An error occurred while calling {0}{1}{2}.\\n&amp;quot;.\n                        format(target_id, &amp;quot;.&amp;quot;, name), value)\nE                   py4j.protocol.Py4JJavaError: An error occurred while calling o8309.count.\nE                   : org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 35.0 failed 1 times, most recent failure: Lost task 0.0 in stage 35.0 (TID 33) (10.20.80.191 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\nE                       check_python_version(infile)\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\nE                       raise PySparkRuntimeError(\nE                   pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nE                   Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\nE                   \nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\nE                   \tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\nE                   \tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\nE                   \tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\nE                   \tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\nE                   \tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\nE                   \tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\nE                   \tat org.apache.spark.scheduler.Task.run(Task.scala:147)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\nE                   \tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\nE                   \tat java.base/java.lang.Thread.run(Thread.java:840)\nE                   \nE                   Driver stacktrace:\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$3(DAGScheduler.scala:3122)\nE                   \tat scala.Option.getOrElse(Option.scala:201)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:3122)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:3114)\nE                   \tat scala.collection.immutable.List.foreach(List.scala:323)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:3114)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1303)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1303)\nE                   \tat scala.Option.foreach(Option.scala:437)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1303)\nE                   \tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3397)\nE                   \tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3328)\nE                   \tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3317)\nE                   \tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:50)\nE                   Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\nE                       check_python_version(infile)\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\nE                       raise PySparkRuntimeError(\nE                   pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nE                   Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\nE                   \nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\nE                   \tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\nE                   \tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\nE                   \tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\nE                   \tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\nE                   \tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\nE                   \tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\nE                   \tat org.apache.spark.scheduler.Task.run(Task.scala:147)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\nE                   \tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\nE                   \tat java.base/java.lang.Thread.run(Thread.java:840)\n\nvenv/lib/python3.14/site-packages/py4j/protocol.py:327: Py4JJavaError\n\n----------------------------- Captured stderr call -----------------------------\n26/01/22 12:17:29 ERROR Executor: Exception in task 0.0 in stage 35.0 (TID 33)\norg.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\n    check_python_version(infile)\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\n    raise PySparkRuntimeError(\npyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nPlease check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n26/01/22 12:17:29 WARN TaskSetManager: Lost task 0.0 in stage 35.0 (TID 33) (10.20.80.191 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\n    check_python_version(infile)\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\n    raise PySparkRuntimeError(\npyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nPlease check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n\n26/01/22 12:17:29 ERROR TaskSetManager: Task 0 in stage 35.0 failed 1 times; aborting job\n&#34;}], &#34;tests/test_groups_transform.py::test_transform_groups_all_valid_agents&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/test_groups_transform.py::test_transform_groups_all_valid_agents&#34;, &#34;duration&#34;: &#34;120 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/test_groups_transform.py::test_transform_groups_all_valid_agents&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;120 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;spark = &amp;lt;pyspark.sql.session.SparkSession object at 0x10ca1de80&amp;gt;\ngroups_schema = StructType([StructField(&amp;#x27;id&amp;#x27;, LongType(), True), StructField(&amp;#x27;name&amp;#x27;, StringType(), True), StructField(&amp;#x27;parent_group_id...(), True), StructField(&amp;#x27;updated_at&amp;#x27;, StringType(), True), StructField(&amp;#x27;agent_ids&amp;#x27;, ArrayType(LongType(), True), True)])\nagents_schema = StructType([StructField(&amp;#x27;agent_id&amp;#x27;, LongType(), True), StructField(&amp;#x27;name&amp;#x27;, StringType(), True)])\n\n    def test_transform_groups_all_valid_agents(spark, groups_schema, agents_schema):\n        raw_data = [\n            (1, &amp;quot;Support&amp;quot;, None, &amp;quot;2024-01-01&amp;quot;, &amp;quot;2024-01-02&amp;quot;, [10, 20])\n        ]\n    \n        agents_data = [\n            (10, &amp;quot;Alice&amp;quot;),\n            (20, &amp;quot;Bob&amp;quot;),\n        ]\n    \n        raw_df = spark.createDataFrame(raw_data, groups_schema)\n        agents_df = spark.createDataFrame(agents_data, agents_schema)\n    \n        groups_df, valid_df, error_df = transform_groups(raw_df, agents_df)\n    \n&amp;gt;       assert groups_df.count() == 1\n               ^^^^^^^^^^^^^^^^^\n\ntests/test_groups_transform.py:57: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \nvenv/lib/python3.14/site-packages/pyspark/sql/classic/dataframe.py:439: in count\n    return int(self._jdf.count())\n               ^^^^^^^^^^^^^^^^^\nvenv/lib/python3.14/site-packages/py4j/java_gateway.py:1362: in __call__\n    return_value = get_return_value(\nvenv/lib/python3.14/site-packages/pyspark/errors/exceptions/captured.py:263: in deco\n    return f(*a, **kw)\n           ^^^^^^^^^^^\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nanswer = &amp;#x27;xro8475&amp;#x27;\ngateway_client = &amp;lt;py4j.clientserver.JavaClient object at 0x10ca1c6e0&amp;gt;\ntarget_id = &amp;#x27;o8401&amp;#x27;, name = &amp;#x27;count&amp;#x27;\n\n    def get_return_value(answer, gateway_client, target_id=None, name=None):\n        &amp;quot;&amp;quot;&amp;quot;Converts an answer received from the Java gateway into a Python object.\n    \n        For example, string representation of integers are converted to Python\n        integer, string representation of objects are converted to JavaObject\n        instances, etc.\n    \n        :param answer: the string returned by the Java gateway\n        :param gateway_client: the gateway client used to communicate with the Java\n            Gateway. Only necessary if the answer is a reference (e.g., object,\n            list, map)\n        :param target_id: the name of the object from which the answer comes from\n            (e.g., *object1* in `object1.hello()`). Optional.\n        :param name: the name of the member from which the answer comes from\n            (e.g., *hello* in `object1.hello()`). Optional.\n        &amp;quot;&amp;quot;&amp;quot;\n        if is_error(answer)[0]:\n            if len(answer) &amp;gt; 1:\n                type = answer[1]\n                value = OUTPUT_CONVERTER[type](answer[2:], gateway_client)\n                if answer[1] == REFERENCE_TYPE:\n&amp;gt;                   raise Py4JJavaError(\n                        &amp;quot;An error occurred while calling {0}{1}{2}.\\n&amp;quot;.\n                        format(target_id, &amp;quot;.&amp;quot;, name), value)\nE                   py4j.protocol.Py4JJavaError: An error occurred while calling o8401.count.\nE                   : org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 36.0 failed 1 times, most recent failure: Lost task 0.0 in stage 36.0 (TID 34) (10.20.80.191 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\nE                       check_python_version(infile)\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\nE                       raise PySparkRuntimeError(\nE                   pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nE                   Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\nE                   \nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\nE                   \tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\nE                   \tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\nE                   \tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\nE                   \tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\nE                   \tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\nE                   \tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\nE                   \tat org.apache.spark.scheduler.Task.run(Task.scala:147)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\nE                   \tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\nE                   \tat java.base/java.lang.Thread.run(Thread.java:840)\nE                   \nE                   Driver stacktrace:\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$3(DAGScheduler.scala:3122)\nE                   \tat scala.Option.getOrElse(Option.scala:201)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:3122)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:3114)\nE                   \tat scala.collection.immutable.List.foreach(List.scala:323)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:3114)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1303)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1303)\nE                   \tat scala.Option.foreach(Option.scala:437)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1303)\nE                   \tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3397)\nE                   \tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3328)\nE                   \tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3317)\nE                   \tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:50)\nE                   Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\nE                       check_python_version(infile)\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\nE                       raise PySparkRuntimeError(\nE                   pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nE                   Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\nE                   \nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\nE                   \tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\nE                   \tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\nE                   \tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\nE                   \tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\nE                   \tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\nE                   \tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\nE                   \tat org.apache.spark.scheduler.Task.run(Task.scala:147)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\nE                   \tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\nE                   \tat java.base/java.lang.Thread.run(Thread.java:840)\n\nvenv/lib/python3.14/site-packages/py4j/protocol.py:327: Py4JJavaError\n\n---------------------------- Captured stderr setup -----------------------------\n26/01/22 12:17:29 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n\n----------------------------- Captured stderr call -----------------------------\n26/01/22 12:17:29 ERROR Executor: Exception in task 0.0 in stage 36.0 (TID 34)\norg.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\n    check_python_version(infile)\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\n    raise PySparkRuntimeError(\npyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nPlease check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n26/01/22 12:17:29 WARN TaskSetManager: Lost task 0.0 in stage 36.0 (TID 34) (10.20.80.191 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\n    check_python_version(infile)\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\n    raise PySparkRuntimeError(\npyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nPlease check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n\n26/01/22 12:17:29 ERROR TaskSetManager: Task 0 in stage 36.0 failed 1 times; aborting job\n&#34;}], &#34;tests/test_groups_transform.py::test_transform_groups_mixed_valid_invalid_agents&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/test_groups_transform.py::test_transform_groups_mixed_valid_invalid_agents&#34;, &#34;duration&#34;: &#34;148 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/test_groups_transform.py::test_transform_groups_mixed_valid_invalid_agents&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;148 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;spark = &amp;lt;pyspark.sql.session.SparkSession object at 0x10ca1de80&amp;gt;\ngroups_schema = StructType([StructField(&amp;#x27;id&amp;#x27;, LongType(), True), StructField(&amp;#x27;name&amp;#x27;, StringType(), True), StructField(&amp;#x27;parent_group_id...(), True), StructField(&amp;#x27;updated_at&amp;#x27;, StringType(), True), StructField(&amp;#x27;agent_ids&amp;#x27;, ArrayType(LongType(), True), True)])\nagents_schema = StructType([StructField(&amp;#x27;agent_id&amp;#x27;, LongType(), True), StructField(&amp;#x27;name&amp;#x27;, StringType(), True)])\n\n    def test_transform_groups_mixed_valid_invalid_agents(spark, groups_schema, agents_schema):\n        raw_data = [\n            (1, &amp;quot;IT&amp;quot;, None, &amp;quot;2024-01-01&amp;quot;, &amp;quot;2024-01-02&amp;quot;, [10, 99])\n        ]\n    \n        agents_data = [\n            (10, &amp;quot;Alice&amp;quot;),\n        ]\n    \n        raw_df = spark.createDataFrame(raw_data, groups_schema)\n        agents_df = spark.createDataFrame(agents_data, agents_schema)\n    \n        groups_df, valid_df, error_df = transform_groups(raw_df, agents_df)\n    \n&amp;gt;       assert valid_df.count() == 1\n               ^^^^^^^^^^^^^^^^\n\ntests/test_groups_transform.py:83: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \nvenv/lib/python3.14/site-packages/pyspark/sql/classic/dataframe.py:439: in count\n    return int(self._jdf.count())\n               ^^^^^^^^^^^^^^^^^\nvenv/lib/python3.14/site-packages/py4j/java_gateway.py:1362: in __call__\n    return_value = get_return_value(\nvenv/lib/python3.14/site-packages/pyspark/errors/exceptions/captured.py:263: in deco\n    return f(*a, **kw)\n           ^^^^^^^^^^^\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nanswer = &amp;#x27;xro8638&amp;#x27;\ngateway_client = &amp;lt;py4j.clientserver.JavaClient object at 0x10ca1c6e0&amp;gt;\ntarget_id = &amp;#x27;o8598&amp;#x27;, name = &amp;#x27;count&amp;#x27;\n\n    def get_return_value(answer, gateway_client, target_id=None, name=None):\n        &amp;quot;&amp;quot;&amp;quot;Converts an answer received from the Java gateway into a Python object.\n    \n        For example, string representation of integers are converted to Python\n        integer, string representation of objects are converted to JavaObject\n        instances, etc.\n    \n        :param answer: the string returned by the Java gateway\n        :param gateway_client: the gateway client used to communicate with the Java\n            Gateway. Only necessary if the answer is a reference (e.g., object,\n            list, map)\n        :param target_id: the name of the object from which the answer comes from\n            (e.g., *object1* in `object1.hello()`). Optional.\n        :param name: the name of the member from which the answer comes from\n            (e.g., *hello* in `object1.hello()`). Optional.\n        &amp;quot;&amp;quot;&amp;quot;\n        if is_error(answer)[0]:\n            if len(answer) &amp;gt; 1:\n                type = answer[1]\n                value = OUTPUT_CONVERTER[type](answer[2:], gateway_client)\n                if answer[1] == REFERENCE_TYPE:\n&amp;gt;                   raise Py4JJavaError(\n                        &amp;quot;An error occurred while calling {0}{1}{2}.\\n&amp;quot;.\n                        format(target_id, &amp;quot;.&amp;quot;, name), value)\nE                   py4j.protocol.Py4JJavaError: An error occurred while calling o8598.count.\nE                   : org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 37.0 failed 1 times, most recent failure: Lost task 0.0 in stage 37.0 (TID 35) (10.20.80.191 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\nE                       check_python_version(infile)\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\nE                       raise PySparkRuntimeError(\nE                   pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nE                   Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\nE                   \nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\nE                   \tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\nE                   \tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\nE                   \tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\nE                   \tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\nE                   \tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\nE                   \tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\nE                   \tat org.apache.spark.scheduler.Task.run(Task.scala:147)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\nE                   \tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\nE                   \tat java.base/java.lang.Thread.run(Thread.java:840)\nE                   \nE                   Driver stacktrace:\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$3(DAGScheduler.scala:3122)\nE                   \tat scala.Option.getOrElse(Option.scala:201)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:3122)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:3114)\nE                   \tat scala.collection.immutable.List.foreach(List.scala:323)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:3114)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1303)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1303)\nE                   \tat scala.Option.foreach(Option.scala:437)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1303)\nE                   \tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3397)\nE                   \tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3328)\nE                   \tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3317)\nE                   \tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:50)\nE                   Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\nE                       check_python_version(infile)\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\nE                       raise PySparkRuntimeError(\nE                   pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nE                   Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\nE                   \nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\nE                   \tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\nE                   \tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\nE                   \tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\nE                   \tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\nE                   \tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\nE                   \tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\nE                   \tat org.apache.spark.scheduler.Task.run(Task.scala:147)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\nE                   \tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\nE                   \tat java.base/java.lang.Thread.run(Thread.java:840)\n\nvenv/lib/python3.14/site-packages/py4j/protocol.py:327: Py4JJavaError\n\n----------------------------- Captured stderr call -----------------------------\n26/01/22 12:17:29 ERROR Executor: Exception in task 0.0 in stage 37.0 (TID 35)\norg.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\n    check_python_version(infile)\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\n    raise PySparkRuntimeError(\npyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nPlease check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n26/01/22 12:17:29 WARN TaskSetManager: Lost task 0.0 in stage 37.0 (TID 35) (10.20.80.191 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\n    check_python_version(infile)\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\n    raise PySparkRuntimeError(\npyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nPlease check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n\n26/01/22 12:17:29 ERROR TaskSetManager: Task 0 in stage 37.0 failed 1 times; aborting job\n26/01/22 12:17:29 WARN TaskSetManager: Lost task 0.0 in stage 38.0 (TID 36) (10.20.80.191 executor driver): TaskKilled (Stage cancelled: [SPARK_JOB_CANCELLED] Job 36 cancelled The corresponding SQL query has failed. SQLSTATE: XXKDA)\n&#34;}], &#34;tests/test_groups_transform.py::test_transform_groups_all_invalid_agents&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/test_groups_transform.py::test_transform_groups_all_invalid_agents&#34;, &#34;duration&#34;: &#34;131 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/test_groups_transform.py::test_transform_groups_all_invalid_agents&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;131 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;spark = &amp;lt;pyspark.sql.session.SparkSession object at 0x10ca1de80&amp;gt;\ngroups_schema = StructType([StructField(&amp;#x27;id&amp;#x27;, LongType(), True), StructField(&amp;#x27;name&amp;#x27;, StringType(), True), StructField(&amp;#x27;parent_group_id...(), True), StructField(&amp;#x27;updated_at&amp;#x27;, StringType(), True), StructField(&amp;#x27;agent_ids&amp;#x27;, ArrayType(LongType(), True), True)])\nagents_schema = StructType([StructField(&amp;#x27;agent_id&amp;#x27;, LongType(), True), StructField(&amp;#x27;name&amp;#x27;, StringType(), True)])\n\n    def test_transform_groups_all_invalid_agents(spark, groups_schema, agents_schema):\n        raw_data = [\n            (2, &amp;quot;HR&amp;quot;, None, &amp;quot;2024-01-01&amp;quot;, &amp;quot;2024-01-02&amp;quot;, [50, 60])\n        ]\n    \n        agents_data = [\n            (10, &amp;quot;Alice&amp;quot;),\n        ]\n    \n        raw_df = spark.createDataFrame(raw_data, groups_schema)\n        agents_df = spark.createDataFrame(agents_data, agents_schema)\n    \n        groups_df, valid_df, error_df = transform_groups(raw_df, agents_df)\n    \n&amp;gt;       assert valid_df.count() == 0\n               ^^^^^^^^^^^^^^^^\n\ntests/test_groups_transform.py:108: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \nvenv/lib/python3.14/site-packages/pyspark/sql/classic/dataframe.py:439: in count\n    return int(self._jdf.count())\n               ^^^^^^^^^^^^^^^^^\nvenv/lib/python3.14/site-packages/py4j/java_gateway.py:1362: in __call__\n    return_value = get_return_value(\nvenv/lib/python3.14/site-packages/pyspark/errors/exceptions/captured.py:263: in deco\n    return f(*a, **kw)\n           ^^^^^^^^^^^\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nanswer = &amp;#x27;xro8800&amp;#x27;\ngateway_client = &amp;lt;py4j.clientserver.JavaClient object at 0x10ca1c6e0&amp;gt;\ntarget_id = &amp;#x27;o8760&amp;#x27;, name = &amp;#x27;count&amp;#x27;\n\n    def get_return_value(answer, gateway_client, target_id=None, name=None):\n        &amp;quot;&amp;quot;&amp;quot;Converts an answer received from the Java gateway into a Python object.\n    \n        For example, string representation of integers are converted to Python\n        integer, string representation of objects are converted to JavaObject\n        instances, etc.\n    \n        :param answer: the string returned by the Java gateway\n        :param gateway_client: the gateway client used to communicate with the Java\n            Gateway. Only necessary if the answer is a reference (e.g., object,\n            list, map)\n        :param target_id: the name of the object from which the answer comes from\n            (e.g., *object1* in `object1.hello()`). Optional.\n        :param name: the name of the member from which the answer comes from\n            (e.g., *hello* in `object1.hello()`). Optional.\n        &amp;quot;&amp;quot;&amp;quot;\n        if is_error(answer)[0]:\n            if len(answer) &amp;gt; 1:\n                type = answer[1]\n                value = OUTPUT_CONVERTER[type](answer[2:], gateway_client)\n                if answer[1] == REFERENCE_TYPE:\n&amp;gt;                   raise Py4JJavaError(\n                        &amp;quot;An error occurred while calling {0}{1}{2}.\\n&amp;quot;.\n                        format(target_id, &amp;quot;.&amp;quot;, name), value)\nE                   py4j.protocol.Py4JJavaError: An error occurred while calling o8760.count.\nE                   : org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 39.0 failed 1 times, most recent failure: Lost task 0.0 in stage 39.0 (TID 37) (10.20.80.191 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\nE                       check_python_version(infile)\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\nE                       raise PySparkRuntimeError(\nE                   pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nE                   Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\nE                   \nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\nE                   \tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\nE                   \tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\nE                   \tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\nE                   \tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\nE                   \tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\nE                   \tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\nE                   \tat org.apache.spark.scheduler.Task.run(Task.scala:147)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\nE                   \tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\nE                   \tat java.base/java.lang.Thread.run(Thread.java:840)\nE                   \nE                   Driver stacktrace:\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$3(DAGScheduler.scala:3122)\nE                   \tat scala.Option.getOrElse(Option.scala:201)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:3122)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:3114)\nE                   \tat scala.collection.immutable.List.foreach(List.scala:323)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:3114)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1303)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1303)\nE                   \tat scala.Option.foreach(Option.scala:437)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1303)\nE                   \tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3397)\nE                   \tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3328)\nE                   \tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3317)\nE                   \tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:50)\nE                   Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\nE                       check_python_version(infile)\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\nE                       raise PySparkRuntimeError(\nE                   pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nE                   Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\nE                   \nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\nE                   \tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\nE                   \tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\nE                   \tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\nE                   \tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\nE                   \tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\nE                   \tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\nE                   \tat org.apache.spark.scheduler.Task.run(Task.scala:147)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\nE                   \tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\nE                   \tat java.base/java.lang.Thread.run(Thread.java:840)\n\nvenv/lib/python3.14/site-packages/py4j/protocol.py:327: Py4JJavaError\n\n----------------------------- Captured stderr call -----------------------------\n26/01/22 12:17:29 ERROR Executor: Exception in task 0.0 in stage 39.0 (TID 37)\norg.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\n    check_python_version(infile)\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\n    raise PySparkRuntimeError(\npyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nPlease check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n26/01/22 12:17:29 WARN TaskSetManager: Lost task 0.0 in stage 39.0 (TID 37) (10.20.80.191 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\n    check_python_version(infile)\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\n    raise PySparkRuntimeError(\npyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nPlease check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n\n26/01/22 12:17:29 ERROR TaskSetManager: Task 0 in stage 39.0 failed 1 times; aborting job\n26/01/22 12:17:29 WARN TaskSetManager: Lost task 0.0 in stage 40.0 (TID 38) (10.20.80.191 executor driver): TaskKilled (Stage cancelled: [SPARK_JOB_CANCELLED] Job 38 cancelled The corresponding SQL query has failed. SQLSTATE: XXKDA)\n&#34;}], &#34;tests/test_groups_transform.py::test_transform_groups_empty_agent_ids&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/test_groups_transform.py::test_transform_groups_empty_agent_ids&#34;, &#34;duration&#34;: &#34;120 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/test_groups_transform.py::test_transform_groups_empty_agent_ids&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;120 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;spark = &amp;lt;pyspark.sql.session.SparkSession object at 0x10ca1de80&amp;gt;\ngroups_schema = StructType([StructField(&amp;#x27;id&amp;#x27;, LongType(), True), StructField(&amp;#x27;name&amp;#x27;, StringType(), True), StructField(&amp;#x27;parent_group_id...(), True), StructField(&amp;#x27;updated_at&amp;#x27;, StringType(), True), StructField(&amp;#x27;agent_ids&amp;#x27;, ArrayType(LongType(), True), True)])\nagents_schema = StructType([StructField(&amp;#x27;agent_id&amp;#x27;, LongType(), True), StructField(&amp;#x27;name&amp;#x27;, StringType(), True)])\n\n    def test_transform_groups_empty_agent_ids(spark, groups_schema, agents_schema):\n        raw_data = [\n            (3, &amp;quot;Finance&amp;quot;, None, &amp;quot;2024-01-01&amp;quot;, &amp;quot;2024-01-02&amp;quot;, [])\n        ]\n    \n        agents_data = [\n            (10, &amp;quot;Alice&amp;quot;),\n        ]\n    \n        raw_df = spark.createDataFrame(raw_data, groups_schema)\n        agents_df = spark.createDataFrame(agents_data, agents_schema)\n    \n        groups_df, valid_df, error_df = transform_groups(raw_df, agents_df)\n    \n&amp;gt;       assert groups_df.count() == 1\n               ^^^^^^^^^^^^^^^^^\n\ntests/test_groups_transform.py:129: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \nvenv/lib/python3.14/site-packages/pyspark/sql/classic/dataframe.py:439: in count\n    return int(self._jdf.count())\n               ^^^^^^^^^^^^^^^^^\nvenv/lib/python3.14/site-packages/py4j/java_gateway.py:1362: in __call__\n    return_value = get_return_value(\nvenv/lib/python3.14/site-packages/pyspark/errors/exceptions/captured.py:263: in deco\n    return f(*a, **kw)\n           ^^^^^^^^^^^\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nanswer = &amp;#x27;xro8962&amp;#x27;\ngateway_client = &amp;lt;py4j.clientserver.JavaClient object at 0x10ca1c6e0&amp;gt;\ntarget_id = &amp;#x27;o8888&amp;#x27;, name = &amp;#x27;count&amp;#x27;\n\n    def get_return_value(answer, gateway_client, target_id=None, name=None):\n        &amp;quot;&amp;quot;&amp;quot;Converts an answer received from the Java gateway into a Python object.\n    \n        For example, string representation of integers are converted to Python\n        integer, string representation of objects are converted to JavaObject\n        instances, etc.\n    \n        :param answer: the string returned by the Java gateway\n        :param gateway_client: the gateway client used to communicate with the Java\n            Gateway. Only necessary if the answer is a reference (e.g., object,\n            list, map)\n        :param target_id: the name of the object from which the answer comes from\n            (e.g., *object1* in `object1.hello()`). Optional.\n        :param name: the name of the member from which the answer comes from\n            (e.g., *hello* in `object1.hello()`). Optional.\n        &amp;quot;&amp;quot;&amp;quot;\n        if is_error(answer)[0]:\n            if len(answer) &amp;gt; 1:\n                type = answer[1]\n                value = OUTPUT_CONVERTER[type](answer[2:], gateway_client)\n                if answer[1] == REFERENCE_TYPE:\n&amp;gt;                   raise Py4JJavaError(\n                        &amp;quot;An error occurred while calling {0}{1}{2}.\\n&amp;quot;.\n                        format(target_id, &amp;quot;.&amp;quot;, name), value)\nE                   py4j.protocol.Py4JJavaError: An error occurred while calling o8888.count.\nE                   : org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 41.0 failed 1 times, most recent failure: Lost task 0.0 in stage 41.0 (TID 39) (10.20.80.191 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\nE                       check_python_version(infile)\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\nE                       raise PySparkRuntimeError(\nE                   pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nE                   Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\nE                   \nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\nE                   \tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\nE                   \tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\nE                   \tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\nE                   \tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\nE                   \tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\nE                   \tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\nE                   \tat org.apache.spark.scheduler.Task.run(Task.scala:147)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\nE                   \tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\nE                   \tat java.base/java.lang.Thread.run(Thread.java:840)\nE                   \nE                   Driver stacktrace:\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$3(DAGScheduler.scala:3122)\nE                   \tat scala.Option.getOrElse(Option.scala:201)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:3122)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:3114)\nE                   \tat scala.collection.immutable.List.foreach(List.scala:323)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:3114)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1303)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1303)\nE                   \tat scala.Option.foreach(Option.scala:437)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1303)\nE                   \tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3397)\nE                   \tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3328)\nE                   \tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3317)\nE                   \tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:50)\nE                   Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\nE                       check_python_version(infile)\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\nE                       raise PySparkRuntimeError(\nE                   pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nE                   Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\nE                   \nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\nE                   \tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\nE                   \tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\nE                   \tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\nE                   \tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\nE                   \tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\nE                   \tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\nE                   \tat org.apache.spark.scheduler.Task.run(Task.scala:147)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\nE                   \tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\nE                   \tat java.base/java.lang.Thread.run(Thread.java:840)\n\nvenv/lib/python3.14/site-packages/py4j/protocol.py:327: Py4JJavaError\n\n----------------------------- Captured stderr call -----------------------------\n26/01/22 12:17:29 ERROR Executor: Exception in task 0.0 in stage 41.0 (TID 39)\norg.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\n    check_python_version(infile)\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\n    raise PySparkRuntimeError(\npyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nPlease check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n26/01/22 12:17:29 WARN TaskSetManager: Lost task 0.0 in stage 41.0 (TID 39) (10.20.80.191 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\n    check_python_version(infile)\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\n    raise PySparkRuntimeError(\npyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nPlease check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n\n26/01/22 12:17:29 ERROR TaskSetManager: Task 0 in stage 41.0 failed 1 times; aborting job\n&#34;}], &#34;tests/test_groups_transform.py::test_transform_groups_null_agent_ids&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/test_groups_transform.py::test_transform_groups_null_agent_ids&#34;, &#34;duration&#34;: &#34;113 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/test_groups_transform.py::test_transform_groups_null_agent_ids&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;113 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;spark = &amp;lt;pyspark.sql.session.SparkSession object at 0x10ca1de80&amp;gt;\ngroups_schema = StructType([StructField(&amp;#x27;id&amp;#x27;, LongType(), True), StructField(&amp;#x27;name&amp;#x27;, StringType(), True), StructField(&amp;#x27;parent_group_id...(), True), StructField(&amp;#x27;updated_at&amp;#x27;, StringType(), True), StructField(&amp;#x27;agent_ids&amp;#x27;, ArrayType(LongType(), True), True)])\nagents_schema = StructType([StructField(&amp;#x27;agent_id&amp;#x27;, LongType(), True), StructField(&amp;#x27;name&amp;#x27;, StringType(), True)])\n\n    def test_transform_groups_null_agent_ids(spark, groups_schema, agents_schema):\n        raw_data = [\n            (4, &amp;quot;Legal&amp;quot;, None, &amp;quot;2024-01-01&amp;quot;, &amp;quot;2024-01-02&amp;quot;, None)\n        ]\n    \n        agents_data = [\n            (10, &amp;quot;Alice&amp;quot;),\n        ]\n    \n        raw_df = spark.createDataFrame(raw_data, groups_schema)\n        agents_df = spark.createDataFrame(agents_data, agents_schema)\n    \n        groups_df, valid_df, error_df = transform_groups(raw_df, agents_df)\n    \n&amp;gt;       assert groups_df.count() == 1\n               ^^^^^^^^^^^^^^^^^\n\ntests/test_groups_transform.py:151: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \nvenv/lib/python3.14/site-packages/pyspark/sql/classic/dataframe.py:439: in count\n    return int(self._jdf.count())\n               ^^^^^^^^^^^^^^^^^\nvenv/lib/python3.14/site-packages/py4j/java_gateway.py:1362: in __call__\n    return_value = get_return_value(\nvenv/lib/python3.14/site-packages/pyspark/errors/exceptions/captured.py:263: in deco\n    return f(*a, **kw)\n           ^^^^^^^^^^^\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nanswer = &amp;#x27;xro9125&amp;#x27;\ngateway_client = &amp;lt;py4j.clientserver.JavaClient object at 0x10ca1c6e0&amp;gt;\ntarget_id = &amp;#x27;o9051&amp;#x27;, name = &amp;#x27;count&amp;#x27;\n\n    def get_return_value(answer, gateway_client, target_id=None, name=None):\n        &amp;quot;&amp;quot;&amp;quot;Converts an answer received from the Java gateway into a Python object.\n    \n        For example, string representation of integers are converted to Python\n        integer, string representation of objects are converted to JavaObject\n        instances, etc.\n    \n        :param answer: the string returned by the Java gateway\n        :param gateway_client: the gateway client used to communicate with the Java\n            Gateway. Only necessary if the answer is a reference (e.g., object,\n            list, map)\n        :param target_id: the name of the object from which the answer comes from\n            (e.g., *object1* in `object1.hello()`). Optional.\n        :param name: the name of the member from which the answer comes from\n            (e.g., *hello* in `object1.hello()`). Optional.\n        &amp;quot;&amp;quot;&amp;quot;\n        if is_error(answer)[0]:\n            if len(answer) &amp;gt; 1:\n                type = answer[1]\n                value = OUTPUT_CONVERTER[type](answer[2:], gateway_client)\n                if answer[1] == REFERENCE_TYPE:\n&amp;gt;                   raise Py4JJavaError(\n                        &amp;quot;An error occurred while calling {0}{1}{2}.\\n&amp;quot;.\n                        format(target_id, &amp;quot;.&amp;quot;, name), value)\nE                   py4j.protocol.Py4JJavaError: An error occurred while calling o9051.count.\nE                   : org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 42.0 failed 1 times, most recent failure: Lost task 0.0 in stage 42.0 (TID 40) (10.20.80.191 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\nE                       check_python_version(infile)\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\nE                       raise PySparkRuntimeError(\nE                   pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nE                   Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\nE                   \nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\nE                   \tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\nE                   \tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\nE                   \tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\nE                   \tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\nE                   \tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\nE                   \tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\nE                   \tat org.apache.spark.scheduler.Task.run(Task.scala:147)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\nE                   \tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\nE                   \tat java.base/java.lang.Thread.run(Thread.java:840)\nE                   \nE                   Driver stacktrace:\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$3(DAGScheduler.scala:3122)\nE                   \tat scala.Option.getOrElse(Option.scala:201)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:3122)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:3114)\nE                   \tat scala.collection.immutable.List.foreach(List.scala:323)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:3114)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1303)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1303)\nE                   \tat scala.Option.foreach(Option.scala:437)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1303)\nE                   \tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3397)\nE                   \tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3328)\nE                   \tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3317)\nE                   \tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:50)\nE                   Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\nE                       check_python_version(infile)\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\nE                       raise PySparkRuntimeError(\nE                   pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nE                   Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\nE                   \nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\nE                   \tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\nE                   \tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\nE                   \tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\nE                   \tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\nE                   \tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\nE                   \tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\nE                   \tat org.apache.spark.scheduler.Task.run(Task.scala:147)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\nE                   \tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\nE                   \tat java.base/java.lang.Thread.run(Thread.java:840)\n\nvenv/lib/python3.14/site-packages/py4j/protocol.py:327: Py4JJavaError\n\n----------------------------- Captured stderr call -----------------------------\n26/01/22 12:17:30 ERROR Executor: Exception in task 0.0 in stage 42.0 (TID 40)\norg.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\n    check_python_version(infile)\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\n    raise PySparkRuntimeError(\npyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nPlease check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n26/01/22 12:17:30 WARN TaskSetManager: Lost task 0.0 in stage 42.0 (TID 40) (10.20.80.191 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\n    check_python_version(infile)\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\n    raise PySparkRuntimeError(\npyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nPlease check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n\n26/01/22 12:17:30 ERROR TaskSetManager: Task 0 in stage 42.0 failed 1 times; aborting job\n&#34;}], &#34;tests/test_groups_transform.py::test_transform_groups_empty_raw_df&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/test_groups_transform.py::test_transform_groups_empty_raw_df&#34;, &#34;duration&#34;: &#34;127 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/test_groups_transform.py::test_transform_groups_empty_raw_df&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;127 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;spark = &amp;lt;pyspark.sql.session.SparkSession object at 0x10ca1de80&amp;gt;\ngroups_schema = StructType([StructField(&amp;#x27;id&amp;#x27;, LongType(), True), StructField(&amp;#x27;name&amp;#x27;, StringType(), True), StructField(&amp;#x27;parent_group_id...(), True), StructField(&amp;#x27;updated_at&amp;#x27;, StringType(), True), StructField(&amp;#x27;agent_ids&amp;#x27;, ArrayType(LongType(), True), True)])\nagents_schema = StructType([StructField(&amp;#x27;agent_id&amp;#x27;, LongType(), True), StructField(&amp;#x27;name&amp;#x27;, StringType(), True)])\n\n    def test_transform_groups_empty_raw_df(spark, groups_schema, agents_schema):\n        raw_df = spark.createDataFrame([], groups_schema)\n    \n        agents_data = [\n            (10, &amp;quot;Alice&amp;quot;),\n        ]\n        agents_df = spark.createDataFrame(agents_data, agents_schema)\n    \n        groups_df, valid_df, error_df = transform_groups(raw_df, agents_df)\n    \n&amp;gt;       assert groups_df.count() == 0\n               ^^^^^^^^^^^^^^^^^\n\ntests/test_groups_transform.py:169: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \nvenv/lib/python3.14/site-packages/pyspark/sql/classic/dataframe.py:439: in count\n    return int(self._jdf.count())\n               ^^^^^^^^^^^^^^^^^\nvenv/lib/python3.14/site-packages/py4j/java_gateway.py:1362: in __call__\n    return_value = get_return_value(\nvenv/lib/python3.14/site-packages/pyspark/errors/exceptions/captured.py:263: in deco\n    return f(*a, **kw)\n           ^^^^^^^^^^^\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nanswer = &amp;#x27;xro9288&amp;#x27;\ngateway_client = &amp;lt;py4j.clientserver.JavaClient object at 0x10ca1c6e0&amp;gt;\ntarget_id = &amp;#x27;o9214&amp;#x27;, name = &amp;#x27;count&amp;#x27;\n\n    def get_return_value(answer, gateway_client, target_id=None, name=None):\n        &amp;quot;&amp;quot;&amp;quot;Converts an answer received from the Java gateway into a Python object.\n    \n        For example, string representation of integers are converted to Python\n        integer, string representation of objects are converted to JavaObject\n        instances, etc.\n    \n        :param answer: the string returned by the Java gateway\n        :param gateway_client: the gateway client used to communicate with the Java\n            Gateway. Only necessary if the answer is a reference (e.g., object,\n            list, map)\n        :param target_id: the name of the object from which the answer comes from\n            (e.g., *object1* in `object1.hello()`). Optional.\n        :param name: the name of the member from which the answer comes from\n            (e.g., *hello* in `object1.hello()`). Optional.\n        &amp;quot;&amp;quot;&amp;quot;\n        if is_error(answer)[0]:\n            if len(answer) &amp;gt; 1:\n                type = answer[1]\n                value = OUTPUT_CONVERTER[type](answer[2:], gateway_client)\n                if answer[1] == REFERENCE_TYPE:\n&amp;gt;                   raise Py4JJavaError(\n                        &amp;quot;An error occurred while calling {0}{1}{2}.\\n&amp;quot;.\n                        format(target_id, &amp;quot;.&amp;quot;, name), value)\nE                   py4j.protocol.Py4JJavaError: An error occurred while calling o9214.count.\nE                   : org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 43.0 failed 1 times, most recent failure: Lost task 0.0 in stage 43.0 (TID 41) (10.20.80.191 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\nE                       check_python_version(infile)\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\nE                       raise PySparkRuntimeError(\nE                   pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nE                   Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\nE                   \nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\nE                   \tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\nE                   \tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\nE                   \tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\nE                   \tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\nE                   \tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\nE                   \tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\nE                   \tat org.apache.spark.scheduler.Task.run(Task.scala:147)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\nE                   \tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\nE                   \tat java.base/java.lang.Thread.run(Thread.java:840)\nE                   \nE                   Driver stacktrace:\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$3(DAGScheduler.scala:3122)\nE                   \tat scala.Option.getOrElse(Option.scala:201)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:3122)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:3114)\nE                   \tat scala.collection.immutable.List.foreach(List.scala:323)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:3114)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1303)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1303)\nE                   \tat scala.Option.foreach(Option.scala:437)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1303)\nE                   \tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3397)\nE                   \tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3328)\nE                   \tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3317)\nE                   \tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:50)\nE                   Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\nE                       check_python_version(infile)\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\nE                       raise PySparkRuntimeError(\nE                   pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nE                   Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\nE                   \nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\nE                   \tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\nE                   \tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\nE                   \tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\nE                   \tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\nE                   \tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\nE                   \tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\nE                   \tat org.apache.spark.scheduler.Task.run(Task.scala:147)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\nE                   \tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\nE                   \tat java.base/java.lang.Thread.run(Thread.java:840)\n\nvenv/lib/python3.14/site-packages/py4j/protocol.py:327: Py4JJavaError\n\n----------------------------- Captured stderr call -----------------------------\n26/01/22 12:17:30 ERROR Executor: Exception in task 0.0 in stage 43.0 (TID 41)\norg.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\n    check_python_version(infile)\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\n    raise PySparkRuntimeError(\npyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nPlease check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n26/01/22 12:17:30 WARN TaskSetManager: Lost task 0.0 in stage 43.0 (TID 41) (10.20.80.191 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\n    check_python_version(infile)\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\n    raise PySparkRuntimeError(\npyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nPlease check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n\n26/01/22 12:17:30 ERROR TaskSetManager: Task 0 in stage 43.0 failed 1 times; aborting job\n&#34;}], &#34;tests/test_groups_transform.py::test_transform_groups_parent_group&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/test_groups_transform.py::test_transform_groups_parent_group&#34;, &#34;duration&#34;: &#34;96 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/test_groups_transform.py::test_transform_groups_parent_group&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;96 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;spark = &amp;lt;pyspark.sql.session.SparkSession object at 0x10ca1de80&amp;gt;\ngroups_schema = StructType([StructField(&amp;#x27;id&amp;#x27;, LongType(), True), StructField(&amp;#x27;name&amp;#x27;, StringType(), True), StructField(&amp;#x27;parent_group_id...(), True), StructField(&amp;#x27;updated_at&amp;#x27;, StringType(), True), StructField(&amp;#x27;agent_ids&amp;#x27;, ArrayType(LongType(), True), True)])\nagents_schema = StructType([StructField(&amp;#x27;agent_id&amp;#x27;, LongType(), True), StructField(&amp;#x27;name&amp;#x27;, StringType(), True)])\n\n    def test_transform_groups_parent_group(spark, groups_schema, agents_schema):\n        raw_data = [\n            (10, &amp;quot;Parent&amp;quot;, None, &amp;quot;2024-01-01&amp;quot;, &amp;quot;2024-01-02&amp;quot;, []),\n            (11, &amp;quot;Child&amp;quot;, 10, &amp;quot;2024-01-01&amp;quot;, &amp;quot;2024-01-02&amp;quot;, [])\n        ]\n    \n        agents_data = []\n    \n        raw_df = spark.createDataFrame(raw_data, groups_schema)\n        agents_df = spark.createDataFrame(agents_data, agents_schema)\n    \n        groups_df, valid_df, error_df = transform_groups(raw_df, agents_df)\n    \n&amp;gt;       rows = {row.group_id: row.parent_group_id for row in groups_df.collect()}\n                                                             ^^^^^^^^^^^^^^^^^^^\n\ntests/test_groups_transform.py:190: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \nvenv/lib/python3.14/site-packages/pyspark/sql/classic/dataframe.py:443: in collect\n    sock_info = self._jdf.collectToPython()\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^\nvenv/lib/python3.14/site-packages/py4j/java_gateway.py:1362: in __call__\n    return_value = get_return_value(\nvenv/lib/python3.14/site-packages/pyspark/errors/exceptions/captured.py:263: in deco\n    return f(*a, **kw)\n           ^^^^^^^^^^^\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nanswer = &amp;#x27;xro9451&amp;#x27;\ngateway_client = &amp;lt;py4j.clientserver.JavaClient object at 0x10ca1c6e0&amp;gt;\ntarget_id = &amp;#x27;o9377&amp;#x27;, name = &amp;#x27;collectToPython&amp;#x27;\n\n    def get_return_value(answer, gateway_client, target_id=None, name=None):\n        &amp;quot;&amp;quot;&amp;quot;Converts an answer received from the Java gateway into a Python object.\n    \n        For example, string representation of integers are converted to Python\n        integer, string representation of objects are converted to JavaObject\n        instances, etc.\n    \n        :param answer: the string returned by the Java gateway\n        :param gateway_client: the gateway client used to communicate with the Java\n            Gateway. Only necessary if the answer is a reference (e.g., object,\n            list, map)\n        :param target_id: the name of the object from which the answer comes from\n            (e.g., *object1* in `object1.hello()`). Optional.\n        :param name: the name of the member from which the answer comes from\n            (e.g., *hello* in `object1.hello()`). Optional.\n        &amp;quot;&amp;quot;&amp;quot;\n        if is_error(answer)[0]:\n            if len(answer) &amp;gt; 1:\n                type = answer[1]\n                value = OUTPUT_CONVERTER[type](answer[2:], gateway_client)\n                if answer[1] == REFERENCE_TYPE:\n&amp;gt;                   raise Py4JJavaError(\n                        &amp;quot;An error occurred while calling {0}{1}{2}.\\n&amp;quot;.\n                        format(target_id, &amp;quot;.&amp;quot;, name), value)\nE                   py4j.protocol.Py4JJavaError: An error occurred while calling o9377.collectToPython.\nE                   : org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 44.0 failed 1 times, most recent failure: Lost task 0.0 in stage 44.0 (TID 42) (10.20.80.191 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\nE                       check_python_version(infile)\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\nE                       raise PySparkRuntimeError(\nE                   pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nE                   Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\nE                   \nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\nE                   \tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\nE                   \tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\nE                   \tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\nE                   \tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\nE                   \tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:402)\nE                   \tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:901)\nE                   \tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:901)\nE                   \tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\nE                   \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\nE                   \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\nE                   \tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\nE                   \tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\nE                   \tat org.apache.spark.scheduler.Task.run(Task.scala:147)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\nE                   \tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\nE                   \tat java.base/java.lang.Thread.run(Thread.java:840)\nE                   \nE                   Driver stacktrace:\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$3(DAGScheduler.scala:3122)\nE                   \tat scala.Option.getOrElse(Option.scala:201)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:3122)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:3114)\nE                   \tat scala.collection.immutable.List.foreach(List.scala:323)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:3114)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1303)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1303)\nE                   \tat scala.Option.foreach(Option.scala:437)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1303)\nE                   \tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3397)\nE                   \tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3328)\nE                   \tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3317)\nE                   \tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:50)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1017)\nE                   \tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2496)\nE                   \tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2517)\nE                   \tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2536)\nE                   \tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2561)\nE                   \tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1057)\nE                   \tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\nE                   \tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\nE                   \tat org.apache.spark.rdd.RDD.withScope(RDD.scala:417)\nE                   \tat org.apache.spark.rdd.RDD.collect(RDD.scala:1056)\nE                   \tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:462)\nE                   \tat org.apache.spark.sql.classic.Dataset.$anonfun$collectToPython$1(Dataset.scala:2085)\nE                   \tat org.apache.spark.sql.classic.Dataset.$anonfun$withAction$2(Dataset.scala:2265)\nE                   \tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:717)\nE                   \tat org.apache.spark.sql.classic.Dataset.$anonfun$withAction$1(Dataset.scala:2263)\nE                   \tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:177)\nE                   \tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:285)\nE                   \tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:139)\nE                   \tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\nE                   \tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)\nE                   \tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)\nE                   \tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)\nE                   \tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:139)\nE                   \tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:308)\nE                   \tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:138)\nE                   \tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\nE                   \tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:92)\nE                   \tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:250)\nE                   \tat org.apache.spark.sql.classic.Dataset.withAction(Dataset.scala:2263)\nE                   \tat org.apache.spark.sql.classic.Dataset.collectToPython(Dataset.scala:2081)\nE                   \tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nE                   \tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\nE                   \tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\nE                   \tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\nE                   \tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\nE                   \tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\nE                   \tat py4j.Gateway.invoke(Gateway.java:282)\nE                   \tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\nE                   \tat py4j.commands.CallCommand.execute(CallCommand.java:79)\nE                   \tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\nE                   \tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\nE                   \tat java.base/java.lang.Thread.run(Thread.java:840)\nE                   Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\nE                       check_python_version(infile)\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\nE                       raise PySparkRuntimeError(\nE                   pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nE                   Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\nE                   \nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\nE                   \tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\nE                   \tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\nE                   \tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\nE                   \tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\nE                   \tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:402)\nE                   \tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:901)\nE                   \tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:901)\nE                   \tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\nE                   \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\nE                   \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\nE                   \tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\nE                   \tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\nE                   \tat org.apache.spark.scheduler.Task.run(Task.scala:147)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\nE                   \tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\nE                   \t... 1 more\n\nvenv/lib/python3.14/site-packages/py4j/protocol.py:327: Py4JJavaError\n\n----------------------------- Captured stderr call -----------------------------\n26/01/22 12:17:30 ERROR Executor: Exception in task 0.0 in stage 44.0 (TID 42)\norg.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\n    check_python_version(infile)\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\n    raise PySparkRuntimeError(\npyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nPlease check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:402)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:901)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:901)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n26/01/22 12:17:30 WARN TaskSetManager: Lost task 0.0 in stage 44.0 (TID 42) (10.20.80.191 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\n    check_python_version(infile)\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\n    raise PySparkRuntimeError(\npyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nPlease check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:402)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:901)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:901)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n\n26/01/22 12:17:30 ERROR TaskSetManager: Task 0 in stage 44.0 failed 1 times; aborting job\n&#34;}], &#34;tests/test_groups_transform.py::test_transform_groups_schema&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Passed&#34;, &#34;testId&#34;: &#34;tests/test_groups_transform.py::test_transform_groups_schema&#34;, &#34;duration&#34;: &#34;47 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Passed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/test_groups_transform.py::test_transform_groups_schema&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;47 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;No log output captured.&#34;}], &#34;tests/test_roles_transform.py::test_transform_roles_valid_flat&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/test_roles_transform.py::test_transform_roles_valid_flat&#34;, &#34;duration&#34;: &#34;107 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/test_roles_transform.py::test_transform_roles_valid_flat&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;107 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;spark = &amp;lt;pyspark.sql.session.SparkSession object at 0x10ca1de80&amp;gt;\nflat_roles_schema = StructType([StructField(&amp;#x27;id&amp;#x27;, StringType(), True), StructField(&amp;#x27;name&amp;#x27;, StringType(), True), StructField(&amp;#x27;description&amp;#x27;,...ringType(), True), StructField(&amp;#x27;created_at&amp;#x27;, TimestampType(), True), StructField(&amp;#x27;updated_at&amp;#x27;, TimestampType(), True)])\n\n    def test_transform_roles_valid_flat(spark, flat_roles_schema):\n        df = spark.createDataFrame(\n            [(&amp;quot;1&amp;quot;, &amp;quot;Admin&amp;quot;, &amp;quot;Administrator role&amp;quot;, True, &amp;quot;support&amp;quot;, None, None)],\n            flat_roles_schema\n        )\n    \n        final_df, error_df = transform_roles(df)\n    \n&amp;gt;       assert final_df.count() == 1\n               ^^^^^^^^^^^^^^^^\n\ntests/test_roles_transform.py:73: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \nvenv/lib/python3.14/site-packages/pyspark/sql/classic/dataframe.py:439: in count\n    return int(self._jdf.count())\n               ^^^^^^^^^^^^^^^^^\nvenv/lib/python3.14/site-packages/py4j/java_gateway.py:1362: in __call__\n    return_value = get_return_value(\nvenv/lib/python3.14/site-packages/pyspark/errors/exceptions/captured.py:263: in deco\n    return f(*a, **kw)\n           ^^^^^^^^^^^\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nanswer = &amp;#x27;xro9753&amp;#x27;\ngateway_client = &amp;lt;py4j.clientserver.JavaClient object at 0x10ca1c6e0&amp;gt;\ntarget_id = &amp;#x27;o9752&amp;#x27;, name = &amp;#x27;count&amp;#x27;\n\n    def get_return_value(answer, gateway_client, target_id=None, name=None):\n        &amp;quot;&amp;quot;&amp;quot;Converts an answer received from the Java gateway into a Python object.\n    \n        For example, string representation of integers are converted to Python\n        integer, string representation of objects are converted to JavaObject\n        instances, etc.\n    \n        :param answer: the string returned by the Java gateway\n        :param gateway_client: the gateway client used to communicate with the Java\n            Gateway. Only necessary if the answer is a reference (e.g., object,\n            list, map)\n        :param target_id: the name of the object from which the answer comes from\n            (e.g., *object1* in `object1.hello()`). Optional.\n        :param name: the name of the member from which the answer comes from\n            (e.g., *hello* in `object1.hello()`). Optional.\n        &amp;quot;&amp;quot;&amp;quot;\n        if is_error(answer)[0]:\n            if len(answer) &amp;gt; 1:\n                type = answer[1]\n                value = OUTPUT_CONVERTER[type](answer[2:], gateway_client)\n                if answer[1] == REFERENCE_TYPE:\n&amp;gt;                   raise Py4JJavaError(\n                        &amp;quot;An error occurred while calling {0}{1}{2}.\\n&amp;quot;.\n                        format(target_id, &amp;quot;.&amp;quot;, name), value)\nE                   py4j.protocol.Py4JJavaError: An error occurred while calling o9752.count.\nE                   : org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 45.0 failed 1 times, most recent failure: Lost task 0.0 in stage 45.0 (TID 43) (10.20.80.191 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\nE                       check_python_version(infile)\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\nE                       raise PySparkRuntimeError(\nE                   pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nE                   Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\nE                   \nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\nE                   \tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\nE                   \tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\nE                   \tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\nE                   \tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\nE                   \tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\nE                   \tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\nE                   \tat org.apache.spark.scheduler.Task.run(Task.scala:147)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\nE                   \tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\nE                   \tat java.base/java.lang.Thread.run(Thread.java:840)\nE                   \nE                   Driver stacktrace:\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$3(DAGScheduler.scala:3122)\nE                   \tat scala.Option.getOrElse(Option.scala:201)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:3122)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:3114)\nE                   \tat scala.collection.immutable.List.foreach(List.scala:323)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:3114)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1303)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1303)\nE                   \tat scala.Option.foreach(Option.scala:437)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1303)\nE                   \tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3397)\nE                   \tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3328)\nE                   \tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3317)\nE                   \tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:50)\nE                   Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\nE                       check_python_version(infile)\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\nE                       raise PySparkRuntimeError(\nE                   pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nE                   Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\nE                   \nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\nE                   \tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\nE                   \tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\nE                   \tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\nE                   \tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\nE                   \tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\nE                   \tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\nE                   \tat org.apache.spark.scheduler.Task.run(Task.scala:147)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\nE                   \tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\nE                   \tat java.base/java.lang.Thread.run(Thread.java:840)\n\nvenv/lib/python3.14/site-packages/py4j/protocol.py:327: Py4JJavaError\n\n---------------------------- Captured stderr setup -----------------------------\n26/01/22 12:17:30 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n\n----------------------------- Captured stderr call -----------------------------\n26/01/22 12:17:30 ERROR Executor: Exception in task 0.0 in stage 45.0 (TID 43)\norg.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\n    check_python_version(infile)\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\n    raise PySparkRuntimeError(\npyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nPlease check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n26/01/22 12:17:30 WARN TaskSetManager: Lost task 0.0 in stage 45.0 (TID 43) (10.20.80.191 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\n    check_python_version(infile)\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\n    raise PySparkRuntimeError(\npyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nPlease check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n\n26/01/22 12:17:30 ERROR TaskSetManager: Task 0 in stage 45.0 failed 1 times; aborting job\n&#34;}], &#34;tests/test_roles_transform.py::test_transform_roles_valid_nested&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/test_roles_transform.py::test_transform_roles_valid_nested&#34;, &#34;duration&#34;: &#34;163 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/test_roles_transform.py::test_transform_roles_valid_nested&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;163 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;spark = &amp;lt;pyspark.sql.session.SparkSession object at 0x10ca1de80&amp;gt;\nnested_roles_schema = StructType([StructField(&amp;#x27;id&amp;#x27;, StringType(), True), StructField(&amp;#x27;role&amp;#x27;, StructType([StructField(&amp;#x27;name&amp;#x27;, StringType(), T...), True), StructField(&amp;#x27;created_at&amp;#x27;, TimestampType(), True), StructField(&amp;#x27;updated_at&amp;#x27;, TimestampType(), True)]), True)])\n\n    def test_transform_roles_valid_nested(spark, nested_roles_schema):\n        df = spark.createDataFrame(\n            [\n                (\n                    &amp;quot;10&amp;quot;,\n                    (&amp;quot;Supervisor&amp;quot;, &amp;quot;Supervisory role&amp;quot;, False, &amp;quot;admin&amp;quot;, None, None)\n                )\n            ],\n            nested_roles_schema\n        )\n    \n        final_df, error_df = transform_roles(df)\n    \n&amp;gt;       assert final_df.count() == 1\n               ^^^^^^^^^^^^^^^^\n\ntests/test_roles_transform.py:97: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \nvenv/lib/python3.14/site-packages/pyspark/sql/classic/dataframe.py:439: in count\n    return int(self._jdf.count())\n               ^^^^^^^^^^^^^^^^^\nvenv/lib/python3.14/site-packages/py4j/java_gateway.py:1362: in __call__\n    return_value = get_return_value(\nvenv/lib/python3.14/site-packages/pyspark/errors/exceptions/captured.py:263: in deco\n    return f(*a, **kw)\n           ^^^^^^^^^^^\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nanswer = &amp;#x27;xro9908&amp;#x27;\ngateway_client = &amp;lt;py4j.clientserver.JavaClient object at 0x10ca1c6e0&amp;gt;\ntarget_id = &amp;#x27;o9907&amp;#x27;, name = &amp;#x27;count&amp;#x27;\n\n    def get_return_value(answer, gateway_client, target_id=None, name=None):\n        &amp;quot;&amp;quot;&amp;quot;Converts an answer received from the Java gateway into a Python object.\n    \n        For example, string representation of integers are converted to Python\n        integer, string representation of objects are converted to JavaObject\n        instances, etc.\n    \n        :param answer: the string returned by the Java gateway\n        :param gateway_client: the gateway client used to communicate with the Java\n            Gateway. Only necessary if the answer is a reference (e.g., object,\n            list, map)\n        :param target_id: the name of the object from which the answer comes from\n            (e.g., *object1* in `object1.hello()`). Optional.\n        :param name: the name of the member from which the answer comes from\n            (e.g., *hello* in `object1.hello()`). Optional.\n        &amp;quot;&amp;quot;&amp;quot;\n        if is_error(answer)[0]:\n            if len(answer) &amp;gt; 1:\n                type = answer[1]\n                value = OUTPUT_CONVERTER[type](answer[2:], gateway_client)\n                if answer[1] == REFERENCE_TYPE:\n&amp;gt;                   raise Py4JJavaError(\n                        &amp;quot;An error occurred while calling {0}{1}{2}.\\n&amp;quot;.\n                        format(target_id, &amp;quot;.&amp;quot;, name), value)\nE                   py4j.protocol.Py4JJavaError: An error occurred while calling o9907.count.\nE                   : org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 46.0 failed 1 times, most recent failure: Lost task 0.0 in stage 46.0 (TID 44) (10.20.80.191 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\nE                       check_python_version(infile)\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\nE                       raise PySparkRuntimeError(\nE                   pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nE                   Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\nE                   \nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\nE                   \tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\nE                   \tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\nE                   \tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\nE                   \tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\nE                   \tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\nE                   \tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\nE                   \tat org.apache.spark.scheduler.Task.run(Task.scala:147)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\nE                   \tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\nE                   \tat java.base/java.lang.Thread.run(Thread.java:840)\nE                   \nE                   Driver stacktrace:\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$3(DAGScheduler.scala:3122)\nE                   \tat scala.Option.getOrElse(Option.scala:201)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:3122)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:3114)\nE                   \tat scala.collection.immutable.List.foreach(List.scala:323)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:3114)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1303)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1303)\nE                   \tat scala.Option.foreach(Option.scala:437)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1303)\nE                   \tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3397)\nE                   \tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3328)\nE                   \tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3317)\nE                   \tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:50)\nE                   Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\nE                       check_python_version(infile)\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\nE                       raise PySparkRuntimeError(\nE                   pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nE                   Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\nE                   \nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\nE                   \tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\nE                   \tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\nE                   \tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\nE                   \tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\nE                   \tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\nE                   \tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\nE                   \tat org.apache.spark.scheduler.Task.run(Task.scala:147)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\nE                   \tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\nE                   \tat java.base/java.lang.Thread.run(Thread.java:840)\n\nvenv/lib/python3.14/site-packages/py4j/protocol.py:327: Py4JJavaError\n\n----------------------------- Captured stderr call -----------------------------\n26/01/22 12:17:30 ERROR Executor: Exception in task 0.0 in stage 46.0 (TID 44)\norg.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\n    check_python_version(infile)\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\n    raise PySparkRuntimeError(\npyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nPlease check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n26/01/22 12:17:30 WARN TaskSetManager: Lost task 0.0 in stage 46.0 (TID 44) (10.20.80.191 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\n    check_python_version(infile)\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\n    raise PySparkRuntimeError(\npyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nPlease check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n\n26/01/22 12:17:30 ERROR TaskSetManager: Task 0 in stage 46.0 failed 1 times; aborting job\n&#34;}], &#34;tests/test_roles_transform.py::test_transform_roles_missing_name_nested&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/test_roles_transform.py::test_transform_roles_missing_name_nested&#34;, &#34;duration&#34;: &#34;111 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/test_roles_transform.py::test_transform_roles_missing_name_nested&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;111 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;spark = &amp;lt;pyspark.sql.session.SparkSession object at 0x10ca1de80&amp;gt;\nnested_roles_schema = StructType([StructField(&amp;#x27;id&amp;#x27;, StringType(), True), StructField(&amp;#x27;role&amp;#x27;, StructType([StructField(&amp;#x27;name&amp;#x27;, StringType(), T...), True), StructField(&amp;#x27;created_at&amp;#x27;, TimestampType(), True), StructField(&amp;#x27;updated_at&amp;#x27;, TimestampType(), True)]), True)])\n\n    def test_transform_roles_missing_name_nested(spark, nested_roles_schema):\n        df = spark.createDataFrame(\n            [\n                (\n                    &amp;quot;5&amp;quot;,\n                    (None, &amp;quot;No name&amp;quot;, True, &amp;quot;support&amp;quot;, None, None)\n                )\n            ],\n            nested_roles_schema\n        )\n    \n        final_df, error_df = transform_roles(df)\n    \n&amp;gt;       assert final_df.count() == 0\n               ^^^^^^^^^^^^^^^^\n\ntests/test_roles_transform.py:122: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \nvenv/lib/python3.14/site-packages/pyspark/sql/classic/dataframe.py:439: in count\n    return int(self._jdf.count())\n               ^^^^^^^^^^^^^^^^^\nvenv/lib/python3.14/site-packages/py4j/java_gateway.py:1362: in __call__\n    return_value = get_return_value(\nvenv/lib/python3.14/site-packages/pyspark/errors/exceptions/captured.py:263: in deco\n    return f(*a, **kw)\n           ^^^^^^^^^^^\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nanswer = &amp;#x27;xro10063&amp;#x27;\ngateway_client = &amp;lt;py4j.clientserver.JavaClient object at 0x10ca1c6e0&amp;gt;\ntarget_id = &amp;#x27;o10062&amp;#x27;, name = &amp;#x27;count&amp;#x27;\n\n    def get_return_value(answer, gateway_client, target_id=None, name=None):\n        &amp;quot;&amp;quot;&amp;quot;Converts an answer received from the Java gateway into a Python object.\n    \n        For example, string representation of integers are converted to Python\n        integer, string representation of objects are converted to JavaObject\n        instances, etc.\n    \n        :param answer: the string returned by the Java gateway\n        :param gateway_client: the gateway client used to communicate with the Java\n            Gateway. Only necessary if the answer is a reference (e.g., object,\n            list, map)\n        :param target_id: the name of the object from which the answer comes from\n            (e.g., *object1* in `object1.hello()`). Optional.\n        :param name: the name of the member from which the answer comes from\n            (e.g., *hello* in `object1.hello()`). Optional.\n        &amp;quot;&amp;quot;&amp;quot;\n        if is_error(answer)[0]:\n            if len(answer) &amp;gt; 1:\n                type = answer[1]\n                value = OUTPUT_CONVERTER[type](answer[2:], gateway_client)\n                if answer[1] == REFERENCE_TYPE:\n&amp;gt;                   raise Py4JJavaError(\n                        &amp;quot;An error occurred while calling {0}{1}{2}.\\n&amp;quot;.\n                        format(target_id, &amp;quot;.&amp;quot;, name), value)\nE                   py4j.protocol.Py4JJavaError: An error occurred while calling o10062.count.\nE                   : org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 47.0 failed 1 times, most recent failure: Lost task 0.0 in stage 47.0 (TID 45) (10.20.80.191 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\nE                       check_python_version(infile)\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\nE                       raise PySparkRuntimeError(\nE                   pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nE                   Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\nE                   \nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\nE                   \tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\nE                   \tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\nE                   \tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\nE                   \tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\nE                   \tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\nE                   \tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\nE                   \tat org.apache.spark.scheduler.Task.run(Task.scala:147)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\nE                   \tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\nE                   \tat java.base/java.lang.Thread.run(Thread.java:840)\nE                   \nE                   Driver stacktrace:\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$3(DAGScheduler.scala:3122)\nE                   \tat scala.Option.getOrElse(Option.scala:201)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:3122)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:3114)\nE                   \tat scala.collection.immutable.List.foreach(List.scala:323)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:3114)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1303)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1303)\nE                   \tat scala.Option.foreach(Option.scala:437)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1303)\nE                   \tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3397)\nE                   \tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3328)\nE                   \tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3317)\nE                   \tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:50)\nE                   Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\nE                       check_python_version(infile)\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\nE                       raise PySparkRuntimeError(\nE                   pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nE                   Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\nE                   \nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\nE                   \tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\nE                   \tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\nE                   \tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\nE                   \tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\nE                   \tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\nE                   \tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\nE                   \tat org.apache.spark.scheduler.Task.run(Task.scala:147)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\nE                   \tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\nE                   \tat java.base/java.lang.Thread.run(Thread.java:840)\n\nvenv/lib/python3.14/site-packages/py4j/protocol.py:327: Py4JJavaError\n\n----------------------------- Captured stderr call -----------------------------\n26/01/22 12:17:30 ERROR Executor: Exception in task 0.0 in stage 47.0 (TID 45)\norg.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\n    check_python_version(infile)\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\n    raise PySparkRuntimeError(\npyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nPlease check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n26/01/22 12:17:30 WARN TaskSetManager: Lost task 0.0 in stage 47.0 (TID 45) (10.20.80.191 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\n    check_python_version(infile)\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\n    raise PySparkRuntimeError(\npyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nPlease check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n\n26/01/22 12:17:30 ERROR TaskSetManager: Task 0 in stage 47.0 failed 1 times; aborting job\n&#34;}], &#34;tests/test_roles_transform.py::test_transform_roles_non_numeric_id&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/test_roles_transform.py::test_transform_roles_non_numeric_id&#34;, &#34;duration&#34;: &#34;114 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/test_roles_transform.py::test_transform_roles_non_numeric_id&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;114 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;spark = &amp;lt;pyspark.sql.session.SparkSession object at 0x10ca1de80&amp;gt;\nflat_roles_schema = StructType([StructField(&amp;#x27;id&amp;#x27;, StringType(), True), StructField(&amp;#x27;name&amp;#x27;, StringType(), True), StructField(&amp;#x27;description&amp;#x27;,...ringType(), True), StructField(&amp;#x27;created_at&amp;#x27;, TimestampType(), True), StructField(&amp;#x27;updated_at&amp;#x27;, TimestampType(), True)])\n\n    def test_transform_roles_non_numeric_id(spark, flat_roles_schema):\n        df = spark.createDataFrame(\n            [(&amp;quot;abc&amp;quot;, &amp;quot;Admin&amp;quot;, &amp;quot;Role&amp;quot;, True, &amp;quot;support&amp;quot;, None, None)],\n            flat_roles_schema\n        )\n    \n        final_df, error_df = transform_roles(df)\n    \n&amp;gt;       assert final_df.count() == 0\n               ^^^^^^^^^^^^^^^^\n\ntests/test_roles_transform.py:137: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \nvenv/lib/python3.14/site-packages/pyspark/sql/classic/dataframe.py:439: in count\n    return int(self._jdf.count())\n               ^^^^^^^^^^^^^^^^^\nvenv/lib/python3.14/site-packages/py4j/java_gateway.py:1362: in __call__\n    return_value = get_return_value(\nvenv/lib/python3.14/site-packages/pyspark/errors/exceptions/captured.py:263: in deco\n    return f(*a, **kw)\n           ^^^^^^^^^^^\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nanswer = &amp;#x27;xro10228&amp;#x27;\ngateway_client = &amp;lt;py4j.clientserver.JavaClient object at 0x10ca1c6e0&amp;gt;\ntarget_id = &amp;#x27;o10227&amp;#x27;, name = &amp;#x27;count&amp;#x27;\n\n    def get_return_value(answer, gateway_client, target_id=None, name=None):\n        &amp;quot;&amp;quot;&amp;quot;Converts an answer received from the Java gateway into a Python object.\n    \n        For example, string representation of integers are converted to Python\n        integer, string representation of objects are converted to JavaObject\n        instances, etc.\n    \n        :param answer: the string returned by the Java gateway\n        :param gateway_client: the gateway client used to communicate with the Java\n            Gateway. Only necessary if the answer is a reference (e.g., object,\n            list, map)\n        :param target_id: the name of the object from which the answer comes from\n            (e.g., *object1* in `object1.hello()`). Optional.\n        :param name: the name of the member from which the answer comes from\n            (e.g., *hello* in `object1.hello()`). Optional.\n        &amp;quot;&amp;quot;&amp;quot;\n        if is_error(answer)[0]:\n            if len(answer) &amp;gt; 1:\n                type = answer[1]\n                value = OUTPUT_CONVERTER[type](answer[2:], gateway_client)\n                if answer[1] == REFERENCE_TYPE:\n&amp;gt;                   raise Py4JJavaError(\n                        &amp;quot;An error occurred while calling {0}{1}{2}.\\n&amp;quot;.\n                        format(target_id, &amp;quot;.&amp;quot;, name), value)\nE                   py4j.protocol.Py4JJavaError: An error occurred while calling o10227.count.\nE                   : org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 48.0 failed 1 times, most recent failure: Lost task 0.0 in stage 48.0 (TID 46) (10.20.80.191 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\nE                       check_python_version(infile)\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\nE                       raise PySparkRuntimeError(\nE                   pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nE                   Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\nE                   \nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\nE                   \tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\nE                   \tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\nE                   \tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\nE                   \tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\nE                   \tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\nE                   \tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\nE                   \tat org.apache.spark.scheduler.Task.run(Task.scala:147)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\nE                   \tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\nE                   \tat java.base/java.lang.Thread.run(Thread.java:840)\nE                   \nE                   Driver stacktrace:\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$3(DAGScheduler.scala:3122)\nE                   \tat scala.Option.getOrElse(Option.scala:201)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:3122)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:3114)\nE                   \tat scala.collection.immutable.List.foreach(List.scala:323)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:3114)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1303)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1303)\nE                   \tat scala.Option.foreach(Option.scala:437)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1303)\nE                   \tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3397)\nE                   \tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3328)\nE                   \tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3317)\nE                   \tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:50)\nE                   Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\nE                       check_python_version(infile)\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\nE                       raise PySparkRuntimeError(\nE                   pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nE                   Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\nE                   \nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\nE                   \tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\nE                   \tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\nE                   \tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\nE                   \tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\nE                   \tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\nE                   \tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\nE                   \tat org.apache.spark.scheduler.Task.run(Task.scala:147)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\nE                   \tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\nE                   \tat java.base/java.lang.Thread.run(Thread.java:840)\n\nvenv/lib/python3.14/site-packages/py4j/protocol.py:327: Py4JJavaError\n\n----------------------------- Captured stderr call -----------------------------\n26/01/22 12:17:31 ERROR Executor: Exception in task 0.0 in stage 48.0 (TID 46)\norg.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\n    check_python_version(infile)\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\n    raise PySparkRuntimeError(\npyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nPlease check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n26/01/22 12:17:31 WARN TaskSetManager: Lost task 0.0 in stage 48.0 (TID 46) (10.20.80.191 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\n    check_python_version(infile)\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\n    raise PySparkRuntimeError(\npyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nPlease check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n\n26/01/22 12:17:31 ERROR TaskSetManager: Task 0 in stage 48.0 failed 1 times; aborting job\n&#34;}], &#34;tests/test_roles_transform.py::test_transform_roles_mixed_flat_nested&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/test_roles_transform.py::test_transform_roles_mixed_flat_nested&#34;, &#34;duration&#34;: &#34;324 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/test_roles_transform.py::test_transform_roles_mixed_flat_nested&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;324 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;spark = &amp;lt;pyspark.sql.session.SparkSession object at 0x10ca1de80&amp;gt;\nflat_roles_schema = StructType([StructField(&amp;#x27;id&amp;#x27;, StringType(), True), StructField(&amp;#x27;name&amp;#x27;, StringType(), True), StructField(&amp;#x27;description&amp;#x27;,...ringType(), True), StructField(&amp;#x27;created_at&amp;#x27;, TimestampType(), True), StructField(&amp;#x27;updated_at&amp;#x27;, TimestampType(), True)])\nnested_roles_schema = StructType([StructField(&amp;#x27;id&amp;#x27;, StringType(), True), StructField(&amp;#x27;role&amp;#x27;, StructType([StructField(&amp;#x27;name&amp;#x27;, StringType(), T...), True), StructField(&amp;#x27;created_at&amp;#x27;, TimestampType(), True), StructField(&amp;#x27;updated_at&amp;#x27;, TimestampType(), True)]), True)])\n\n    def test_transform_roles_mixed_flat_nested(spark, flat_roles_schema, nested_roles_schema):\n        flat_df = spark.createDataFrame(\n            [(&amp;quot;1&amp;quot;, &amp;quot;Admin&amp;quot;, &amp;quot;Administrator&amp;quot;, True, &amp;quot;support&amp;quot;, None, None)],\n            flat_roles_schema\n        )\n    \n        nested_df = spark.createDataFrame(\n            [\n                (\n                    &amp;quot;2&amp;quot;,\n                    (&amp;quot;Agent&amp;quot;, &amp;quot;Agent role&amp;quot;, False, &amp;quot;support&amp;quot;, None, None)\n                )\n            ],\n            nested_roles_schema\n        )\n    \n        df = flat_df.unionByName(nested_df, allowMissingColumns=True)\n    \n        final_df, error_df = transform_roles(df)\n    \n&amp;gt;       assert final_df.count() == 2\n               ^^^^^^^^^^^^^^^^\n\ntests/test_roles_transform.py:164: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \nvenv/lib/python3.14/site-packages/pyspark/sql/classic/dataframe.py:439: in count\n    return int(self._jdf.count())\n               ^^^^^^^^^^^^^^^^^\nvenv/lib/python3.14/site-packages/py4j/java_gateway.py:1362: in __call__\n    return_value = get_return_value(\nvenv/lib/python3.14/site-packages/pyspark/errors/exceptions/captured.py:263: in deco\n    return f(*a, **kw)\n           ^^^^^^^^^^^\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nanswer = &amp;#x27;xro10416&amp;#x27;\ngateway_client = &amp;lt;py4j.clientserver.JavaClient object at 0x10ca1c6e0&amp;gt;\ntarget_id = &amp;#x27;o10415&amp;#x27;, name = &amp;#x27;count&amp;#x27;\n\n    def get_return_value(answer, gateway_client, target_id=None, name=None):\n        &amp;quot;&amp;quot;&amp;quot;Converts an answer received from the Java gateway into a Python object.\n    \n        For example, string representation of integers are converted to Python\n        integer, string representation of objects are converted to JavaObject\n        instances, etc.\n    \n        :param answer: the string returned by the Java gateway\n        :param gateway_client: the gateway client used to communicate with the Java\n            Gateway. Only necessary if the answer is a reference (e.g., object,\n            list, map)\n        :param target_id: the name of the object from which the answer comes from\n            (e.g., *object1* in `object1.hello()`). Optional.\n        :param name: the name of the member from which the answer comes from\n            (e.g., *hello* in `object1.hello()`). Optional.\n        &amp;quot;&amp;quot;&amp;quot;\n        if is_error(answer)[0]:\n            if len(answer) &amp;gt; 1:\n                type = answer[1]\n                value = OUTPUT_CONVERTER[type](answer[2:], gateway_client)\n                if answer[1] == REFERENCE_TYPE:\n&amp;gt;                   raise Py4JJavaError(\n                        &amp;quot;An error occurred while calling {0}{1}{2}.\\n&amp;quot;.\n                        format(target_id, &amp;quot;.&amp;quot;, name), value)\nE                   py4j.protocol.Py4JJavaError: An error occurred while calling o10415.count.\nE                   : org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 49.0 failed 1 times, most recent failure: Lost task 0.0 in stage 49.0 (TID 47) (10.20.80.191 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\nE                       check_python_version(infile)\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\nE                       raise PySparkRuntimeError(\nE                   pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nE                   Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\nE                   \nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\nE                   \tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\nE                   \tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\nE                   \tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\nE                   \tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\nE                   \tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\nE                   \tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\nE                   \tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\nE                   \tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\nE                   \tat org.apache.spark.scheduler.Task.run(Task.scala:147)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\nE                   \tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\nE                   \tat java.base/java.lang.Thread.run(Thread.java:840)\nE                   \nE                   Driver stacktrace:\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$3(DAGScheduler.scala:3122)\nE                   \tat scala.Option.getOrElse(Option.scala:201)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:3122)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:3114)\nE                   \tat scala.collection.immutable.List.foreach(List.scala:323)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:3114)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1303)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1303)\nE                   \tat scala.Option.foreach(Option.scala:437)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1303)\nE                   \tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3397)\nE                   \tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3328)\nE                   \tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3317)\nE                   \tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:50)\nE                   Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\nE                       check_python_version(infile)\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\nE                       raise PySparkRuntimeError(\nE                   pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nE                   Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\nE                   \nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\nE                   \tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\nE                   \tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\nE                   \tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\nE                   \tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\nE                   \tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\nE                   \tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\nE                   \tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\nE                   \tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\nE                   \tat org.apache.spark.scheduler.Task.run(Task.scala:147)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\nE                   \tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\nE                   \tat java.base/java.lang.Thread.run(Thread.java:840)\n\nvenv/lib/python3.14/site-packages/py4j/protocol.py:327: Py4JJavaError\n\n----------------------------- Captured stderr call -----------------------------\n26/01/22 12:17:31 ERROR Executor: Exception in task 0.0 in stage 49.0 (TID 47)\norg.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\n    check_python_version(infile)\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\n    raise PySparkRuntimeError(\npyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nPlease check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n26/01/22 12:17:31 WARN TaskSetManager: Lost task 0.0 in stage 49.0 (TID 47) (10.20.80.191 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\n    check_python_version(infile)\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\n    raise PySparkRuntimeError(\npyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nPlease check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n\n26/01/22 12:17:31 ERROR TaskSetManager: Task 0 in stage 49.0 failed 1 times; aborting job\n26/01/22 12:17:31 WARN TaskSetManager: Lost task 1.0 in stage 49.0 (TID 48) (10.20.80.191 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 0 in stage 49.0 failed 1 times, most recent failure: Lost task 0.0 in stage 49.0 (TID 47) (10.20.80.191 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\n    check_python_version(infile)\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\n    raise PySparkRuntimeError(\npyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nPlease check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n\nDriver stacktrace:)\n&#34;}], &#34;tests/test_roles_transform.py::test_transform_roles_extra_fields_ignored&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/test_roles_transform.py::test_transform_roles_extra_fields_ignored&#34;, &#34;duration&#34;: &#34;188 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/test_roles_transform.py::test_transform_roles_extra_fields_ignored&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;188 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;spark = &amp;lt;pyspark.sql.session.SparkSession object at 0x10ca1de80&amp;gt;\n\n    def test_transform_roles_extra_fields_ignored(spark):\n        schema = StructType([\n            StructField(&amp;quot;id&amp;quot;, StringType(), True),\n            StructField(&amp;quot;name&amp;quot;, StringType(), True),\n            StructField(&amp;quot;description&amp;quot;, StringType(), True),\n            StructField(&amp;quot;default&amp;quot;, BooleanType(), True),\n            StructField(&amp;quot;agent_type&amp;quot;, StringType(), True),\n            StructField(&amp;quot;created_at&amp;quot;, TimestampType(), True),\n            StructField(&amp;quot;updated_at&amp;quot;, TimestampType(), True),\n            StructField(&amp;quot;unexpected&amp;quot;, StringType(), True),\n        ])\n    \n        df = spark.createDataFrame(\n            [(&amp;quot;7&amp;quot;, &amp;quot;Custom&amp;quot;, &amp;quot;Extra field&amp;quot;, True, &amp;quot;support&amp;quot;, None, None, &amp;quot;boom&amp;quot;)],\n            schema\n        )\n    \n        final_df, error_df = transform_roles(df)\n    \n&amp;gt;       assert final_df.count() == 1\n               ^^^^^^^^^^^^^^^^\n\ntests/test_roles_transform.py:190: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \nvenv/lib/python3.14/site-packages/pyspark/sql/classic/dataframe.py:439: in count\n    return int(self._jdf.count())\n               ^^^^^^^^^^^^^^^^^\nvenv/lib/python3.14/site-packages/py4j/java_gateway.py:1362: in __call__\n    return_value = get_return_value(\nvenv/lib/python3.14/site-packages/pyspark/errors/exceptions/captured.py:263: in deco\n    return f(*a, **kw)\n           ^^^^^^^^^^^\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nanswer = &amp;#x27;xro10586&amp;#x27;\ngateway_client = &amp;lt;py4j.clientserver.JavaClient object at 0x10ca1c6e0&amp;gt;\ntarget_id = &amp;#x27;o10585&amp;#x27;, name = &amp;#x27;count&amp;#x27;\n\n    def get_return_value(answer, gateway_client, target_id=None, name=None):\n        &amp;quot;&amp;quot;&amp;quot;Converts an answer received from the Java gateway into a Python object.\n    \n        For example, string representation of integers are converted to Python\n        integer, string representation of objects are converted to JavaObject\n        instances, etc.\n    \n        :param answer: the string returned by the Java gateway\n        :param gateway_client: the gateway client used to communicate with the Java\n            Gateway. Only necessary if the answer is a reference (e.g., object,\n            list, map)\n        :param target_id: the name of the object from which the answer comes from\n            (e.g., *object1* in `object1.hello()`). Optional.\n        :param name: the name of the member from which the answer comes from\n            (e.g., *hello* in `object1.hello()`). Optional.\n        &amp;quot;&amp;quot;&amp;quot;\n        if is_error(answer)[0]:\n            if len(answer) &amp;gt; 1:\n                type = answer[1]\n                value = OUTPUT_CONVERTER[type](answer[2:], gateway_client)\n                if answer[1] == REFERENCE_TYPE:\n&amp;gt;                   raise Py4JJavaError(\n                        &amp;quot;An error occurred while calling {0}{1}{2}.\\n&amp;quot;.\n                        format(target_id, &amp;quot;.&amp;quot;, name), value)\nE                   py4j.protocol.Py4JJavaError: An error occurred while calling o10585.count.\nE                   : org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 50.0 failed 1 times, most recent failure: Lost task 0.0 in stage 50.0 (TID 49) (10.20.80.191 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\nE                       check_python_version(infile)\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\nE                       raise PySparkRuntimeError(\nE                   pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nE                   Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\nE                   \nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\nE                   \tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\nE                   \tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\nE                   \tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\nE                   \tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\nE                   \tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\nE                   \tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\nE                   \tat org.apache.spark.scheduler.Task.run(Task.scala:147)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\nE                   \tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\nE                   \tat java.base/java.lang.Thread.run(Thread.java:840)\nE                   \nE                   Driver stacktrace:\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$3(DAGScheduler.scala:3122)\nE                   \tat scala.Option.getOrElse(Option.scala:201)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:3122)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:3114)\nE                   \tat scala.collection.immutable.List.foreach(List.scala:323)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:3114)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1303)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1303)\nE                   \tat scala.Option.foreach(Option.scala:437)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1303)\nE                   \tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3397)\nE                   \tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3328)\nE                   \tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3317)\nE                   \tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:50)\nE                   Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\nE                       check_python_version(infile)\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\nE                       raise PySparkRuntimeError(\nE                   pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nE                   Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\nE                   \nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\nE                   \tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\nE                   \tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\nE                   \tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\nE                   \tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\nE                   \tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\nE                   \tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\nE                   \tat org.apache.spark.scheduler.Task.run(Task.scala:147)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\nE                   \tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\nE                   \tat java.base/java.lang.Thread.run(Thread.java:840)\n\nvenv/lib/python3.14/site-packages/py4j/protocol.py:327: Py4JJavaError\n\n----------------------------- Captured stderr call -----------------------------\n26/01/22 12:17:31 ERROR Executor: Exception in task 0.0 in stage 50.0 (TID 49)\norg.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\n    check_python_version(infile)\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\n    raise PySparkRuntimeError(\npyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nPlease check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n26/01/22 12:17:31 WARN TaskSetManager: Lost task 0.0 in stage 50.0 (TID 49) (10.20.80.191 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\n    check_python_version(infile)\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\n    raise PySparkRuntimeError(\npyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nPlease check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n\n26/01/22 12:17:31 ERROR TaskSetManager: Task 0 in stage 50.0 failed 1 times; aborting job\n&#34;}], &#34;tests/test_roles_transform.py::test_transform_roles_empty_df&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/test_roles_transform.py::test_transform_roles_empty_df&#34;, &#34;duration&#34;: &#34;164 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/test_roles_transform.py::test_transform_roles_empty_df&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;164 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;spark = &amp;lt;pyspark.sql.session.SparkSession object at 0x10ca1de80&amp;gt;\nflat_roles_schema = StructType([StructField(&amp;#x27;id&amp;#x27;, StringType(), True), StructField(&amp;#x27;name&amp;#x27;, StringType(), True), StructField(&amp;#x27;description&amp;#x27;,...ringType(), True), StructField(&amp;#x27;created_at&amp;#x27;, TimestampType(), True), StructField(&amp;#x27;updated_at&amp;#x27;, TimestampType(), True)])\n\n    def test_transform_roles_empty_df(spark, flat_roles_schema):\n        df = spark.createDataFrame([], flat_roles_schema)\n    \n        final_df, error_df = transform_roles(df)\n    \n&amp;gt;       assert final_df.count() == 0\n               ^^^^^^^^^^^^^^^^\n\ntests/test_roles_transform.py:202: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \nvenv/lib/python3.14/site-packages/pyspark/sql/classic/dataframe.py:439: in count\n    return int(self._jdf.count())\n               ^^^^^^^^^^^^^^^^^\nvenv/lib/python3.14/site-packages/py4j/java_gateway.py:1362: in __call__\n    return_value = get_return_value(\nvenv/lib/python3.14/site-packages/pyspark/errors/exceptions/captured.py:263: in deco\n    return f(*a, **kw)\n           ^^^^^^^^^^^\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nanswer = &amp;#x27;xro10751&amp;#x27;\ngateway_client = &amp;lt;py4j.clientserver.JavaClient object at 0x10ca1c6e0&amp;gt;\ntarget_id = &amp;#x27;o10750&amp;#x27;, name = &amp;#x27;count&amp;#x27;\n\n    def get_return_value(answer, gateway_client, target_id=None, name=None):\n        &amp;quot;&amp;quot;&amp;quot;Converts an answer received from the Java gateway into a Python object.\n    \n        For example, string representation of integers are converted to Python\n        integer, string representation of objects are converted to JavaObject\n        instances, etc.\n    \n        :param answer: the string returned by the Java gateway\n        :param gateway_client: the gateway client used to communicate with the Java\n            Gateway. Only necessary if the answer is a reference (e.g., object,\n            list, map)\n        :param target_id: the name of the object from which the answer comes from\n            (e.g., *object1* in `object1.hello()`). Optional.\n        :param name: the name of the member from which the answer comes from\n            (e.g., *hello* in `object1.hello()`). Optional.\n        &amp;quot;&amp;quot;&amp;quot;\n        if is_error(answer)[0]:\n            if len(answer) &amp;gt; 1:\n                type = answer[1]\n                value = OUTPUT_CONVERTER[type](answer[2:], gateway_client)\n                if answer[1] == REFERENCE_TYPE:\n&amp;gt;                   raise Py4JJavaError(\n                        &amp;quot;An error occurred while calling {0}{1}{2}.\\n&amp;quot;.\n                        format(target_id, &amp;quot;.&amp;quot;, name), value)\nE                   py4j.protocol.Py4JJavaError: An error occurred while calling o10750.count.\nE                   : org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 51.0 failed 1 times, most recent failure: Lost task 0.0 in stage 51.0 (TID 50) (10.20.80.191 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\nE                       check_python_version(infile)\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\nE                       raise PySparkRuntimeError(\nE                   pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nE                   Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\nE                   \nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\nE                   \tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\nE                   \tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\nE                   \tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\nE                   \tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\nE                   \tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\nE                   \tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\nE                   \tat org.apache.spark.scheduler.Task.run(Task.scala:147)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\nE                   \tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\nE                   \tat java.base/java.lang.Thread.run(Thread.java:840)\nE                   \nE                   Driver stacktrace:\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$3(DAGScheduler.scala:3122)\nE                   \tat scala.Option.getOrElse(Option.scala:201)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:3122)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:3114)\nE                   \tat scala.collection.immutable.List.foreach(List.scala:323)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:3114)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1303)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1303)\nE                   \tat scala.Option.foreach(Option.scala:437)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1303)\nE                   \tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3397)\nE                   \tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3328)\nE                   \tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3317)\nE                   \tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:50)\nE                   Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\nE                       check_python_version(infile)\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\nE                       raise PySparkRuntimeError(\nE                   pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nE                   Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\nE                   \nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\nE                   \tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\nE                   \tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\nE                   \tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\nE                   \tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\nE                   \tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\nE                   \tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\nE                   \tat org.apache.spark.scheduler.Task.run(Task.scala:147)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\nE                   \tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\nE                   \tat java.base/java.lang.Thread.run(Thread.java:840)\n\nvenv/lib/python3.14/site-packages/py4j/protocol.py:327: Py4JJavaError\n\n----------------------------- Captured stderr call -----------------------------\n26/01/22 12:17:31 ERROR Executor: Exception in task 0.0 in stage 51.0 (TID 50)\norg.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\n    check_python_version(infile)\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\n    raise PySparkRuntimeError(\npyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nPlease check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n26/01/22 12:17:31 WARN TaskSetManager: Lost task 0.0 in stage 51.0 (TID 50) (10.20.80.191 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\n    check_python_version(infile)\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\n    raise PySparkRuntimeError(\npyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nPlease check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n\n26/01/22 12:17:31 ERROR TaskSetManager: Task 0 in stage 51.0 failed 1 times; aborting job\n&#34;}], &#34;tests/test_roles_transform.py::test_raw_record_present_in_error_df&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/test_roles_transform.py::test_raw_record_present_in_error_df&#34;, &#34;duration&#34;: &#34;163 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/test_roles_transform.py::test_raw_record_present_in_error_df&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;163 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;spark = &amp;lt;pyspark.sql.session.SparkSession object at 0x10ca1de80&amp;gt;\nflat_roles_schema = StructType([StructField(&amp;#x27;id&amp;#x27;, StringType(), True), StructField(&amp;#x27;name&amp;#x27;, StringType(), True), StructField(&amp;#x27;description&amp;#x27;,...ringType(), True), StructField(&amp;#x27;created_at&amp;#x27;, TimestampType(), True), StructField(&amp;#x27;updated_at&amp;#x27;, TimestampType(), True)])\n\n    def test_raw_record_present_in_error_df(spark, flat_roles_schema):\n        df = spark.createDataFrame(\n            [(None, None, &amp;quot;Bad role&amp;quot;, False, &amp;quot;support&amp;quot;, None, None)],\n            flat_roles_schema\n        )\n    \n        _, error_df = transform_roles(df)\n    \n&amp;gt;       raw = error_df.collect()[0].raw_record\n              ^^^^^^^^^^^^^^^^^^\n\ntests/test_roles_transform.py:217: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \nvenv/lib/python3.14/site-packages/pyspark/sql/classic/dataframe.py:443: in collect\n    sock_info = self._jdf.collectToPython()\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^\nvenv/lib/python3.14/site-packages/py4j/java_gateway.py:1362: in __call__\n    return_value = get_return_value(\nvenv/lib/python3.14/site-packages/pyspark/errors/exceptions/captured.py:263: in deco\n    return f(*a, **kw)\n           ^^^^^^^^^^^\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nanswer = &amp;#x27;xro10916&amp;#x27;\ngateway_client = &amp;lt;py4j.clientserver.JavaClient object at 0x10ca1c6e0&amp;gt;\ntarget_id = &amp;#x27;o10902&amp;#x27;, name = &amp;#x27;collectToPython&amp;#x27;\n\n    def get_return_value(answer, gateway_client, target_id=None, name=None):\n        &amp;quot;&amp;quot;&amp;quot;Converts an answer received from the Java gateway into a Python object.\n    \n        For example, string representation of integers are converted to Python\n        integer, string representation of objects are converted to JavaObject\n        instances, etc.\n    \n        :param answer: the string returned by the Java gateway\n        :param gateway_client: the gateway client used to communicate with the Java\n            Gateway. Only necessary if the answer is a reference (e.g., object,\n            list, map)\n        :param target_id: the name of the object from which the answer comes from\n            (e.g., *object1* in `object1.hello()`). Optional.\n        :param name: the name of the member from which the answer comes from\n            (e.g., *hello* in `object1.hello()`). Optional.\n        &amp;quot;&amp;quot;&amp;quot;\n        if is_error(answer)[0]:\n            if len(answer) &amp;gt; 1:\n                type = answer[1]\n                value = OUTPUT_CONVERTER[type](answer[2:], gateway_client)\n                if answer[1] == REFERENCE_TYPE:\n&amp;gt;                   raise Py4JJavaError(\n                        &amp;quot;An error occurred while calling {0}{1}{2}.\\n&amp;quot;.\n                        format(target_id, &amp;quot;.&amp;quot;, name), value)\nE                   py4j.protocol.Py4JJavaError: An error occurred while calling o10902.collectToPython.\nE                   : org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 52.0 failed 1 times, most recent failure: Lost task 0.0 in stage 52.0 (TID 51) (10.20.80.191 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\nE                       check_python_version(infile)\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\nE                       raise PySparkRuntimeError(\nE                   pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nE                   Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\nE                   \nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\nE                   \tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\nE                   \tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\nE                   \tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\nE                   \tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\nE                   \tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:402)\nE                   \tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:901)\nE                   \tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:901)\nE                   \tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\nE                   \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\nE                   \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\nE                   \tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\nE                   \tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\nE                   \tat org.apache.spark.scheduler.Task.run(Task.scala:147)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\nE                   \tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\nE                   \tat java.base/java.lang.Thread.run(Thread.java:840)\nE                   \nE                   Driver stacktrace:\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$3(DAGScheduler.scala:3122)\nE                   \tat scala.Option.getOrElse(Option.scala:201)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:3122)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:3114)\nE                   \tat scala.collection.immutable.List.foreach(List.scala:323)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:3114)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1303)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1303)\nE                   \tat scala.Option.foreach(Option.scala:437)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1303)\nE                   \tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3397)\nE                   \tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3328)\nE                   \tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3317)\nE                   \tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:50)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1017)\nE                   \tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2496)\nE                   \tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2517)\nE                   \tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2536)\nE                   \tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2561)\nE                   \tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1057)\nE                   \tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\nE                   \tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\nE                   \tat org.apache.spark.rdd.RDD.withScope(RDD.scala:417)\nE                   \tat org.apache.spark.rdd.RDD.collect(RDD.scala:1056)\nE                   \tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:462)\nE                   \tat org.apache.spark.sql.classic.Dataset.$anonfun$collectToPython$1(Dataset.scala:2085)\nE                   \tat org.apache.spark.sql.classic.Dataset.$anonfun$withAction$2(Dataset.scala:2265)\nE                   \tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:717)\nE                   \tat org.apache.spark.sql.classic.Dataset.$anonfun$withAction$1(Dataset.scala:2263)\nE                   \tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:177)\nE                   \tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:285)\nE                   \tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:139)\nE                   \tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\nE                   \tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)\nE                   \tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)\nE                   \tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)\nE                   \tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:139)\nE                   \tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:308)\nE                   \tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:138)\nE                   \tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\nE                   \tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:92)\nE                   \tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:250)\nE                   \tat org.apache.spark.sql.classic.Dataset.withAction(Dataset.scala:2263)\nE                   \tat org.apache.spark.sql.classic.Dataset.collectToPython(Dataset.scala:2081)\nE                   \tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nE                   \tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\nE                   \tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\nE                   \tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\nE                   \tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\nE                   \tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\nE                   \tat py4j.Gateway.invoke(Gateway.java:282)\nE                   \tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\nE                   \tat py4j.commands.CallCommand.execute(CallCommand.java:79)\nE                   \tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\nE                   \tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\nE                   \tat java.base/java.lang.Thread.run(Thread.java:840)\nE                   Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\nE                       check_python_version(infile)\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\nE                       raise PySparkRuntimeError(\nE                   pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nE                   Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\nE                   \nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\nE                   \tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\nE                   \tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\nE                   \tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\nE                   \tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\nE                   \tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:402)\nE                   \tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:901)\nE                   \tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:901)\nE                   \tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\nE                   \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\nE                   \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\nE                   \tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\nE                   \tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\nE                   \tat org.apache.spark.scheduler.Task.run(Task.scala:147)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\nE                   \tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\nE                   \t... 1 more\n\nvenv/lib/python3.14/site-packages/py4j/protocol.py:327: Py4JJavaError\n\n----------------------------- Captured stderr call -----------------------------\n26/01/22 12:17:32 ERROR Executor: Exception in task 0.0 in stage 52.0 (TID 51)\norg.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\n    check_python_version(infile)\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\n    raise PySparkRuntimeError(\npyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nPlease check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:402)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:901)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:901)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n26/01/22 12:17:32 WARN TaskSetManager: Lost task 0.0 in stage 52.0 (TID 51) (10.20.80.191 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\n    check_python_version(infile)\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\n    raise PySparkRuntimeError(\npyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nPlease check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:402)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:901)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:901)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n\n26/01/22 12:17:32 ERROR TaskSetManager: Task 0 in stage 52.0 failed 1 times; aborting job\n&#34;}], &#34;tests/test_transactions_transform.py::test_api_rate_used&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/test_transactions_transform.py::test_api_rate_used&#34;, &#34;duration&#34;: &#34;368 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/test_transactions_transform.py::test_api_rate_used&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;368 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;mock_get_rate = &amp;lt;MagicMock name=&amp;#x27;get_rate_to_usd&amp;#x27; id=&amp;#x27;4512665280&amp;#x27;&amp;gt;\nspark = &amp;lt;pyspark.sql.session.SparkSession object at 0x10ca1de80&amp;gt;\n\n    @patch(&amp;quot;src.pipelines.transactions.transactions_transform.get_rate_to_usd&amp;quot;)\n    def test_api_rate_used(mock_get_rate, spark):\n        mock_get_rate.return_value = 0.5\n    \n        df = spark.createDataFrame([{\n            &amp;quot;results&amp;quot;: [{\n                &amp;quot;id&amp;quot;: &amp;quot;t1&amp;quot;,\n                &amp;quot;uuid&amp;quot;: &amp;quot;u1&amp;quot;,\n                &amp;quot;occurredTime&amp;quot;: &amp;quot;2024-01-01T10:00:00&amp;quot;,\n                &amp;quot;updatedTime&amp;quot;: &amp;quot;2024-01-01T11:00:00&amp;quot;,\n                &amp;quot;userId&amp;quot;: &amp;quot;user1&amp;quot;,\n                &amp;quot;userUuid&amp;quot;: &amp;quot;uu1&amp;quot;,\n                &amp;quot;userName&amp;quot;: &amp;quot;API User&amp;quot;,\n                &amp;quot;merchantName&amp;quot;: &amp;quot;Amazon&amp;quot;,\n                &amp;quot;rawMerchantName&amp;quot;: &amp;quot;AMZN&amp;quot;,\n                &amp;quot;cardId&amp;quot;: &amp;quot;c1&amp;quot;,\n                &amp;quot;cardUuid&amp;quot;: &amp;quot;cu1&amp;quot;,\n                &amp;quot;budgetId&amp;quot;: &amp;quot;b1&amp;quot;,\n                &amp;quot;budgetUuid&amp;quot;: &amp;quot;bu1&amp;quot;,\n                &amp;quot;currencyData&amp;quot;: {\n                    &amp;quot;originalCurrencyAmount&amp;quot;: 1000,\n                    &amp;quot;exponent&amp;quot;: 2,\n                    &amp;quot;originalCurrencyCode&amp;quot;: &amp;quot;INR&amp;quot;,\n                    &amp;quot;exchangeRate&amp;quot;: None\n                }\n            }]\n        }], schema=raw_schema())\n    \n&amp;gt;       valid_df, error_df, state_df = transform_transactions(df, &amp;quot;day1&amp;quot;)\n                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\ntests/test_transactions_transform.py:112: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \nsrc/pipelines/transactions/transactions_transform.py:143: in transform_transactions\n    .collect()\n     ^^^^^^^^^\nvenv/lib/python3.14/site-packages/pyspark/sql/classic/dataframe.py:443: in collect\n    sock_info = self._jdf.collectToPython()\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^\nvenv/lib/python3.14/site-packages/py4j/java_gateway.py:1362: in __call__\n    return_value = get_return_value(\nvenv/lib/python3.14/site-packages/pyspark/errors/exceptions/captured.py:263: in deco\n    return f(*a, **kw)\n           ^^^^^^^^^^^\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nanswer = &amp;#x27;xro11302&amp;#x27;\ngateway_client = &amp;lt;py4j.clientserver.JavaClient object at 0x10ca1c6e0&amp;gt;\ntarget_id = &amp;#x27;o11301&amp;#x27;, name = &amp;#x27;collectToPython&amp;#x27;\n\n    def get_return_value(answer, gateway_client, target_id=None, name=None):\n        &amp;quot;&amp;quot;&amp;quot;Converts an answer received from the Java gateway into a Python object.\n    \n        For example, string representation of integers are converted to Python\n        integer, string representation of objects are converted to JavaObject\n        instances, etc.\n    \n        :param answer: the string returned by the Java gateway\n        :param gateway_client: the gateway client used to communicate with the Java\n            Gateway. Only necessary if the answer is a reference (e.g., object,\n            list, map)\n        :param target_id: the name of the object from which the answer comes from\n            (e.g., *object1* in `object1.hello()`). Optional.\n        :param name: the name of the member from which the answer comes from\n            (e.g., *hello* in `object1.hello()`). Optional.\n        &amp;quot;&amp;quot;&amp;quot;\n        if is_error(answer)[0]:\n            if len(answer) &amp;gt; 1:\n                type = answer[1]\n                value = OUTPUT_CONVERTER[type](answer[2:], gateway_client)\n                if answer[1] == REFERENCE_TYPE:\n&amp;gt;                   raise Py4JJavaError(\n                        &amp;quot;An error occurred while calling {0}{1}{2}.\\n&amp;quot;.\n                        format(target_id, &amp;quot;.&amp;quot;, name), value)\nE                   py4j.protocol.Py4JJavaError: An error occurred while calling o11301.collectToPython.\nE                   : org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 53.0 failed 1 times, most recent failure: Lost task 0.0 in stage 53.0 (TID 52) (10.20.80.191 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\nE                       check_python_version(infile)\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\nE                       raise PySparkRuntimeError(\nE                   pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nE                   Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\nE                   \nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\nE                   \tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\nE                   \tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.sort_addToSorter_0$(Unknown Source)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\nE                   \tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\nE                   \tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\nE                   \tat org.apache.spark.sql.execution.aggregate.SortAggregateExec.$anonfun$doExecute$1(SortAggregateExec.scala:63)\nE                   \tat org.apache.spark.sql.execution.aggregate.SortAggregateExec.$anonfun$doExecute$1$adapted(SortAggregateExec.scala:60)\nE                   \tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:888)\nE                   \tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:888)\nE                   \tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\nE                   \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\nE                   \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\nE                   \tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\nE                   \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\nE                   \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\nE                   \tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\nE                   \tat org.apache.spark.scheduler.Task.run(Task.scala:147)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\nE                   \tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\nE                   \tat java.base/java.lang.Thread.run(Thread.java:840)\nE                   \nE                   Driver stacktrace:\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$3(DAGScheduler.scala:3122)\nE                   \tat scala.Option.getOrElse(Option.scala:201)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:3122)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:3114)\nE                   \tat scala.collection.immutable.List.foreach(List.scala:323)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:3114)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1303)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1303)\nE                   \tat scala.Option.foreach(Option.scala:437)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1303)\nE                   \tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3397)\nE                   \tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3328)\nE                   \tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3317)\nE                   \tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:50)\nE                   Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\nE                       check_python_version(infile)\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\nE                       raise PySparkRuntimeError(\nE                   pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nE                   Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\nE                   \nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\nE                   \tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\nE                   \tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.sort_addToSorter_0$(Unknown Source)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\nE                   \tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\nE                   \tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\nE                   \tat org.apache.spark.sql.execution.aggregate.SortAggregateExec.$anonfun$doExecute$1(SortAggregateExec.scala:63)\nE                   \tat org.apache.spark.sql.execution.aggregate.SortAggregateExec.$anonfun$doExecute$1$adapted(SortAggregateExec.scala:60)\nE                   \tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:888)\nE                   \tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:888)\nE                   \tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\nE                   \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\nE                   \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\nE                   \tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\nE                   \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\nE                   \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\nE                   \tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\nE                   \tat org.apache.spark.scheduler.Task.run(Task.scala:147)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\nE                   \tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\nE                   \tat java.base/java.lang.Thread.run(Thread.java:840)\n\nvenv/lib/python3.14/site-packages/py4j/protocol.py:327: Py4JJavaError\n\n---------------------------- Captured stderr setup -----------------------------\n26/01/22 12:17:32 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n\n----------------------------- Captured stdout call -----------------------------\n=== Starting Transactions Transform for day1 ===\n\n----------------------------- Captured stderr call -----------------------------\n26/01/22 12:17:32 ERROR Executor: Exception in task 0.0 in stage 53.0 (TID 52)\norg.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\n    check_python_version(infile)\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\n    raise PySparkRuntimeError(\npyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nPlease check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.sort_addToSorter_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n\tat org.apache.spark.sql.execution.aggregate.SortAggregateExec.$anonfun$doExecute$1(SortAggregateExec.scala:63)\n\tat org.apache.spark.sql.execution.aggregate.SortAggregateExec.$anonfun$doExecute$1$adapted(SortAggregateExec.scala:60)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:888)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:888)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n26/01/22 12:17:32 WARN TaskSetManager: Lost task 0.0 in stage 53.0 (TID 52) (10.20.80.191 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\n    check_python_version(infile)\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\n    raise PySparkRuntimeError(\npyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nPlease check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.sort_addToSorter_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n\tat org.apache.spark.sql.execution.aggregate.SortAggregateExec.$anonfun$doExecute$1(SortAggregateExec.scala:63)\n\tat org.apache.spark.sql.execution.aggregate.SortAggregateExec.$anonfun$doExecute$1$adapted(SortAggregateExec.scala:60)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:888)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:888)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n\n26/01/22 12:17:32 ERROR TaskSetManager: Task 0 in stage 53.0 failed 1 times; aborting job\n&#34;}], &#34;tests/test_transactions_transform.py::test_payload_fallback_used&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/test_transactions_transform.py::test_payload_fallback_used&#34;, &#34;duration&#34;: &#34;401 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/test_transactions_transform.py::test_payload_fallback_used&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;401 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;mock_get_rate = &amp;lt;MagicMock name=&amp;#x27;get_rate_to_usd&amp;#x27; id=&amp;#x27;4512663264&amp;#x27;&amp;gt;\nspark = &amp;lt;pyspark.sql.session.SparkSession object at 0x10ca1de80&amp;gt;\n\n    @patch(&amp;quot;src.pipelines.transactions.transactions_transform.get_rate_to_usd&amp;quot;)\n    def test_payload_fallback_used(mock_get_rate, spark):\n        mock_get_rate.return_value = None\n    \n        df = spark.createDataFrame([{\n            &amp;quot;results&amp;quot;: [{\n                &amp;quot;id&amp;quot;: &amp;quot;t2&amp;quot;,\n                &amp;quot;uuid&amp;quot;: &amp;quot;u2&amp;quot;,\n                &amp;quot;occurredTime&amp;quot;: &amp;quot;2024-01-02T10:00:00&amp;quot;,\n                &amp;quot;updatedTime&amp;quot;: &amp;quot;2024-01-02T11:00:00&amp;quot;,\n                &amp;quot;userId&amp;quot;: &amp;quot;user2&amp;quot;,\n                &amp;quot;userUuid&amp;quot;: &amp;quot;uu2&amp;quot;,\n                &amp;quot;userName&amp;quot;: &amp;quot;Payload User&amp;quot;,\n                &amp;quot;merchantName&amp;quot;: &amp;quot;Flipkart&amp;quot;,\n                &amp;quot;rawMerchantName&amp;quot;: &amp;quot;FLPK&amp;quot;,\n                &amp;quot;cardId&amp;quot;: &amp;quot;c2&amp;quot;,\n                &amp;quot;cardUuid&amp;quot;: &amp;quot;cu2&amp;quot;,\n                &amp;quot;budgetId&amp;quot;: &amp;quot;b2&amp;quot;,\n                &amp;quot;budgetUuid&amp;quot;: &amp;quot;bu2&amp;quot;,\n                &amp;quot;currencyData&amp;quot;: {\n                    &amp;quot;originalCurrencyAmount&amp;quot;: 2000,\n                    &amp;quot;exponent&amp;quot;: 2,\n                    &amp;quot;originalCurrencyCode&amp;quot;: &amp;quot;EUR&amp;quot;,\n                    &amp;quot;exchangeRate&amp;quot;: 2.0\n                }\n            }]\n        }], schema=raw_schema())\n    \n&amp;gt;       valid_df, error_df, _ = transform_transactions(df, &amp;quot;day1&amp;quot;)\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\ntests/test_transactions_transform.py:156: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \nsrc/pipelines/transactions/transactions_transform.py:143: in transform_transactions\n    .collect()\n     ^^^^^^^^^\nvenv/lib/python3.14/site-packages/pyspark/sql/classic/dataframe.py:443: in collect\n    sock_info = self._jdf.collectToPython()\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^\nvenv/lib/python3.14/site-packages/py4j/java_gateway.py:1362: in __call__\n    return_value = get_return_value(\nvenv/lib/python3.14/site-packages/pyspark/errors/exceptions/captured.py:263: in deco\n    return f(*a, **kw)\n           ^^^^^^^^^^^\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nanswer = &amp;#x27;xro11692&amp;#x27;\ngateway_client = &amp;lt;py4j.clientserver.JavaClient object at 0x10ca1c6e0&amp;gt;\ntarget_id = &amp;#x27;o11691&amp;#x27;, name = &amp;#x27;collectToPython&amp;#x27;\n\n    def get_return_value(answer, gateway_client, target_id=None, name=None):\n        &amp;quot;&amp;quot;&amp;quot;Converts an answer received from the Java gateway into a Python object.\n    \n        For example, string representation of integers are converted to Python\n        integer, string representation of objects are converted to JavaObject\n        instances, etc.\n    \n        :param answer: the string returned by the Java gateway\n        :param gateway_client: the gateway client used to communicate with the Java\n            Gateway. Only necessary if the answer is a reference (e.g., object,\n            list, map)\n        :param target_id: the name of the object from which the answer comes from\n            (e.g., *object1* in `object1.hello()`). Optional.\n        :param name: the name of the member from which the answer comes from\n            (e.g., *hello* in `object1.hello()`). Optional.\n        &amp;quot;&amp;quot;&amp;quot;\n        if is_error(answer)[0]:\n            if len(answer) &amp;gt; 1:\n                type = answer[1]\n                value = OUTPUT_CONVERTER[type](answer[2:], gateway_client)\n                if answer[1] == REFERENCE_TYPE:\n&amp;gt;                   raise Py4JJavaError(\n                        &amp;quot;An error occurred while calling {0}{1}{2}.\\n&amp;quot;.\n                        format(target_id, &amp;quot;.&amp;quot;, name), value)\nE                   py4j.protocol.Py4JJavaError: An error occurred while calling o11691.collectToPython.\nE                   : org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 54.0 failed 1 times, most recent failure: Lost task 0.0 in stage 54.0 (TID 53) (10.20.80.191 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\nE                       check_python_version(infile)\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\nE                       raise PySparkRuntimeError(\nE                   pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nE                   Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\nE                   \nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\nE                   \tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\nE                   \tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.sort_addToSorter_0$(Unknown Source)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\nE                   \tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\nE                   \tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\nE                   \tat org.apache.spark.sql.execution.aggregate.SortAggregateExec.$anonfun$doExecute$1(SortAggregateExec.scala:63)\nE                   \tat org.apache.spark.sql.execution.aggregate.SortAggregateExec.$anonfun$doExecute$1$adapted(SortAggregateExec.scala:60)\nE                   \tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:888)\nE                   \tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:888)\nE                   \tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\nE                   \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\nE                   \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\nE                   \tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\nE                   \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\nE                   \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\nE                   \tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\nE                   \tat org.apache.spark.scheduler.Task.run(Task.scala:147)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\nE                   \tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\nE                   \tat java.base/java.lang.Thread.run(Thread.java:840)\nE                   \nE                   Driver stacktrace:\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$3(DAGScheduler.scala:3122)\nE                   \tat scala.Option.getOrElse(Option.scala:201)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:3122)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:3114)\nE                   \tat scala.collection.immutable.List.foreach(List.scala:323)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:3114)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1303)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1303)\nE                   \tat scala.Option.foreach(Option.scala:437)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1303)\nE                   \tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3397)\nE                   \tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3328)\nE                   \tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3317)\nE                   \tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:50)\nE                   Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\nE                       check_python_version(infile)\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\nE                       raise PySparkRuntimeError(\nE                   pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nE                   Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\nE                   \nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\nE                   \tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\nE                   \tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.sort_addToSorter_0$(Unknown Source)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\nE                   \tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\nE                   \tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\nE                   \tat org.apache.spark.sql.execution.aggregate.SortAggregateExec.$anonfun$doExecute$1(SortAggregateExec.scala:63)\nE                   \tat org.apache.spark.sql.execution.aggregate.SortAggregateExec.$anonfun$doExecute$1$adapted(SortAggregateExec.scala:60)\nE                   \tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:888)\nE                   \tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:888)\nE                   \tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\nE                   \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\nE                   \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\nE                   \tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\nE                   \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\nE                   \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\nE                   \tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\nE                   \tat org.apache.spark.scheduler.Task.run(Task.scala:147)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\nE                   \tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\nE                   \tat java.base/java.lang.Thread.run(Thread.java:840)\n\nvenv/lib/python3.14/site-packages/py4j/protocol.py:327: Py4JJavaError\n\n----------------------------- Captured stdout call -----------------------------\n=== Starting Transactions Transform for day1 ===\n\n----------------------------- Captured stderr call -----------------------------\n26/01/22 12:17:33 ERROR Executor: Exception in task 0.0 in stage 54.0 (TID 53)\norg.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\n    check_python_version(infile)\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\n    raise PySparkRuntimeError(\npyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nPlease check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.sort_addToSorter_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n\tat org.apache.spark.sql.execution.aggregate.SortAggregateExec.$anonfun$doExecute$1(SortAggregateExec.scala:63)\n\tat org.apache.spark.sql.execution.aggregate.SortAggregateExec.$anonfun$doExecute$1$adapted(SortAggregateExec.scala:60)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:888)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:888)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n26/01/22 12:17:33 WARN TaskSetManager: Lost task 0.0 in stage 54.0 (TID 53) (10.20.80.191 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\n    check_python_version(infile)\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\n    raise PySparkRuntimeError(\npyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nPlease check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.sort_addToSorter_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n\tat org.apache.spark.sql.execution.aggregate.SortAggregateExec.$anonfun$doExecute$1(SortAggregateExec.scala:63)\n\tat org.apache.spark.sql.execution.aggregate.SortAggregateExec.$anonfun$doExecute$1$adapted(SortAggregateExec.scala:60)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:888)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:888)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n\n26/01/22 12:17:33 ERROR TaskSetManager: Task 0 in stage 54.0 failed 1 times; aborting job\n&#34;}], &#34;tests/test_transactions_transform.py::test_no_fx_rate_available&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/test_transactions_transform.py::test_no_fx_rate_available&#34;, &#34;duration&#34;: &#34;300 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/test_transactions_transform.py::test_no_fx_rate_available&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;300 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;mock_get_rate = &amp;lt;MagicMock name=&amp;#x27;get_rate_to_usd&amp;#x27; id=&amp;#x27;4512662256&amp;#x27;&amp;gt;\nspark = &amp;lt;pyspark.sql.session.SparkSession object at 0x10ca1de80&amp;gt;\n\n    @patch(&amp;quot;src.pipelines.transactions.transactions_transform.get_rate_to_usd&amp;quot;)\n    def test_no_fx_rate_available(mock_get_rate, spark):\n        mock_get_rate.return_value = None\n    \n        df = spark.createDataFrame([{\n            &amp;quot;results&amp;quot;: [{\n                &amp;quot;id&amp;quot;: &amp;quot;t3&amp;quot;,\n                &amp;quot;uuid&amp;quot;: &amp;quot;u3&amp;quot;,\n                &amp;quot;occurredTime&amp;quot;: &amp;quot;2024-01-03T10:00:00&amp;quot;,\n                &amp;quot;updatedTime&amp;quot;: &amp;quot;2024-01-03T11:00:00&amp;quot;,\n                &amp;quot;userId&amp;quot;: &amp;quot;user3&amp;quot;,\n                &amp;quot;userUuid&amp;quot;: &amp;quot;uu3&amp;quot;,\n                &amp;quot;userName&amp;quot;: &amp;quot;NoRate&amp;quot;,\n                &amp;quot;merchantName&amp;quot;: &amp;quot;Unknown&amp;quot;,\n                &amp;quot;rawMerchantName&amp;quot;: &amp;quot;UNK&amp;quot;,\n                &amp;quot;cardId&amp;quot;: &amp;quot;c3&amp;quot;,\n                &amp;quot;cardUuid&amp;quot;: &amp;quot;cu3&amp;quot;,\n                &amp;quot;budgetId&amp;quot;: &amp;quot;b3&amp;quot;,\n                &amp;quot;budgetUuid&amp;quot;: &amp;quot;bu3&amp;quot;,\n                &amp;quot;currencyData&amp;quot;: {\n                    &amp;quot;originalCurrencyAmount&amp;quot;: 3000,\n                    &amp;quot;exponent&amp;quot;: 2,\n                    &amp;quot;originalCurrencyCode&amp;quot;: &amp;quot;JPY&amp;quot;,\n                    &amp;quot;exchangeRate&amp;quot;: None\n                }\n            }]\n        }], schema=raw_schema())\n    \n&amp;gt;       valid_df, error_df, _ = transform_transactions(df, &amp;quot;day1&amp;quot;)\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\ntests/test_transactions_transform.py:195: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \nsrc/pipelines/transactions/transactions_transform.py:143: in transform_transactions\n    .collect()\n     ^^^^^^^^^\nvenv/lib/python3.14/site-packages/pyspark/sql/classic/dataframe.py:443: in collect\n    sock_info = self._jdf.collectToPython()\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^\nvenv/lib/python3.14/site-packages/py4j/java_gateway.py:1362: in __call__\n    return_value = get_return_value(\nvenv/lib/python3.14/site-packages/pyspark/errors/exceptions/captured.py:263: in deco\n    return f(*a, **kw)\n           ^^^^^^^^^^^\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nanswer = &amp;#x27;xro12082&amp;#x27;\ngateway_client = &amp;lt;py4j.clientserver.JavaClient object at 0x10ca1c6e0&amp;gt;\ntarget_id = &amp;#x27;o12081&amp;#x27;, name = &amp;#x27;collectToPython&amp;#x27;\n\n    def get_return_value(answer, gateway_client, target_id=None, name=None):\n        &amp;quot;&amp;quot;&amp;quot;Converts an answer received from the Java gateway into a Python object.\n    \n        For example, string representation of integers are converted to Python\n        integer, string representation of objects are converted to JavaObject\n        instances, etc.\n    \n        :param answer: the string returned by the Java gateway\n        :param gateway_client: the gateway client used to communicate with the Java\n            Gateway. Only necessary if the answer is a reference (e.g., object,\n            list, map)\n        :param target_id: the name of the object from which the answer comes from\n            (e.g., *object1* in `object1.hello()`). Optional.\n        :param name: the name of the member from which the answer comes from\n            (e.g., *hello* in `object1.hello()`). Optional.\n        &amp;quot;&amp;quot;&amp;quot;\n        if is_error(answer)[0]:\n            if len(answer) &amp;gt; 1:\n                type = answer[1]\n                value = OUTPUT_CONVERTER[type](answer[2:], gateway_client)\n                if answer[1] == REFERENCE_TYPE:\n&amp;gt;                   raise Py4JJavaError(\n                        &amp;quot;An error occurred while calling {0}{1}{2}.\\n&amp;quot;.\n                        format(target_id, &amp;quot;.&amp;quot;, name), value)\nE                   py4j.protocol.Py4JJavaError: An error occurred while calling o12081.collectToPython.\nE                   : org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 55.0 failed 1 times, most recent failure: Lost task 0.0 in stage 55.0 (TID 54) (10.20.80.191 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\nE                       check_python_version(infile)\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\nE                       raise PySparkRuntimeError(\nE                   pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nE                   Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\nE                   \nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\nE                   \tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\nE                   \tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.sort_addToSorter_0$(Unknown Source)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\nE                   \tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\nE                   \tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\nE                   \tat org.apache.spark.sql.execution.aggregate.SortAggregateExec.$anonfun$doExecute$1(SortAggregateExec.scala:63)\nE                   \tat org.apache.spark.sql.execution.aggregate.SortAggregateExec.$anonfun$doExecute$1$adapted(SortAggregateExec.scala:60)\nE                   \tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:888)\nE                   \tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:888)\nE                   \tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\nE                   \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\nE                   \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\nE                   \tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\nE                   \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\nE                   \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\nE                   \tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\nE                   \tat org.apache.spark.scheduler.Task.run(Task.scala:147)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\nE                   \tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\nE                   \tat java.base/java.lang.Thread.run(Thread.java:840)\nE                   \nE                   Driver stacktrace:\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$3(DAGScheduler.scala:3122)\nE                   \tat scala.Option.getOrElse(Option.scala:201)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:3122)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:3114)\nE                   \tat scala.collection.immutable.List.foreach(List.scala:323)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:3114)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1303)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1303)\nE                   \tat scala.Option.foreach(Option.scala:437)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1303)\nE                   \tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3397)\nE                   \tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3328)\nE                   \tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3317)\nE                   \tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:50)\nE                   Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\nE                       check_python_version(infile)\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\nE                       raise PySparkRuntimeError(\nE                   pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nE                   Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\nE                   \nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\nE                   \tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\nE                   \tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.sort_addToSorter_0$(Unknown Source)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\nE                   \tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\nE                   \tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\nE                   \tat org.apache.spark.sql.execution.aggregate.SortAggregateExec.$anonfun$doExecute$1(SortAggregateExec.scala:63)\nE                   \tat org.apache.spark.sql.execution.aggregate.SortAggregateExec.$anonfun$doExecute$1$adapted(SortAggregateExec.scala:60)\nE                   \tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:888)\nE                   \tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:888)\nE                   \tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\nE                   \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\nE                   \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\nE                   \tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\nE                   \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\nE                   \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\nE                   \tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\nE                   \tat org.apache.spark.scheduler.Task.run(Task.scala:147)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\nE                   \tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\nE                   \tat java.base/java.lang.Thread.run(Thread.java:840)\n\nvenv/lib/python3.14/site-packages/py4j/protocol.py:327: Py4JJavaError\n\n----------------------------- Captured stdout call -----------------------------\n=== Starting Transactions Transform for day1 ===\n\n----------------------------- Captured stderr call -----------------------------\n26/01/22 12:17:33 ERROR Executor: Exception in task 0.0 in stage 55.0 (TID 54)\norg.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\n    check_python_version(infile)\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\n    raise PySparkRuntimeError(\npyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nPlease check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.sort_addToSorter_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n\tat org.apache.spark.sql.execution.aggregate.SortAggregateExec.$anonfun$doExecute$1(SortAggregateExec.scala:63)\n\tat org.apache.spark.sql.execution.aggregate.SortAggregateExec.$anonfun$doExecute$1$adapted(SortAggregateExec.scala:60)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:888)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:888)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n26/01/22 12:17:33 WARN TaskSetManager: Lost task 0.0 in stage 55.0 (TID 54) (10.20.80.191 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\n    check_python_version(infile)\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\n    raise PySparkRuntimeError(\npyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nPlease check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.sort_addToSorter_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n\tat org.apache.spark.sql.execution.aggregate.SortAggregateExec.$anonfun$doExecute$1(SortAggregateExec.scala:63)\n\tat org.apache.spark.sql.execution.aggregate.SortAggregateExec.$anonfun$doExecute$1$adapted(SortAggregateExec.scala:60)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:888)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:888)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n\n26/01/22 12:17:33 ERROR TaskSetManager: Task 0 in stage 55.0 failed 1 times; aborting job\n&#34;}], &#34;tests/test_transactions_transform.py::test_deduplication&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/test_transactions_transform.py::test_deduplication&#34;, &#34;duration&#34;: &#34;266 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/test_transactions_transform.py::test_deduplication&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;266 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;mock_get_rate = &amp;lt;MagicMock name=&amp;#x27;get_rate_to_usd&amp;#x27; id=&amp;#x27;4512667296&amp;#x27;&amp;gt;\nspark = &amp;lt;pyspark.sql.session.SparkSession object at 0x10ca1de80&amp;gt;\n\n    @patch(&amp;quot;src.pipelines.transactions.transactions_transform.get_rate_to_usd&amp;quot;)\n    def test_deduplication(mock_get_rate, spark):\n        mock_get_rate.return_value = 1.0\n    \n        df = spark.createDataFrame([{\n            &amp;quot;results&amp;quot;: [{\n                &amp;quot;id&amp;quot;: &amp;quot;t4&amp;quot;,\n                &amp;quot;uuid&amp;quot;: &amp;quot;u4&amp;quot;,\n                &amp;quot;occurredTime&amp;quot;: &amp;quot;2024-01-04T10:00:00&amp;quot;,\n                &amp;quot;updatedTime&amp;quot;: &amp;quot;2024-01-04T11:00:00&amp;quot;,\n                &amp;quot;userId&amp;quot;: &amp;quot;user4&amp;quot;,\n                &amp;quot;userUuid&amp;quot;: &amp;quot;uu4&amp;quot;,\n                &amp;quot;userName&amp;quot;: &amp;quot;Dup&amp;quot;,\n                &amp;quot;merchantName&amp;quot;: &amp;quot;Zomato&amp;quot;,\n                &amp;quot;rawMerchantName&amp;quot;: &amp;quot;ZMT&amp;quot;,\n                &amp;quot;cardId&amp;quot;: &amp;quot;c4&amp;quot;,\n                &amp;quot;cardUuid&amp;quot;: &amp;quot;cu4&amp;quot;,\n                &amp;quot;budgetId&amp;quot;: &amp;quot;b4&amp;quot;,\n                &amp;quot;budgetUuid&amp;quot;: &amp;quot;bu4&amp;quot;,\n                &amp;quot;currencyData&amp;quot;: {\n                    &amp;quot;originalCurrencyAmount&amp;quot;: 4000,\n                    &amp;quot;exponent&amp;quot;: 2,\n                    &amp;quot;originalCurrencyCode&amp;quot;: &amp;quot;USD&amp;quot;,\n                    &amp;quot;exchangeRate&amp;quot;: 1.0\n                }\n            }, {\n                &amp;quot;id&amp;quot;: &amp;quot;t4&amp;quot;,\n                &amp;quot;uuid&amp;quot;: &amp;quot;u4&amp;quot;,\n                &amp;quot;occurredTime&amp;quot;: &amp;quot;2024-01-04T10:00:00&amp;quot;,\n                &amp;quot;updatedTime&amp;quot;: &amp;quot;2024-01-04T11:00:00&amp;quot;,\n                &amp;quot;userId&amp;quot;: &amp;quot;user4&amp;quot;,\n                &amp;quot;userUuid&amp;quot;: &amp;quot;uu4&amp;quot;,\n                &amp;quot;userName&amp;quot;: &amp;quot;Dup&amp;quot;,\n                &amp;quot;merchantName&amp;quot;: &amp;quot;Zomato&amp;quot;,\n                &amp;quot;rawMerchantName&amp;quot;: &amp;quot;ZMT&amp;quot;,\n                &amp;quot;cardId&amp;quot;: &amp;quot;c4&amp;quot;,\n                &amp;quot;cardUuid&amp;quot;: &amp;quot;cu4&amp;quot;,\n                &amp;quot;budgetId&amp;quot;: &amp;quot;b4&amp;quot;,\n                &amp;quot;budgetUuid&amp;quot;: &amp;quot;bu4&amp;quot;,\n                &amp;quot;currencyData&amp;quot;: {\n                    &amp;quot;originalCurrencyAmount&amp;quot;: 4000,\n                    &amp;quot;exponent&amp;quot;: 2,\n                    &amp;quot;originalCurrencyCode&amp;quot;: &amp;quot;USD&amp;quot;,\n                    &amp;quot;exchangeRate&amp;quot;: 1.0\n                }\n            }]\n        }], schema=raw_schema())\n    \n&amp;gt;       valid_df, error_df, _ = transform_transactions(df, &amp;quot;day1&amp;quot;)\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\ntests/test_transactions_transform.py:251: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \nsrc/pipelines/transactions/transactions_transform.py:143: in transform_transactions\n    .collect()\n     ^^^^^^^^^\nvenv/lib/python3.14/site-packages/pyspark/sql/classic/dataframe.py:443: in collect\n    sock_info = self._jdf.collectToPython()\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^\nvenv/lib/python3.14/site-packages/py4j/java_gateway.py:1362: in __call__\n    return_value = get_return_value(\nvenv/lib/python3.14/site-packages/pyspark/errors/exceptions/captured.py:263: in deco\n    return f(*a, **kw)\n           ^^^^^^^^^^^\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nanswer = &amp;#x27;xro12472&amp;#x27;\ngateway_client = &amp;lt;py4j.clientserver.JavaClient object at 0x10ca1c6e0&amp;gt;\ntarget_id = &amp;#x27;o12471&amp;#x27;, name = &amp;#x27;collectToPython&amp;#x27;\n\n    def get_return_value(answer, gateway_client, target_id=None, name=None):\n        &amp;quot;&amp;quot;&amp;quot;Converts an answer received from the Java gateway into a Python object.\n    \n        For example, string representation of integers are converted to Python\n        integer, string representation of objects are converted to JavaObject\n        instances, etc.\n    \n        :param answer: the string returned by the Java gateway\n        :param gateway_client: the gateway client used to communicate with the Java\n            Gateway. Only necessary if the answer is a reference (e.g., object,\n            list, map)\n        :param target_id: the name of the object from which the answer comes from\n            (e.g., *object1* in `object1.hello()`). Optional.\n        :param name: the name of the member from which the answer comes from\n            (e.g., *hello* in `object1.hello()`). Optional.\n        &amp;quot;&amp;quot;&amp;quot;\n        if is_error(answer)[0]:\n            if len(answer) &amp;gt; 1:\n                type = answer[1]\n                value = OUTPUT_CONVERTER[type](answer[2:], gateway_client)\n                if answer[1] == REFERENCE_TYPE:\n&amp;gt;                   raise Py4JJavaError(\n                        &amp;quot;An error occurred while calling {0}{1}{2}.\\n&amp;quot;.\n                        format(target_id, &amp;quot;.&amp;quot;, name), value)\nE                   py4j.protocol.Py4JJavaError: An error occurred while calling o12471.collectToPython.\nE                   : org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 56.0 failed 1 times, most recent failure: Lost task 0.0 in stage 56.0 (TID 55) (10.20.80.191 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\nE                       check_python_version(infile)\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\nE                       raise PySparkRuntimeError(\nE                   pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nE                   Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\nE                   \nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\nE                   \tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\nE                   \tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.sort_addToSorter_0$(Unknown Source)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\nE                   \tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\nE                   \tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\nE                   \tat org.apache.spark.sql.execution.aggregate.SortAggregateExec.$anonfun$doExecute$1(SortAggregateExec.scala:63)\nE                   \tat org.apache.spark.sql.execution.aggregate.SortAggregateExec.$anonfun$doExecute$1$adapted(SortAggregateExec.scala:60)\nE                   \tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:888)\nE                   \tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:888)\nE                   \tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\nE                   \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\nE                   \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\nE                   \tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\nE                   \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\nE                   \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\nE                   \tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\nE                   \tat org.apache.spark.scheduler.Task.run(Task.scala:147)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\nE                   \tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\nE                   \tat java.base/java.lang.Thread.run(Thread.java:840)\nE                   \nE                   Driver stacktrace:\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$3(DAGScheduler.scala:3122)\nE                   \tat scala.Option.getOrElse(Option.scala:201)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:3122)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:3114)\nE                   \tat scala.collection.immutable.List.foreach(List.scala:323)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:3114)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1303)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1303)\nE                   \tat scala.Option.foreach(Option.scala:437)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1303)\nE                   \tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3397)\nE                   \tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3328)\nE                   \tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3317)\nE                   \tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:50)\nE                   Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\nE                       check_python_version(infile)\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\nE                       raise PySparkRuntimeError(\nE                   pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nE                   Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\nE                   \nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\nE                   \tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\nE                   \tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.sort_addToSorter_0$(Unknown Source)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\nE                   \tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\nE                   \tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\nE                   \tat org.apache.spark.sql.execution.aggregate.SortAggregateExec.$anonfun$doExecute$1(SortAggregateExec.scala:63)\nE                   \tat org.apache.spark.sql.execution.aggregate.SortAggregateExec.$anonfun$doExecute$1$adapted(SortAggregateExec.scala:60)\nE                   \tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:888)\nE                   \tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:888)\nE                   \tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\nE                   \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\nE                   \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\nE                   \tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\nE                   \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\nE                   \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\nE                   \tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\nE                   \tat org.apache.spark.scheduler.Task.run(Task.scala:147)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\nE                   \tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\nE                   \tat java.base/java.lang.Thread.run(Thread.java:840)\n\nvenv/lib/python3.14/site-packages/py4j/protocol.py:327: Py4JJavaError\n\n----------------------------- Captured stdout call -----------------------------\n=== Starting Transactions Transform for day1 ===\n\n----------------------------- Captured stderr call -----------------------------\n26/01/22 12:17:33 ERROR Executor: Exception in task 0.0 in stage 56.0 (TID 55)\norg.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\n    check_python_version(infile)\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\n    raise PySparkRuntimeError(\npyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nPlease check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.sort_addToSorter_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n\tat org.apache.spark.sql.execution.aggregate.SortAggregateExec.$anonfun$doExecute$1(SortAggregateExec.scala:63)\n\tat org.apache.spark.sql.execution.aggregate.SortAggregateExec.$anonfun$doExecute$1$adapted(SortAggregateExec.scala:60)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:888)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:888)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n26/01/22 12:17:33 WARN TaskSetManager: Lost task 0.0 in stage 56.0 (TID 55) (10.20.80.191 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\n    check_python_version(infile)\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\n    raise PySparkRuntimeError(\npyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nPlease check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.sort_addToSorter_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n\tat org.apache.spark.sql.execution.aggregate.SortAggregateExec.$anonfun$doExecute$1(SortAggregateExec.scala:63)\n\tat org.apache.spark.sql.execution.aggregate.SortAggregateExec.$anonfun$doExecute$1$adapted(SortAggregateExec.scala:60)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:888)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:888)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n\n26/01/22 12:17:33 ERROR TaskSetManager: Task 0 in stage 56.0 failed 1 times; aborting job\n&#34;}], &#34;tests/test_transactions_transform.py::test_flat_input_supported&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/test_transactions_transform.py::test_flat_input_supported&#34;, &#34;duration&#34;: &#34;284 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/test_transactions_transform.py::test_flat_input_supported&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;284 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;mock_get_rate = &amp;lt;MagicMock name=&amp;#x27;get_rate_to_usd&amp;#x27; id=&amp;#x27;4512668304&amp;#x27;&amp;gt;\nspark = &amp;lt;pyspark.sql.session.SparkSession object at 0x10ca1de80&amp;gt;\n\n    @patch(&amp;quot;src.pipelines.transactions.transactions_transform.get_rate_to_usd&amp;quot;)\n    def test_flat_input_supported(mock_get_rate, spark):\n        mock_get_rate.return_value = 1.0\n    \n        df = spark.createDataFrame([{\n            &amp;quot;id&amp;quot;: &amp;quot;t5&amp;quot;,\n            &amp;quot;uuid&amp;quot;: &amp;quot;u5&amp;quot;,\n            &amp;quot;occurredTime&amp;quot;: &amp;quot;2024-01-05T10:00:00&amp;quot;,\n            &amp;quot;updatedTime&amp;quot;: &amp;quot;2024-01-05T11:00:00&amp;quot;,\n            &amp;quot;userId&amp;quot;: &amp;quot;user5&amp;quot;,\n            &amp;quot;userUuid&amp;quot;: &amp;quot;uu5&amp;quot;,\n            &amp;quot;userName&amp;quot;: &amp;quot;Flat&amp;quot;,\n            &amp;quot;merchantName&amp;quot;: &amp;quot;Test&amp;quot;,\n            &amp;quot;rawMerchantName&amp;quot;: &amp;quot;TST&amp;quot;,\n            &amp;quot;cardId&amp;quot;: &amp;quot;c5&amp;quot;,\n            &amp;quot;cardUuid&amp;quot;: &amp;quot;cu5&amp;quot;,\n            &amp;quot;budgetId&amp;quot;: &amp;quot;b5&amp;quot;,\n            &amp;quot;budgetUuid&amp;quot;: &amp;quot;bu5&amp;quot;,\n            &amp;quot;originalCurrencyAmount&amp;quot;: 5000,\n            &amp;quot;exponent&amp;quot;: 2,\n            &amp;quot;originalCurrencyCode&amp;quot;: &amp;quot;USD&amp;quot;,\n            &amp;quot;exchangeRate&amp;quot;: 1.0,\n        }], schema=flat_schema())\n    \n&amp;gt;       valid_df, error_df, _ = transform_transactions(df, &amp;quot;day1&amp;quot;)\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\ntests/test_transactions_transform.py:283: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \nsrc/pipelines/transactions/transactions_transform.py:143: in transform_transactions\n    .collect()\n     ^^^^^^^^^\nvenv/lib/python3.14/site-packages/pyspark/sql/classic/dataframe.py:443: in collect\n    sock_info = self._jdf.collectToPython()\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^\nvenv/lib/python3.14/site-packages/py4j/java_gateway.py:1362: in __call__\n    return_value = get_return_value(\nvenv/lib/python3.14/site-packages/pyspark/errors/exceptions/captured.py:263: in deco\n    return f(*a, **kw)\n           ^^^^^^^^^^^\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nanswer = &amp;#x27;xro12880&amp;#x27;\ngateway_client = &amp;lt;py4j.clientserver.JavaClient object at 0x10ca1c6e0&amp;gt;\ntarget_id = &amp;#x27;o12879&amp;#x27;, name = &amp;#x27;collectToPython&amp;#x27;\n\n    def get_return_value(answer, gateway_client, target_id=None, name=None):\n        &amp;quot;&amp;quot;&amp;quot;Converts an answer received from the Java gateway into a Python object.\n    \n        For example, string representation of integers are converted to Python\n        integer, string representation of objects are converted to JavaObject\n        instances, etc.\n    \n        :param answer: the string returned by the Java gateway\n        :param gateway_client: the gateway client used to communicate with the Java\n            Gateway. Only necessary if the answer is a reference (e.g., object,\n            list, map)\n        :param target_id: the name of the object from which the answer comes from\n            (e.g., *object1* in `object1.hello()`). Optional.\n        :param name: the name of the member from which the answer comes from\n            (e.g., *hello* in `object1.hello()`). Optional.\n        &amp;quot;&amp;quot;&amp;quot;\n        if is_error(answer)[0]:\n            if len(answer) &amp;gt; 1:\n                type = answer[1]\n                value = OUTPUT_CONVERTER[type](answer[2:], gateway_client)\n                if answer[1] == REFERENCE_TYPE:\n&amp;gt;                   raise Py4JJavaError(\n                        &amp;quot;An error occurred while calling {0}{1}{2}.\\n&amp;quot;.\n                        format(target_id, &amp;quot;.&amp;quot;, name), value)\nE                   py4j.protocol.Py4JJavaError: An error occurred while calling o12879.collectToPython.\nE                   : org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 57.0 failed 1 times, most recent failure: Lost task 0.0 in stage 57.0 (TID 56) (10.20.80.191 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\nE                       check_python_version(infile)\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\nE                       raise PySparkRuntimeError(\nE                   pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nE                   Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\nE                   \nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\nE                   \tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\nE                   \tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.sort_addToSorter_0$(Unknown Source)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\nE                   \tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\nE                   \tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\nE                   \tat org.apache.spark.sql.execution.aggregate.SortAggregateExec.$anonfun$doExecute$1(SortAggregateExec.scala:63)\nE                   \tat org.apache.spark.sql.execution.aggregate.SortAggregateExec.$anonfun$doExecute$1$adapted(SortAggregateExec.scala:60)\nE                   \tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:888)\nE                   \tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:888)\nE                   \tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\nE                   \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\nE                   \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\nE                   \tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\nE                   \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\nE                   \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\nE                   \tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\nE                   \tat org.apache.spark.scheduler.Task.run(Task.scala:147)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\nE                   \tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\nE                   \tat java.base/java.lang.Thread.run(Thread.java:840)\nE                   \nE                   Driver stacktrace:\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$3(DAGScheduler.scala:3122)\nE                   \tat scala.Option.getOrElse(Option.scala:201)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:3122)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:3114)\nE                   \tat scala.collection.immutable.List.foreach(List.scala:323)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:3114)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1303)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1303)\nE                   \tat scala.Option.foreach(Option.scala:437)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1303)\nE                   \tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3397)\nE                   \tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3328)\nE                   \tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3317)\nE                   \tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:50)\nE                   Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\nE                       check_python_version(infile)\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\nE                       raise PySparkRuntimeError(\nE                   pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nE                   Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\nE                   \nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\nE                   \tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\nE                   \tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.sort_addToSorter_0$(Unknown Source)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\nE                   \tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\nE                   \tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\nE                   \tat org.apache.spark.sql.execution.aggregate.SortAggregateExec.$anonfun$doExecute$1(SortAggregateExec.scala:63)\nE                   \tat org.apache.spark.sql.execution.aggregate.SortAggregateExec.$anonfun$doExecute$1$adapted(SortAggregateExec.scala:60)\nE                   \tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:888)\nE                   \tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:888)\nE                   \tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\nE                   \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\nE                   \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\nE                   \tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\nE                   \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\nE                   \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\nE                   \tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\nE                   \tat org.apache.spark.scheduler.Task.run(Task.scala:147)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\nE                   \tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\nE                   \tat java.base/java.lang.Thread.run(Thread.java:840)\n\nvenv/lib/python3.14/site-packages/py4j/protocol.py:327: Py4JJavaError\n\n----------------------------- Captured stdout call -----------------------------\n=== Starting Transactions Transform for day1 ===\n\n----------------------------- Captured stderr call -----------------------------\n26/01/22 12:17:34 ERROR Executor: Exception in task 0.0 in stage 57.0 (TID 56)\norg.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\n    check_python_version(infile)\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\n    raise PySparkRuntimeError(\npyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nPlease check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.sort_addToSorter_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n\tat org.apache.spark.sql.execution.aggregate.SortAggregateExec.$anonfun$doExecute$1(SortAggregateExec.scala:63)\n\tat org.apache.spark.sql.execution.aggregate.SortAggregateExec.$anonfun$doExecute$1$adapted(SortAggregateExec.scala:60)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:888)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:888)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n26/01/22 12:17:34 WARN TaskSetManager: Lost task 0.0 in stage 57.0 (TID 56) (10.20.80.191 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\n    check_python_version(infile)\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\n    raise PySparkRuntimeError(\npyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nPlease check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.sort_addToSorter_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n\tat org.apache.spark.sql.execution.aggregate.SortAggregateExec.$anonfun$doExecute$1(SortAggregateExec.scala:63)\n\tat org.apache.spark.sql.execution.aggregate.SortAggregateExec.$anonfun$doExecute$1$adapted(SortAggregateExec.scala:60)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:888)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:888)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n\n26/01/22 12:17:34 ERROR TaskSetManager: Task 0 in stage 57.0 failed 1 times; aborting job\n&#34;}], &#34;tests/test_transactions_transform.py::test_idempotency_key_deterministic&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/test_transactions_transform.py::test_idempotency_key_deterministic&#34;, &#34;duration&#34;: &#34;199 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/test_transactions_transform.py::test_idempotency_key_deterministic&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;199 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;mock_get_rate = &amp;lt;MagicMock name=&amp;#x27;get_rate_to_usd&amp;#x27; id=&amp;#x27;4512668976&amp;#x27;&amp;gt;\nspark = &amp;lt;pyspark.sql.session.SparkSession object at 0x10ca1de80&amp;gt;\n\n    @patch(&amp;quot;src.pipelines.transactions.transactions_transform.get_rate_to_usd&amp;quot;)\n    def test_idempotency_key_deterministic(mock_get_rate, spark):\n        mock_get_rate.return_value = 1.0\n    \n        df = spark.createDataFrame([{\n            &amp;quot;results&amp;quot;: [{\n                &amp;quot;id&amp;quot;: &amp;quot;t6&amp;quot;,\n                &amp;quot;uuid&amp;quot;: &amp;quot;u6&amp;quot;,\n                &amp;quot;occurredTime&amp;quot;: &amp;quot;2024-01-06T10:00:00&amp;quot;,\n                &amp;quot;updatedTime&amp;quot;: &amp;quot;2024-01-06T11:00:00&amp;quot;,\n                &amp;quot;userId&amp;quot;: &amp;quot;user6&amp;quot;,\n                &amp;quot;userUuid&amp;quot;: &amp;quot;uu6&amp;quot;,\n                &amp;quot;userName&amp;quot;: &amp;quot;Stable&amp;quot;,\n                &amp;quot;merchantName&amp;quot;: &amp;quot;Test&amp;quot;,\n                &amp;quot;rawMerchantName&amp;quot;: &amp;quot;TST&amp;quot;,\n                &amp;quot;cardId&amp;quot;: &amp;quot;c6&amp;quot;,\n                &amp;quot;cardUuid&amp;quot;: &amp;quot;cu6&amp;quot;,\n                &amp;quot;budgetId&amp;quot;: &amp;quot;b6&amp;quot;,\n                &amp;quot;budgetUuid&amp;quot;: &amp;quot;bu6&amp;quot;,\n                &amp;quot;currencyData&amp;quot;: {\n                    &amp;quot;originalCurrencyAmount&amp;quot;: 6000,\n                    &amp;quot;exponent&amp;quot;: 2,\n                    &amp;quot;originalCurrencyCode&amp;quot;: &amp;quot;USD&amp;quot;,\n                    &amp;quot;exchangeRate&amp;quot;: 1.0\n                }\n            }]\n        }], schema=raw_schema())\n    \n&amp;gt;       valid_df, _, _ = transform_transactions(df, &amp;quot;day1&amp;quot;)\n                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\ntests/test_transactions_transform.py:319: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \nsrc/pipelines/transactions/transactions_transform.py:143: in transform_transactions\n    .collect()\n     ^^^^^^^^^\nvenv/lib/python3.14/site-packages/pyspark/sql/classic/dataframe.py:443: in collect\n    sock_info = self._jdf.collectToPython()\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^\nvenv/lib/python3.14/site-packages/py4j/java_gateway.py:1362: in __call__\n    return_value = get_return_value(\nvenv/lib/python3.14/site-packages/pyspark/errors/exceptions/captured.py:263: in deco\n    return f(*a, **kw)\n           ^^^^^^^^^^^\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nanswer = &amp;#x27;xro13270&amp;#x27;\ngateway_client = &amp;lt;py4j.clientserver.JavaClient object at 0x10ca1c6e0&amp;gt;\ntarget_id = &amp;#x27;o13269&amp;#x27;, name = &amp;#x27;collectToPython&amp;#x27;\n\n    def get_return_value(answer, gateway_client, target_id=None, name=None):\n        &amp;quot;&amp;quot;&amp;quot;Converts an answer received from the Java gateway into a Python object.\n    \n        For example, string representation of integers are converted to Python\n        integer, string representation of objects are converted to JavaObject\n        instances, etc.\n    \n        :param answer: the string returned by the Java gateway\n        :param gateway_client: the gateway client used to communicate with the Java\n            Gateway. Only necessary if the answer is a reference (e.g., object,\n            list, map)\n        :param target_id: the name of the object from which the answer comes from\n            (e.g., *object1* in `object1.hello()`). Optional.\n        :param name: the name of the member from which the answer comes from\n            (e.g., *hello* in `object1.hello()`). Optional.\n        &amp;quot;&amp;quot;&amp;quot;\n        if is_error(answer)[0]:\n            if len(answer) &amp;gt; 1:\n                type = answer[1]\n                value = OUTPUT_CONVERTER[type](answer[2:], gateway_client)\n                if answer[1] == REFERENCE_TYPE:\n&amp;gt;                   raise Py4JJavaError(\n                        &amp;quot;An error occurred while calling {0}{1}{2}.\\n&amp;quot;.\n                        format(target_id, &amp;quot;.&amp;quot;, name), value)\nE                   py4j.protocol.Py4JJavaError: An error occurred while calling o13269.collectToPython.\nE                   : org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 58.0 failed 1 times, most recent failure: Lost task 0.0 in stage 58.0 (TID 57) (10.20.80.191 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\nE                       check_python_version(infile)\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\nE                       raise PySparkRuntimeError(\nE                   pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nE                   Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\nE                   \nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\nE                   \tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\nE                   \tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.sort_addToSorter_0$(Unknown Source)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\nE                   \tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\nE                   \tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\nE                   \tat org.apache.spark.sql.execution.aggregate.SortAggregateExec.$anonfun$doExecute$1(SortAggregateExec.scala:63)\nE                   \tat org.apache.spark.sql.execution.aggregate.SortAggregateExec.$anonfun$doExecute$1$adapted(SortAggregateExec.scala:60)\nE                   \tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:888)\nE                   \tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:888)\nE                   \tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\nE                   \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\nE                   \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\nE                   \tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\nE                   \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\nE                   \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\nE                   \tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\nE                   \tat org.apache.spark.scheduler.Task.run(Task.scala:147)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\nE                   \tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\nE                   \tat java.base/java.lang.Thread.run(Thread.java:840)\nE                   \nE                   Driver stacktrace:\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$3(DAGScheduler.scala:3122)\nE                   \tat scala.Option.getOrElse(Option.scala:201)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:3122)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:3114)\nE                   \tat scala.collection.immutable.List.foreach(List.scala:323)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:3114)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1303)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1303)\nE                   \tat scala.Option.foreach(Option.scala:437)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1303)\nE                   \tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3397)\nE                   \tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3328)\nE                   \tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3317)\nE                   \tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:50)\nE                   Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\nE                       check_python_version(infile)\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\nE                       raise PySparkRuntimeError(\nE                   pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nE                   Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\nE                   \nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\nE                   \tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\nE                   \tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.sort_addToSorter_0$(Unknown Source)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\nE                   \tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\nE                   \tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\nE                   \tat org.apache.spark.sql.execution.aggregate.SortAggregateExec.$anonfun$doExecute$1(SortAggregateExec.scala:63)\nE                   \tat org.apache.spark.sql.execution.aggregate.SortAggregateExec.$anonfun$doExecute$1$adapted(SortAggregateExec.scala:60)\nE                   \tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:888)\nE                   \tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:888)\nE                   \tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\nE                   \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\nE                   \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\nE                   \tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\nE                   \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\nE                   \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\nE                   \tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\nE                   \tat org.apache.spark.scheduler.Task.run(Task.scala:147)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\nE                   \tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\nE                   \tat java.base/java.lang.Thread.run(Thread.java:840)\n\nvenv/lib/python3.14/site-packages/py4j/protocol.py:327: Py4JJavaError\n\n----------------------------- Captured stdout call -----------------------------\n=== Starting Transactions Transform for day1 ===\n\n----------------------------- Captured stderr call -----------------------------\n26/01/22 12:17:34 ERROR Executor: Exception in task 0.0 in stage 58.0 (TID 57)\norg.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\n    check_python_version(infile)\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\n    raise PySparkRuntimeError(\npyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nPlease check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.sort_addToSorter_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n\tat org.apache.spark.sql.execution.aggregate.SortAggregateExec.$anonfun$doExecute$1(SortAggregateExec.scala:63)\n\tat org.apache.spark.sql.execution.aggregate.SortAggregateExec.$anonfun$doExecute$1$adapted(SortAggregateExec.scala:60)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:888)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:888)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n26/01/22 12:17:34 WARN TaskSetManager: Lost task 0.0 in stage 58.0 (TID 57) (10.20.80.191 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\n    check_python_version(infile)\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\n    raise PySparkRuntimeError(\npyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nPlease check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.sort_addToSorter_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n\tat org.apache.spark.sql.execution.aggregate.SortAggregateExec.$anonfun$doExecute$1(SortAggregateExec.scala:63)\n\tat org.apache.spark.sql.execution.aggregate.SortAggregateExec.$anonfun$doExecute$1$adapted(SortAggregateExec.scala:60)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:888)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:888)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n\n26/01/22 12:17:34 ERROR TaskSetManager: Task 0 in stage 58.0 failed 1 times; aborting job\n&#34;}], &#34;tests/test_transactions_transform.py::test_zero_amount_invalid&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/test_transactions_transform.py::test_zero_amount_invalid&#34;, &#34;duration&#34;: &#34;197 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/test_transactions_transform.py::test_zero_amount_invalid&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;197 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;mock_get_rate = &amp;lt;MagicMock name=&amp;#x27;get_rate_to_usd&amp;#x27; id=&amp;#x27;4512667968&amp;#x27;&amp;gt;\nspark = &amp;lt;pyspark.sql.session.SparkSession object at 0x10ca1de80&amp;gt;\n\n    @patch(&amp;quot;src.pipelines.transactions.transactions_transform.get_rate_to_usd&amp;quot;)\n    def test_zero_amount_invalid(mock_get_rate, spark):\n        mock_get_rate.return_value = 1.0\n    \n        df = spark.createDataFrame([{\n            &amp;quot;id&amp;quot;: &amp;quot;t7&amp;quot;,\n            &amp;quot;uuid&amp;quot;: &amp;quot;u7&amp;quot;,\n            &amp;quot;occurredTime&amp;quot;: &amp;quot;2024-01-07T10:00:00&amp;quot;,\n            &amp;quot;updatedTime&amp;quot;: &amp;quot;2024-01-07T11:00:00&amp;quot;,\n            &amp;quot;userId&amp;quot;: &amp;quot;user7&amp;quot;,\n            &amp;quot;userUuid&amp;quot;: &amp;quot;uu7&amp;quot;,\n            &amp;quot;userName&amp;quot;: &amp;quot;ZeroAmount&amp;quot;,\n            &amp;quot;merchantName&amp;quot;: &amp;quot;Test&amp;quot;,\n            &amp;quot;rawMerchantName&amp;quot;: &amp;quot;TST&amp;quot;,\n            &amp;quot;cardId&amp;quot;: &amp;quot;c7&amp;quot;,\n            &amp;quot;cardUuid&amp;quot;: &amp;quot;cu7&amp;quot;,\n            &amp;quot;budgetId&amp;quot;: &amp;quot;b7&amp;quot;,\n            &amp;quot;budgetUuid&amp;quot;: &amp;quot;bu7&amp;quot;,\n            &amp;quot;originalCurrencyAmount&amp;quot;: 0,\n            &amp;quot;exponent&amp;quot;: 2,\n            &amp;quot;originalCurrencyCode&amp;quot;: &amp;quot;USD&amp;quot;,\n            &amp;quot;exchangeRate&amp;quot;: 1.0,\n        }], schema=flat_schema())\n    \n&amp;gt;       valid_df, error_df, _ = transform_transactions(df, &amp;quot;day1&amp;quot;)\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\ntests/test_transactions_transform.py:352: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \nsrc/pipelines/transactions/transactions_transform.py:143: in transform_transactions\n    .collect()\n     ^^^^^^^^^\nvenv/lib/python3.14/site-packages/pyspark/sql/classic/dataframe.py:443: in collect\n    sock_info = self._jdf.collectToPython()\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^\nvenv/lib/python3.14/site-packages/py4j/java_gateway.py:1362: in __call__\n    return_value = get_return_value(\nvenv/lib/python3.14/site-packages/pyspark/errors/exceptions/captured.py:263: in deco\n    return f(*a, **kw)\n           ^^^^^^^^^^^\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nanswer = &amp;#x27;xro13678&amp;#x27;\ngateway_client = &amp;lt;py4j.clientserver.JavaClient object at 0x10ca1c6e0&amp;gt;\ntarget_id = &amp;#x27;o13677&amp;#x27;, name = &amp;#x27;collectToPython&amp;#x27;\n\n    def get_return_value(answer, gateway_client, target_id=None, name=None):\n        &amp;quot;&amp;quot;&amp;quot;Converts an answer received from the Java gateway into a Python object.\n    \n        For example, string representation of integers are converted to Python\n        integer, string representation of objects are converted to JavaObject\n        instances, etc.\n    \n        :param answer: the string returned by the Java gateway\n        :param gateway_client: the gateway client used to communicate with the Java\n            Gateway. Only necessary if the answer is a reference (e.g., object,\n            list, map)\n        :param target_id: the name of the object from which the answer comes from\n            (e.g., *object1* in `object1.hello()`). Optional.\n        :param name: the name of the member from which the answer comes from\n            (e.g., *hello* in `object1.hello()`). Optional.\n        &amp;quot;&amp;quot;&amp;quot;\n        if is_error(answer)[0]:\n            if len(answer) &amp;gt; 1:\n                type = answer[1]\n                value = OUTPUT_CONVERTER[type](answer[2:], gateway_client)\n                if answer[1] == REFERENCE_TYPE:\n&amp;gt;                   raise Py4JJavaError(\n                        &amp;quot;An error occurred while calling {0}{1}{2}.\\n&amp;quot;.\n                        format(target_id, &amp;quot;.&amp;quot;, name), value)\nE                   py4j.protocol.Py4JJavaError: An error occurred while calling o13677.collectToPython.\nE                   : org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 59.0 failed 1 times, most recent failure: Lost task 0.0 in stage 59.0 (TID 58) (10.20.80.191 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\nE                       check_python_version(infile)\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\nE                       raise PySparkRuntimeError(\nE                   pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nE                   Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\nE                   \nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\nE                   \tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\nE                   \tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.sort_addToSorter_0$(Unknown Source)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\nE                   \tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\nE                   \tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\nE                   \tat org.apache.spark.sql.execution.aggregate.SortAggregateExec.$anonfun$doExecute$1(SortAggregateExec.scala:63)\nE                   \tat org.apache.spark.sql.execution.aggregate.SortAggregateExec.$anonfun$doExecute$1$adapted(SortAggregateExec.scala:60)\nE                   \tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:888)\nE                   \tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:888)\nE                   \tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\nE                   \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\nE                   \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\nE                   \tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\nE                   \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\nE                   \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\nE                   \tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\nE                   \tat org.apache.spark.scheduler.Task.run(Task.scala:147)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\nE                   \tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\nE                   \tat java.base/java.lang.Thread.run(Thread.java:840)\nE                   \nE                   Driver stacktrace:\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$3(DAGScheduler.scala:3122)\nE                   \tat scala.Option.getOrElse(Option.scala:201)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:3122)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:3114)\nE                   \tat scala.collection.immutable.List.foreach(List.scala:323)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:3114)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1303)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1303)\nE                   \tat scala.Option.foreach(Option.scala:437)\nE                   \tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1303)\nE                   \tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3397)\nE                   \tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3328)\nE                   \tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3317)\nE                   \tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:50)\nE                   Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\nE                       check_python_version(infile)\nE                     File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\nE                       raise PySparkRuntimeError(\nE                   pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nE                   Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\nE                   \nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\nE                   \tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\nE                   \tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\nE                   \tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.sort_addToSorter_0$(Unknown Source)\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\nE                   \tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\nE                   \tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\nE                   \tat org.apache.spark.sql.execution.aggregate.SortAggregateExec.$anonfun$doExecute$1(SortAggregateExec.scala:63)\nE                   \tat org.apache.spark.sql.execution.aggregate.SortAggregateExec.$anonfun$doExecute$1$adapted(SortAggregateExec.scala:60)\nE                   \tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:888)\nE                   \tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:888)\nE                   \tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\nE                   \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\nE                   \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\nE                   \tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\nE                   \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\nE                   \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)\nE                   \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\nE                   \tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\nE                   \tat org.apache.spark.scheduler.Task.run(Task.scala:147)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\nE                   \tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\nE                   \tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\nE                   \tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\nE                   \tat java.base/java.lang.Thread.run(Thread.java:840)\n\nvenv/lib/python3.14/site-packages/py4j/protocol.py:327: Py4JJavaError\n\n----------------------------- Captured stdout call -----------------------------\n=== Starting Transactions Transform for day1 ===\n\n----------------------------- Captured stderr call -----------------------------\n26/01/22 12:17:34 ERROR Executor: Exception in task 0.0 in stage 59.0 (TID 58)\norg.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\n    check_python_version(infile)\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\n    raise PySparkRuntimeError(\npyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nPlease check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.sort_addToSorter_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n\tat org.apache.spark.sql.execution.aggregate.SortAggregateExec.$anonfun$doExecute$1(SortAggregateExec.scala:63)\n\tat org.apache.spark.sql.execution.aggregate.SortAggregateExec.$anonfun$doExecute$1$adapted(SortAggregateExec.scala:60)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:888)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:888)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n26/01/22 12:17:34 WARN TaskSetManager: Lost task 0.0 in stage 59.0 (TID 58) (10.20.80.191 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py&amp;quot;, line 3305, in main\n    check_python_version(infile)\n  File &amp;quot;/Users/rithvik/zluri-data-pipeline/venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker_util.py&amp;quot;, line 77, in check_python_version\n    raise PySparkRuntimeError(\npyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.14, PySpark cannot run with different minor versions.\nPlease check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.sort_addToSorter_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n\tat org.apache.spark.sql.execution.aggregate.SortAggregateExec.$anonfun$doExecute$1(SortAggregateExec.scala:63)\n\tat org.apache.spark.sql.execution.aggregate.SortAggregateExec.$anonfun$doExecute$1$adapted(SortAggregateExec.scala:60)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:888)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:888)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n\n26/01/22 12:17:34 ERROR TaskSetManager: Task 0 in stage 59.0 failed 1 times; aborting job\n&#34;}]}, &#34;renderCollapsed&#34;: [&#34;passed&#34;], &#34;initialSort&#34;: &#34;result&#34;, &#34;title&#34;: &#34;report.html&#34;}"></div>
    <script>
      (function(){function r(e,n,t){function o(i,f){if(!n[i]){if(!e[i]){var c="function"==typeof require&&require;if(!f&&c)return c(i,!0);if(u)return u(i,!0);var a=new Error("Cannot find module '"+i+"'");throw a.code="MODULE_NOT_FOUND",a}var p=n[i]={exports:{}};e[i][0].call(p.exports,function(r){var n=e[i][1][r];return o(n||r)},p,p.exports,r,e,n,t)}return n[i].exports}for(var u="function"==typeof require&&require,i=0;i<t.length;i++)o(t[i]);return o}return r})()({1:[function(require,module,exports){
const { getCollapsedCategory, setCollapsedIds } = require('./storage.js')

class DataManager {
    setManager(data) {
        const collapsedCategories = [...getCollapsedCategory(data.renderCollapsed)]
        const collapsedIds = []
        const tests = Object.values(data.tests).flat().map((test, index) => {
            const collapsed = collapsedCategories.includes(test.result.toLowerCase())
            const id = `test_${index}`
            if (collapsed) {
                collapsedIds.push(id)
            }
            return {
                ...test,
                id,
                collapsed,
            }
        })
        const dataBlob = { ...data, tests }
        this.data = { ...dataBlob }
        this.renderData = { ...dataBlob }
        setCollapsedIds(collapsedIds)
    }

    get allData() {
        return { ...this.data }
    }

    resetRender() {
        this.renderData = { ...this.data }
    }

    setRender(data) {
        this.renderData.tests = [...data]
    }

    toggleCollapsedItem(id) {
        this.renderData.tests = this.renderData.tests.map((test) =>
            test.id === id ? { ...test, collapsed: !test.collapsed } : test,
        )
    }

    set allCollapsed(collapsed) {
        this.renderData = { ...this.renderData, tests: [...this.renderData.tests.map((test) => (
            { ...test, collapsed }
        ))] }
    }

    get testSubset() {
        return [...this.renderData.tests]
    }

    get environment() {
        return this.renderData.environment
    }

    get initialSort() {
        return this.data.initialSort
    }
}

module.exports = {
    manager: new DataManager(),
}

},{"./storage.js":8}],2:[function(require,module,exports){
const mediaViewer = require('./mediaviewer.js')
const templateEnvRow = document.getElementById('template_environment_row')
const templateResult = document.getElementById('template_results-table__tbody')

function htmlToElements(html) {
    const temp = document.createElement('template')
    temp.innerHTML = html
    return temp.content.childNodes
}

const find = (selector, elem) => {
    if (!elem) {
        elem = document
    }
    return elem.querySelector(selector)
}

const findAll = (selector, elem) => {
    if (!elem) {
        elem = document
    }
    return [...elem.querySelectorAll(selector)]
}

const dom = {
    getStaticRow: (key, value) => {
        const envRow = templateEnvRow.content.cloneNode(true)
        const isObj = typeof value === 'object' && value !== null
        const values = isObj ? Object.keys(value).map((k) => `${k}: ${value[k]}`) : null

        const valuesElement = htmlToElements(
            values ? `<ul>${values.map((val) => `<li>${val}</li>`).join('')}<ul>` : `<div>${value}</div>`)[0]
        const td = findAll('td', envRow)
        td[0].textContent = key
        td[1].appendChild(valuesElement)

        return envRow
    },
    getResultTBody: ({ testId, id, log, extras, resultsTableRow, tableHtml, result, collapsed }) => {
        const resultBody = templateResult.content.cloneNode(true)
        resultBody.querySelector('tbody').classList.add(result.toLowerCase())
        resultBody.querySelector('tbody').id = testId
        resultBody.querySelector('.collapsible').dataset.id = id

        resultsTableRow.forEach((html) => {
            const t = document.createElement('template')
            t.innerHTML = html
            resultBody.querySelector('.collapsible').appendChild(t.content)
        })

        if (log) {
            // Wrap lines starting with "E" with span.error to color those lines red
            const wrappedLog = log.replace(/^E.*$/gm, (match) => `<span class="error">${match}</span>`)
            resultBody.querySelector('.log').innerHTML = wrappedLog
        } else {
            resultBody.querySelector('.log').remove()
        }

        if (collapsed) {
            resultBody.querySelector('.collapsible > .col-result')?.classList.add('collapsed')
            resultBody.querySelector('.extras-row').classList.add('hidden')
        } else {
            resultBody.querySelector('.collapsible > .col-result')?.classList.remove('collapsed')
        }

        const media = []
        extras?.forEach(({ name, format_type, content }) => {
            if (['image', 'video'].includes(format_type)) {
                media.push({ path: content, name, format_type })
            }

            if (format_type === 'html') {
                resultBody.querySelector('.extraHTML').insertAdjacentHTML('beforeend', `<div>${content}</div>`)
            }
        })
        mediaViewer.setup(resultBody, media)

        // Add custom html from the pytest_html_results_table_html hook
        tableHtml?.forEach((item) => {
            resultBody.querySelector('td[class="extra"]').insertAdjacentHTML('beforeend', item)
        })

        return resultBody
    },
}

module.exports = {
    dom,
    htmlToElements,
    find,
    findAll,
}

},{"./mediaviewer.js":6}],3:[function(require,module,exports){
const { manager } = require('./datamanager.js')
const { doSort } = require('./sort.js')
const storageModule = require('./storage.js')

const getFilteredSubSet = (filter) =>
    manager.allData.tests.filter(({ result }) => filter.includes(result.toLowerCase()))

const doInitFilter = () => {
    const currentFilter = storageModule.getVisible()
    const filteredSubset = getFilteredSubSet(currentFilter)
    manager.setRender(filteredSubset)
}

const doFilter = (type, show) => {
    if (show) {
        storageModule.showCategory(type)
    } else {
        storageModule.hideCategory(type)
    }

    const currentFilter = storageModule.getVisible()
    const filteredSubset = getFilteredSubSet(currentFilter)
    manager.setRender(filteredSubset)

    const sortColumn = storageModule.getSort()
    doSort(sortColumn, true)
}

module.exports = {
    doFilter,
    doInitFilter,
}

},{"./datamanager.js":1,"./sort.js":7,"./storage.js":8}],4:[function(require,module,exports){
const { redraw, bindEvents, renderStatic } = require('./main.js')
const { doInitFilter } = require('./filter.js')
const { doInitSort } = require('./sort.js')
const { manager } = require('./datamanager.js')
const data = JSON.parse(document.getElementById('data-container').dataset.jsonblob)

function init() {
    manager.setManager(data)
    doInitFilter()
    doInitSort()
    renderStatic()
    redraw()
    bindEvents()
}

init()

},{"./datamanager.js":1,"./filter.js":3,"./main.js":5,"./sort.js":7}],5:[function(require,module,exports){
const { dom, find, findAll } = require('./dom.js')
const { manager } = require('./datamanager.js')
const { doSort } = require('./sort.js')
const { doFilter } = require('./filter.js')
const {
    getVisible,
    getCollapsedIds,
    setCollapsedIds,
    getSort,
    getSortDirection,
    possibleFilters,
} = require('./storage.js')

const removeChildren = (node) => {
    while (node.firstChild) {
        node.removeChild(node.firstChild)
    }
}

const renderStatic = () => {
    const renderEnvironmentTable = () => {
        const environment = manager.environment
        const rows = Object.keys(environment).map((key) => dom.getStaticRow(key, environment[key]))
        const table = document.getElementById('environment')
        removeChildren(table)
        rows.forEach((row) => table.appendChild(row))
    }
    renderEnvironmentTable()
}

const addItemToggleListener = (elem) => {
    elem.addEventListener('click', ({ target }) => {
        const id = target.parentElement.dataset.id
        manager.toggleCollapsedItem(id)

        const collapsedIds = getCollapsedIds()
        if (collapsedIds.includes(id)) {
            const updated = collapsedIds.filter((item) => item !== id)
            setCollapsedIds(updated)
        } else {
            collapsedIds.push(id)
            setCollapsedIds(collapsedIds)
        }
        redraw()
    })
}

const renderContent = (tests) => {
    const sortAttr = getSort(manager.initialSort)
    const sortAsc = JSON.parse(getSortDirection())
    const rows = tests.map(dom.getResultTBody)
    const table = document.getElementById('results-table')
    const tableHeader = document.getElementById('results-table-head')

    const newTable = document.createElement('table')
    newTable.id = 'results-table'

    // remove all sorting classes and set the relevant
    findAll('.sortable', tableHeader).forEach((elem) => elem.classList.remove('asc', 'desc'))
    tableHeader.querySelector(`.sortable[data-column-type="${sortAttr}"]`)?.classList.add(sortAsc ? 'desc' : 'asc')
    newTable.appendChild(tableHeader)

    if (!rows.length) {
        const emptyTable = document.getElementById('template_results-table__body--empty').content.cloneNode(true)
        newTable.appendChild(emptyTable)
    } else {
        rows.forEach((row) => {
            if (!!row) {
                findAll('.collapsible td:not(.col-links', row).forEach(addItemToggleListener)
                find('.logexpander', row).addEventListener('click',
                    (evt) => evt.target.parentNode.classList.toggle('expanded'),
                )
                newTable.appendChild(row)
            }
        })
    }

    table.replaceWith(newTable)
}

const renderDerived = () => {
    const currentFilter = getVisible()
    possibleFilters.forEach((result) => {
        const input = document.querySelector(`input[data-test-result="${result}"]`)
        input.checked = currentFilter.includes(result)
    })
}

const bindEvents = () => {
    const filterColumn = (evt) => {
        const { target: element } = evt
        const { testResult } = element.dataset

        doFilter(testResult, element.checked)
        const collapsedIds = getCollapsedIds()
        const updated = manager.renderData.tests.map((test) => {
            return {
                ...test,
                collapsed: collapsedIds.includes(test.id),
            }
        })
        manager.setRender(updated)
        redraw()
    }

    const header = document.getElementById('environment-header')
    header.addEventListener('click', () => {
        const table = document.getElementById('environment')
        table.classList.toggle('hidden')
        header.classList.toggle('collapsed')
    })

    findAll('input[name="filter_checkbox"]').forEach((elem) => {
        elem.addEventListener('click', filterColumn)
    })

    findAll('.sortable').forEach((elem) => {
        elem.addEventListener('click', (evt) => {
            const { target: element } = evt
            const { columnType } = element.dataset
            doSort(columnType)
            redraw()
        })
    })

    document.getElementById('show_all_details').addEventListener('click', () => {
        manager.allCollapsed = false
        setCollapsedIds([])
        redraw()
    })
    document.getElementById('hide_all_details').addEventListener('click', () => {
        manager.allCollapsed = true
        const allIds = manager.renderData.tests.map((test) => test.id)
        setCollapsedIds(allIds)
        redraw()
    })
}

const redraw = () => {
    const { testSubset } = manager

    renderContent(testSubset)
    renderDerived()
}

module.exports = {
    redraw,
    bindEvents,
    renderStatic,
}

},{"./datamanager.js":1,"./dom.js":2,"./filter.js":3,"./sort.js":7,"./storage.js":8}],6:[function(require,module,exports){
class MediaViewer {
    constructor(assets) {
        this.assets = assets
        this.index = 0
    }

    nextActive() {
        this.index = this.index === this.assets.length - 1 ? 0 : this.index + 1
        return [this.activeFile, this.index]
    }

    prevActive() {
        this.index = this.index === 0 ? this.assets.length - 1 : this.index -1
        return [this.activeFile, this.index]
    }

    get currentIndex() {
        return this.index
    }

    get activeFile() {
        return this.assets[this.index]
    }
}


const setup = (resultBody, assets) => {
    if (!assets.length) {
        resultBody.querySelector('.media').classList.add('hidden')
        return
    }

    const mediaViewer = new MediaViewer(assets)
    const container = resultBody.querySelector('.media-container')
    const leftArrow = resultBody.querySelector('.media-container__nav--left')
    const rightArrow = resultBody.querySelector('.media-container__nav--right')
    const mediaName = resultBody.querySelector('.media__name')
    const counter = resultBody.querySelector('.media__counter')
    const imageEl = resultBody.querySelector('img')
    const sourceEl = resultBody.querySelector('source')
    const videoEl = resultBody.querySelector('video')

    const setImg = (media, index) => {
        if (media?.format_type === 'image') {
            imageEl.src = media.path

            imageEl.classList.remove('hidden')
            videoEl.classList.add('hidden')
        } else if (media?.format_type === 'video') {
            sourceEl.src = media.path

            videoEl.classList.remove('hidden')
            imageEl.classList.add('hidden')
        }

        mediaName.innerText = media?.name
        counter.innerText = `${index + 1} / ${assets.length}`
    }
    setImg(mediaViewer.activeFile, mediaViewer.currentIndex)

    const moveLeft = () => {
        const [media, index] = mediaViewer.prevActive()
        setImg(media, index)
    }
    const doRight = () => {
        const [media, index] = mediaViewer.nextActive()
        setImg(media, index)
    }
    const openImg = () => {
        window.open(mediaViewer.activeFile.path, '_blank')
    }
    if (assets.length === 1) {
        container.classList.add('media-container--fullscreen')
    } else {
        leftArrow.addEventListener('click', moveLeft)
        rightArrow.addEventListener('click', doRight)
    }
    imageEl.addEventListener('click', openImg)
}

module.exports = {
    setup,
}

},{}],7:[function(require,module,exports){
const { manager } = require('./datamanager.js')
const storageModule = require('./storage.js')

const genericSort = (list, key, ascending, customOrder) => {
    let sorted
    if (customOrder) {
        sorted = list.sort((a, b) => {
            const aValue = a.result.toLowerCase()
            const bValue = b.result.toLowerCase()

            const aIndex = customOrder.findIndex((item) => item.toLowerCase() === aValue)
            const bIndex = customOrder.findIndex((item) => item.toLowerCase() === bValue)

            // Compare the indices to determine the sort order
            return aIndex - bIndex
        })
    } else {
        sorted = list.sort((a, b) => a[key] === b[key] ? 0 : a[key] > b[key] ? 1 : -1)
    }

    if (ascending) {
        sorted.reverse()
    }
    return sorted
}

const durationSort = (list, ascending) => {
    const parseDuration = (duration) => {
        if (duration.includes(':')) {
            // If it's in the format "HH:mm:ss"
            const [hours, minutes, seconds] = duration.split(':').map(Number)
            return (hours * 3600 + minutes * 60 + seconds) * 1000
        } else {
            // If it's in the format "nnn ms"
            return parseInt(duration)
        }
    }
    const sorted = list.sort((a, b) => parseDuration(a['duration']) - parseDuration(b['duration']))
    if (ascending) {
        sorted.reverse()
    }
    return sorted
}

const doInitSort = () => {
    const type = storageModule.getSort(manager.initialSort)
    const ascending = storageModule.getSortDirection()
    const list = manager.testSubset
    const initialOrder = ['Error', 'Failed', 'Rerun', 'XFailed', 'XPassed', 'Skipped', 'Passed']

    storageModule.setSort(type)
    storageModule.setSortDirection(ascending)

    if (type?.toLowerCase() === 'original') {
        manager.setRender(list)
    } else {
        let sortedList
        switch (type) {
        case 'duration':
            sortedList = durationSort(list, ascending)
            break
        case 'result':
            sortedList = genericSort(list, type, ascending, initialOrder)
            break
        default:
            sortedList = genericSort(list, type, ascending)
            break
        }
        manager.setRender(sortedList)
    }
}

const doSort = (type, skipDirection) => {
    const newSortType = storageModule.getSort(manager.initialSort) !== type
    const currentAsc = storageModule.getSortDirection()
    let ascending
    if (skipDirection) {
        ascending = currentAsc
    } else {
        ascending = newSortType ? false : !currentAsc
    }
    storageModule.setSort(type)
    storageModule.setSortDirection(ascending)

    const list = manager.testSubset
    const sortedList = type === 'duration' ? durationSort(list, ascending) : genericSort(list, type, ascending)
    manager.setRender(sortedList)
}

module.exports = {
    doInitSort,
    doSort,
}

},{"./datamanager.js":1,"./storage.js":8}],8:[function(require,module,exports){
const possibleFilters = [
    'passed',
    'skipped',
    'failed',
    'error',
    'xfailed',
    'xpassed',
    'rerun',
]

const getVisible = () => {
    const url = new URL(window.location.href)
    const settings = new URLSearchParams(url.search).get('visible')
    const lower = (item) => {
        const lowerItem = item.toLowerCase()
        if (possibleFilters.includes(lowerItem)) {
            return lowerItem
        }
        return null
    }
    return settings === null ?
        possibleFilters :
        [...new Set(settings?.split(',').map(lower).filter((item) => item))]
}

const hideCategory = (categoryToHide) => {
    const url = new URL(window.location.href)
    const visibleParams = new URLSearchParams(url.search).get('visible')
    const currentVisible = visibleParams ? visibleParams.split(',') : [...possibleFilters]
    const settings = [...new Set(currentVisible)].filter((f) => f !== categoryToHide).join(',')

    url.searchParams.set('visible', settings)
    window.history.pushState({}, null, unescape(url.href))
}

const showCategory = (categoryToShow) => {
    if (typeof window === 'undefined') {
        return
    }
    const url = new URL(window.location.href)
    const currentVisible = new URLSearchParams(url.search).get('visible')?.split(',').filter(Boolean) ||
        [...possibleFilters]
    const settings = [...new Set([categoryToShow, ...currentVisible])]
    const noFilter = possibleFilters.length === settings.length || !settings.length

    noFilter ? url.searchParams.delete('visible') : url.searchParams.set('visible', settings.join(','))
    window.history.pushState({}, null, unescape(url.href))
}

const getSort = (initialSort) => {
    const url = new URL(window.location.href)
    let sort = new URLSearchParams(url.search).get('sort')
    if (!sort) {
        sort = initialSort || 'result'
    }
    return sort
}

const setSort = (type) => {
    const url = new URL(window.location.href)
    url.searchParams.set('sort', type)
    window.history.pushState({}, null, unescape(url.href))
}

const getCollapsedCategory = (renderCollapsed) => {
    let categories
    if (typeof window !== 'undefined') {
        const url = new URL(window.location.href)
        const collapsedItems = new URLSearchParams(url.search).get('collapsed')
        switch (true) {
        case !renderCollapsed && collapsedItems === null:
            categories = ['passed']
            break
        case collapsedItems?.length === 0 || /^["']{2}$/.test(collapsedItems):
            categories = []
            break
        case /^all$/.test(collapsedItems) || collapsedItems === null && /^all$/.test(renderCollapsed):
            categories = [...possibleFilters]
            break
        default:
            categories = collapsedItems?.split(',').map((item) => item.toLowerCase()) || renderCollapsed
            break
        }
    } else {
        categories = []
    }
    return categories
}

const getSortDirection = () => JSON.parse(sessionStorage.getItem('sortAsc')) || false
const setSortDirection = (ascending) => sessionStorage.setItem('sortAsc', ascending)

const getCollapsedIds = () => JSON.parse(sessionStorage.getItem('collapsedIds')) || []
const setCollapsedIds = (list) => sessionStorage.setItem('collapsedIds', JSON.stringify(list))

module.exports = {
    getVisible,
    hideCategory,
    showCategory,
    getCollapsedIds,
    setCollapsedIds,
    getSort,
    setSort,
    getSortDirection,
    setSortDirection,
    getCollapsedCategory,
    possibleFilters,
}

},{}]},{},[4]);
    </script>
  </footer>
  </body>
</html>